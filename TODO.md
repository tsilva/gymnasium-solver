- recheck if deterministic version still converges with feature extractor fixes
- TASK: solve Pong-v5_objects, then propagate to other envs
- BUG: run is being logged even when training is not started
- test rlib
- BUG: Pong-v5_objects not converging test fixes
- BUG: total timesteps grwoth is not constant (print table)
maxout pong objects / bold terminal
- Add support for resuming training runs, this requires loading hyperparameters and schedulers to be timestep based; must also start from last timestep
https://github.com/kenjyoung/MinAtar
- Change config files so that they only say their mention algo_id in experiment name
- Apply atari wrapper to atari envs
- TODO: compare atari breakout fps vs rlzoosb3
- BUG: last inspect frame for atari is first frame of next episode
- WISHLIST: impelemnt SEEDRL with PPO to massively scale training
- can I create an exploration model by just making loss higher the more the model can predict the future?
- print effective rollout step size
- TODO: add config file beautifier that ensure attributes are set in the correct order
- TODO: add config file validator that ensures that all attributes are set and that they are set to the correct type
- TODO: inspect.py add support for showing raw vs processed frames, including frame stacks
- runs: along with each checkpoint we are saving a json file with the metrics at that checkpoint. We also want to save a CSV with the rollout data for that epoch; this data should contain exactly the same data as the table in inspect.py, so we can encapsualte the function that generates the csv and reuse it in both places.
- FEAT: add vizdoom support
- FEAT: add stable-retro support

- inspect.py: add LLM debugging support
- BUG: checkpoint jsons not storing correct metrics
- BUG: CartPole-v1 PPO is not training as fast; 115f9c73faed6785a0bd58c37f55298324e90f43 was ok
- add support for resuming training from a checkpoint
- FEAT: Add support for specifyingpo extra reward metrics for each environment, then make the reward shaper assign value for each of those rewards
- FEAT: add support for creating publishable video for youtube
- Generalized scheduling for metrics, make those be logged
- REFACTOR: rollout buffer can be much more efficient (review how sb3 does it) -- our fps is still 1000 below sb3
- TODO: Figure out why CartPole-v1/PPO works better with Tahn activation than ReLU
- BUG: PPO can solve FrozenLake-v1, but REINFORCE cannot. REINFORCE is likely not implemented correctly.
- BUG: is REINFORCE well implemented? are we waiting until the end of the episode to update the policy?
- FEAT: train on cloud server (eg: lightning cloud)
- FEAT: normalize rewards?
- FEAT: add normalization support 
- FEAT: add discrete env support
- Create benchmarking script to find optimal parallelization for env/machine combo
- TEST: predict next state to learn faster
- FEAT: add [Minari support](https://minari.farama.org/)
- BUG: videos not logged at correct timesteps
- FEAT : Normalize returns for REINFORCE
- FEAT: add warning confirming if ale-py has been compiled to target architecture (avoid rosetta in silicon macs)
- BUG: baseline_mean/std are being recorded even when not used
- BUG: metrics are not well sorted yet
- FEAT: add support for fully recording last eval
- BUG: check that were are matching rlzoo mstop criteria
- Pong-RAM: Add support for byte-selection
MaxAndSkipEnv
- BUG: video step is still not alligned
- Adjust eval policy for atari
	•	Normalize each byte (e.g., to [0,1]) and consider embedding bits (treat bytes as 8 bits).
	•	Try longer training and more seeds; RAM setups often need more steps to stabilize.
	•	If you can, expose extra emulator registers (RAM+) to reduce partial observability.
- How to measure advantage of different seeds versus reusing same env.
- Consider increasing sample efficiency by figuring out how different are transitions between different envs
- FEAT: add determinism check at beginning to make sure that rollout benefits from multiple envs (eg: Pong, test on PongDeterministic)
- FEAT: create cartpole reward shaper that prioritizes centering the pole
- FEAT: track output distribution
- BUG: fix thread safety issues with async eval collector (copy model weights with lock)
- FEAT: consider computing mean reward by timesteps, this way in eval we just have to request n_steps = reward_threshold * N, this will make it easier to support vectorized envs
- FEAT: a2c (only after reinforce/ppo is stable)
- FEAT: add same linear decay features as rlzoo
- FEAT: add interactive mode support
- CHECK: run rollout through dataloader process, do we always get n_batches? assert it 
- add support for plotting charts as text and feeding to llm, check how end of training does it
- https://cs.stanford.edu/people/karpathy/reinforcejs/index.html
- https://alazareva.github.io/rl_playground/
- FEAT: log results to huggingface
- FEAT: Write wandb diagnostics script, use claude desktop to debug
- FEAT: support continuous environments
- FEAT: support for multi-env rollout collectors
- FEAT: add multitask heads support (eg: Atari, Sega Genesis) -- consider large output space
- Ask agent for next learning steps/tasks (prompt file)
- REFACTOR: add type hints where applicable