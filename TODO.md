- BUG: REINFORCE not working
- BUG: REINFORCE is calculating returns using value model?
- how do advnatges make it into final calc

- confirm that buffer growth is under control
- FEAT: get reward threshold from env specs (hardcoded)
- add n_timesteps support
- add ability to hardcode reward threshold
- check that were are matching rlzoo stop criteria
- solve mountaincar with framestacking
- rlzoo save model, run it in my model and compare rollout results (set seed and 1 env)
- measure obs mean/var
- make sure training uses same collecotrs so thry calc mean reward through their reward window, make sure it inits through config
- record videos in bg with model copy
- check if rlzoo solves mountaincar
- rlzoo better due to missing param support like decay
- save best model/agent checkpoints (use trainer) / background tasks records video 
- Try solving MountainCar-v0 with PPO + frame stacking (no reward shaping)
- Solve PongRAM-v0 with PPO
- FEAT: log steps/episodes to progress bar
- BUG: eval is being calculated before window is full, consider evaling frequently by n_steps instead of n_episodes
- BUG: fix thread safety issues with async eval collector (copy model weights with lock)
- FEAT: support for softcoding activations
- FEAT: train for convergence without deterministic policy
- FEAT: a2c (only after reinforce/ppo is stable), add baseline subtraction
- FEAT: add normalization support
- FEAT: track output distribution
- FEAT: support continuous environments
- FEAT: support for multi-env rollout collectors
- FEAT: add multitask heads support (eg: Atari, Sega Genesis) -- consider large output space
- CHECK: run rollout through dataloader process, do we always get n_batches?
- CHECK: assert that dataloader is always going through all expected batches
- TODO: solve Pong-RAM with PPO
- When I group by episodes I discard data from that rollout that won't be included in the next sequence of trajectories, so I need to make sure I don't lose data
- FEAT: add alert support to metric tracker (algo dependent)
- make best model be saved to wandb
- log results to huggingface?
- TODO: make evaluation run in background and keep a mean reward window, it picks up the model params set up in it, runs N envs in sequence with N workers and 
- Write wandb diagnostics script, use claude desktop to debug
- add support for plotting charts as text and feeding to llm, check how end of training does it
- track environment stats, observarion stats, reward distributions, etc
- change api to match sb3
- https://cs.stanford.edu/people/karpathy/reinforcejs/index.html
- https://alazareva.github.io/rl_playground/