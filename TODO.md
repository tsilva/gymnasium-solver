- FEAT: change logging key to train/total_timesteps?
- FEAT: improve metric descriptions
- FEAT: write guide on how to monitor training
- FEAT: try training on deterministic Pong env
- FEAT: is REINFORCE well implemented? are we waiting until the end of the episode to update the policy?
- FEAT: train on cloud server
- FEAT: no deterministic evals in atari envs?
- FEAT: add challenge_id to configs
- FEAT: normalize rewards?
- FEAT: reward shaper for Pong-v5, points whenever paddle hits ball
- FEAT: make checkpointing directories be prefixed by challenge_id
- FEAT: add normalization support
- FEAT: add discrete env support
- Cleanup environment files by using inheritance
- FEAT: add support for softcoding activations
- Create benchmarking script to find optimal parallelization for env/machine combo
- TEST: predict next state to learn faster
- BUG: CartPole-v1/PPO training performance has regressed vs rlzoo
- FEAT: log ent_coef (may change it mid traingin)
- TEST: does benchmark script still give expected number of FPS on Pong? test with RAM vs RGB
- FEAT: add Minatari support
- BUG: videos not logged at correct timesteps
- FEAT: improve config structurtee
- FEAT : Normalize returns for REINFORCE
- FEAT: add warning confirming if ale-py has been compiled to target architecture (avoid rosetta in silicon macs)
- BUG: baseline_mean/std are being recorded even when not used
- BUG: metrics are not well sorted yet
- FEAT: add support for stopping training on reward threshold reached
- FEAT: add support for fully recording last eval
- FEAT: create cartpole-v1 reward shaper that prioritizes centering the pole
- BUG: check that were are matching rlzoo stop criteria
- Pong-RAM: Add support for byte-selection
MaxAndSkipEnv
- Should I normalize returns for REINFORCE?
- BUG: video step is still not alligned
- BUG: step 100 = reward 99
- Adjust eval policy for atari
	•	Normalize each byte (e.g., to [0,1]) and consider embedding bits (treat bytes as 8 bits).
	•	Try longer training and more seeds; RAM setups often need more steps to stabilize.
	•	If you can, expose extra emulator registers (RAM+) to reduce partial observability.
- How to measure advantage of different seeds versus reusing same env.
- ✅ Try creating local run folder with assets, create own run id and assign it to wandb if possible - IMPLEMENTED: runs/runid/ structure
- Consider increasing sample efficiency by figuring out how different are transitions between different envs
- FEAT: add determinism check at beginning to make sure that rollout benefits from multiple envs (eg: Pong, test on PongDeterministic)
- FEAT: create cartpole reward shaper that prioritizes centering the pole
- FEAT: track output distribution
- BUG: confirm that buffer growth is under control
- BUG: fix thread safety issues with async eval collector (copy model weights with lock)
- FEAT: consider computing mean reward by timesteps, this way in eval we just have to request n_steps = reward_threshold * N, this will make it easier to support vectorized envs
- FEAT: support for softcoding activations
- FEAT: a2c (only after reinforce/ppo is stable)
- FEAT: add same linear decay features as rlzoo
- FEAT: add interactive mode support
- CHECK: run rollout through dataloader process, do we always get n_batches? assert it 
- add support for plotting charts as text and feeding to llm, check how end of training does it
- https://cs.stanford.edu/people/karpathy/reinforcejs/index.html
- https://alazareva.github.io/rl_playground/
- FEAT: log results to huggingface
- FEAT: Write wandb diagnostics script, use claude desktop to debug
- FEAT: support continuous environments
- FEAT: support for multi-env rollout collectors
- FEAT: add multitask heads support (eg: Atari, Sega Genesis) -- consider large output space
- Ask agent for next learning steps/tasks (prompt file)
- REFACTOR: add type hints where applicable