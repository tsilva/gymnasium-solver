- inspect.py: make steps be zero-based (they currently start at 1)
- inspect.py: in the bottom where there is a summary, that should be a tabbed component, with one of the tabs showing the environment spec, the other showing the model spec, the other showing the checkpoint metrics
- inspect.py: add support for exporting table to CSV
- inspect.py: BUG: mc_return in last step should be zero
- inspect.py: BUG: it seems that the latest_run symlink is updated when I run inspector, this should not happen, no run should be created 
- inspect.py: add support for using random policy?
- runs: along with each checkpoint we are saving a json file with the metrics at that checkpoint. We also want to save a CSV with the rollout data for that epoch; this data should contain exactly the same data as the table in inspect.py, so we can encapsualte the function that generates the csv and reuse it in both places.
- BUG: videos folder is still being created
- BUG: checkpoint jsons not storing correct metrics
- BUG: evaluation is recording full episodes every epoch (this is not correct)
- BUG: CartPole-v1 PPO is not training as fast; 115f9c73faed6785a0bd58c37f55298324e90f43 was ok
- add support for resuming training from a checkpoint
- FEAT: Add support for specifyingpo extra reward metrics for each environment, then make the reward shaper assign value for each of those rewards
- FEAT: add support for creating publishable video for youtube
- Generalized scheduling for metrics, make those be logged
- FEAT: ask copilot to create its own isntructrions namely to generate its own techical documentation that it keeps up to date
- REFACTOR: rollout buffer can be much more efficient (review how sb3 does it) -- our fps is still 1000 below sb3
- TODO: Figure out why CartPole-v1/PPO works better with Tahn activation than ReLU
- BUG: PPO can solve FrozenLake-v1, but REINFORCE cannot. REINFORCE is likely not implemented correctly.
- BUG: is REINFORCE well implemented? are we waiting until the end of the episode to update the policy?
- FEAT: train on cloud server (eg: lightning cloud)
- FEAT: normalize rewards?
- FEAT: add normalization support
- FEAT: add discrete env support
- Create benchmarking script to find optimal parallelization for env/machine combo
- TEST: predict next state to learn faster
- FEAT: add [Minari support](https://minari.farama.org/)
- BUG: videos not logged at correct timesteps
- FEAT : Normalize returns for REINFORCE
- FEAT: add warning confirming if ale-py has been compiled to target architecture (avoid rosetta in silicon macs)
- BUG: baseline_mean/std are being recorded even when not used
- BUG: metrics are not well sorted yet
- FEAT: add support for fully recording last eval
- BUG: check that were are matching rlzoo mstop criteria
- Pong-RAM: Add support for byte-selection
MaxAndSkipEnv
- BUG: video step is still not alligned
- BUG: step 100 = reward 99
- FEAT: add vizdoom support
- FEAT: add stable-retro support
- Adjust eval policy for atari
	•	Normalize each byte (e.g., to [0,1]) and consider embedding bits (treat bytes as 8 bits).
	•	Try longer training and more seeds; RAM setups often need more steps to stabilize.
	•	If you can, expose extra emulator registers (RAM+) to reduce partial observability.
- How to measure advantage of different seeds versus reusing same env.
- Consider increasing sample efficiency by figuring out how different are transitions between different envs
- FEAT: add determinism check at beginning to make sure that rollout benefits from multiple envs (eg: Pong, test on PongDeterministic)
- FEAT: create cartpole reward shaper that prioritizes centering the pole
- FEAT: track output distribution
- BUG: fix thread safety issues with async eval collector (copy model weights with lock)
- FEAT: consider computing mean reward by timesteps, this way in eval we just have to request n_steps = reward_threshold * N, this will make it easier to support vectorized envs
- FEAT: a2c (only after reinforce/ppo is stable)
- FEAT: support for softcoding activations
- FEAT: add same linear decay features as rlzoo
- FEAT: add interactive mode support
- CHECK: run rollout through dataloader process, do we always get n_batches? assert it 
- add support for plotting charts as text and feeding to llm, check how end of training does it
- https://cs.stanford.edu/people/karpathy/reinforcejs/index.html
- https://alazareva.github.io/rl_playground/
- FEAT: log results to huggingface
- FEAT: Write wandb diagnostics script, use claude desktop to debug
- FEAT: support continuous environments
- FEAT: support for multi-env rollout collectors
- FEAT: add multitask heads support (eg: Atari, Sega Genesis) -- consider large output space
- Ask agent for next learning steps/tasks (prompt file)
- REFACTOR: add type hints where applicable