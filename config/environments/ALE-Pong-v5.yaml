# ALE/Pong-v5 configurations with explicit base envs and PPO overlays

# =====================
# Environment Specification
# =====================
spec: &spec
  source: https://ale.farama.org/environments/pong/
  description: Control right paddle; score when ball passes opponent, lose when it passes you.

  render_fps: 60

  action_space:
    discrete: 6
    labels: {0: NOOP, 1: FIRE, 2: RIGHT, 3: LEFT, 4: RIGHTFIRE, 5: LEFTFIRE}
    full_space: {count: 18, enable_with: full_action_space=True}

  observation_space:
    default: state
    variants:
      state:
        shape: [15]
        dtype: float32
        components:
          - {name: Player.y, range: [-1.0, 1.0]}
          - {name: Player.dy, range: [-1.0, 1.0]}
          - {name: Enemy.y, range: [-1.0, 1.0]}
          - {name: Enemy.dy, range: [-1.0, 1.0]}
          - {name: Ball.x, range: [-1.0, 1.0]}
          - {name: Ball.y, range: [-1.0, 1.0]}
          - {name: Ball.dx, range: [-1.0, 1.0]}
          - {name: Ball.dy, range: [-1.0, 1.0]}
          - {name: Ball.visible, range: [-1.0, 1.0]}
          - {name: LastAction.NOOP, range: [0.0, 1.0]}
          - {name: LastAction.FIRE, range: [0.0, 1.0]}
          - {name: LastAction.RIGHT, range: [0.0, 1.0]}
          - {name: LastAction.LEFT, range: [0.0, 1.0]}
          - {name: LastAction.RIGHTFIRE, range: [0.0, 1.0]}
          - {name: LastAction.LEFTFIRE, range: [0.0, 1.0]}

  rewards:
    per_point: +1/-1
    range: [-1, 1]

  returns:
    episodic: score difference (player score - opponent score)
    range: [-21, 21]
    threshold_solved: 18

  modes:
    values: [0, 1]
    default: 0

  difficulties:
    values: [0, 1, 2, 3]
    default: 0

# =====================
# Spec Anchors
# =====================
_spec_objects: &spec_objects
  <<: *spec
  action_space:
    discrete: 3
    labels: {0: NOOP, 1: RIGHT, 2: LEFT}
    note: Subset via DiscreteActionSpaceRemapperWrapper mapping [0, 2, 3]
  observation_space:
    default: state
    variants:
      state:
        shape: [12]
        dtype: float32
        components:
          - {name: Player.y, range: [-1.0, 1.0]}
          - {name: Player.dy, range: [-1.0, 1.0]}
          - {name: Enemy.y, range: [-1.0, 1.0]}
          - {name: Enemy.dy, range: [-1.0, 1.0]}
          - {name: Ball.x, range: [-1.0, 1.0]}
          - {name: Ball.y, range: [-1.0, 1.0]}
          - {name: Ball.dx, range: [-1.0, 1.0]}
          - {name: Ball.dy, range: [-1.0, 1.0]}
          - {name: Ball.visible, range: [-1.0, 1.0]}
          - {name: LastAction.NOOP, range: [0.0, 1.0]}
          - {name: LastAction.RIGHT, range: [0.0, 1.0]}
          - {name: LastAction.LEFT, range: [0.0, 1.0]}

_spec_rgb: &spec_rgb
  <<: *spec
  observation_space:
    default: rgb
    variants:
      rgb:
        shape: [3, 210, 160]
        dtype: uint8
        note: RGB frames from ALE (channel-first format)

# =====================
# Configuration Anchors
# =====================

# Shared environment id
_env: &env
  env_id: ALE/Pong-v5

# ==========================
# Object observation variants
# ==========================
_obj_env: &obj_env
  <<: *env
  spec: *spec_objects
  obs_type: "objects"
  env_wrappers:
    - { id: PongV5_FeatureExtractor, action_ids: [0, 2, 3] }
    - { id: DiscreteActionSpaceRemapperWrapper, mapping: [0, 2, 3] }

# Object-based observations with PPO - uses extracted game objects as features
objects_ppo: &objects_ppo
  <<: *obj_env
  algo_id: ppo
  description: "PPO with object-based observations using extracted game objects as features"
  n_epochs: 15
  eval_warmup_epochs: 200
  eval_freq_epochs: 5
  eval_episodes: 10
  ent_coef: 0.005
  policy_lr: 0.002
  n_steps: 256

# Object-based observations with PPO - deterministic version (no sticky actions)
objects_deterministic_ppo:
  <<: *objects_ppo
  description: "PPO with object-based observations - deterministic version (no sticky actions)"
  env_kwargs:
    repeat_action_probability: 0.0
  early_stop_on_train_threshold: 20.0  # Override env spec threshold (18) with custom value
  eval_freq_epochs: null # Disable evaluation during training

# ==========================
# RGB observation variants
# ==========================
_rgb_env: &rgb_env
  <<: *env
  spec: *spec_rgb
  obs_type: "rgb"

# RGB observations with PPO - uses ALE atari vectorization
# ALE atari vec automatically applies: 4-frame stacking, grayscale, 84x84 resize
# Observation shape: (4, 84, 84) - 4 grayscale frames stacked, NOT 3-channel RGB
# Config frame_stack/grayscale_obs/resize_obs reflect these Atari defaults
rgb_ppo: &rgb_ppo
  <<: *rgb_env
  algo_id: ppo
  description: "PPO with 4-frame grayscale stacks (84x84) via ALE atari vectorization"
  max_env_steps: 10_000_000
  n_epochs: 15
  ent_coef: {start: 0.02, end: 0.001}  # linear decay from 0.02 to 0.001 over full training
  policy_lr: 0.0025
  clip_range: 0.1
  n_steps: 128
  eval_warmup_epochs: 200
  eval_freq_epochs: 5
  eval_episodes: 10

# RGB observations with PPO - deterministic version (no sticky actions)
# Uses ALE atari vectorization for 10x faster training
rgb_deterministic_ppo:
  <<: *rgb_ppo
  description: "PPO with RGB observations - deterministic version with ALE atari vectorization"
  env_kwargs:
    repeat_action_probability: 0.0
