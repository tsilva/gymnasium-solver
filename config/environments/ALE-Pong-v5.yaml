# ALE/Pong-v5 unified configurations (rgb, ram, objects)

# Common env id anchor
_env: &env
  env_id: ALE/Pong-v5

# =====================
# RGB observation setup
# =====================
_rgb_base: &rgb_base
  <<: *env
  obs_type: "rgb"
  policy: 'cnn'
  # CNN defaults suitable for 84x84 inputs
  policy_kwargs:
    channels: [32, 64, 64]
    kernel_sizes: [8, 4, 3]
    strides: [4, 2, 1]
  grayscale_obs: true
  resize_obs: true
  frame_stack: 4
  accelerator: 'gpu'
  devices: 'auto'
  eval_freq_epochs: 1
  eval_episodes: 1

_rgb_ppo_base: &rgb_ppo_base
  <<: *rgb_base
  algo_id: ppo
  max_timesteps: 5e6
  n_envs: 8
  n_steps: 256             # 8*256 = 2048 samples/update
  batch_size: 1024         # 2k rollout â†’ 2 minibatches of 1024
  n_epochs: 3
  clip_range: 0.1
  policy_lr: 1e-4          # Slightly lower LR for stability on pixels
  frame_stack: 2           # Improve temporal information (typical Atari default)
  eval_freq_epochs: 10     # Less frequent eval due to computational cost
  eval_episodes: 5

# =====================
# RAM observation setup
# =====================
_ram_base: &ram_base
  <<: *env
  obs_type: "ram"
  normalize_obs: "static"

_ram_env_deterministic: &ram_env_deterministic
  <<: *ram_base
  env_kwargs:
    repeat_action_probability: 0.0  # Makes the environment deterministic

_ram_ppo_base: &ram_ppo_base
  <<: *ram_base
  algo_id: ppo
  max_timesteps: 5e6
  n_envs: 8               # Reduced for better sample efficiency
  n_steps: 512            # Reduced for more frequent updates
  batch_size: 512         # Larger batches for stable gradients
  n_epochs: 10            # More epochs to better utilize data
  policy_lr: 0.0003       # Higher learning rate for faster learning
  eval_freq_epochs: 50    # Less frequent eval to focus on training
  eval_episodes: 10       # More episodes for reliable evaluation

# ==========================
# Object observation setup
# ==========================
_obj_base: &obj_base
  <<: *env
  obs_type: "objects"
  hidden_dims: [256, 256]
  eval_warmup_epochs: 100
  eval_freq_epochs: 10 
  eval_episodes: 10  
  gamma: 0.99
  gae_lambda: 0.95
  env_wrappers:
    # Extract paddle/ball positions and velocities into compact normalized vector
    - id: PongV5_FeatureExtractor

_obj_env_deterministic: &obj_env_deterministic
  <<: *obj_base
  env_kwargs:
    repeat_action_probability: 0.0  # Makes the environment deterministic

# Compose the full wrapper stack for objects (feature extractor + remap + shaped reward)
_obj_wrappers_full: &obj_wrappers_full
  - { id: PongV5_FeatureExtractor }
  - { id: DiscreteActionSpaceRemapperWrapper, mapping: [0, 2, 3] }
  - { id: ActionRewardShaper, rewards: [0.001, 0.0, 0.0] }

_obj_current: &obj_current
  <<: *obj_env_deterministic
  env_wrappers: *obj_wrappers_full

_obj_ppo_base: &obj_ppo_base
  algo_id: ppo
  max_timesteps: 2e5
  n_envs: 32
  subproc: true
  vf_coef: 0.5
  n_epochs: 15
  n_steps: 256
  batch_size: 0.25 
  ent_coef: 0.005
  clip_range: 0.2
  policy_lr: 2e-3
  normalize_advantages: "rollout"

# =====================
# Public variants
# =====================

# RGB variants
ppo_rgb:
  <<: *rgb_ppo_base

# RAM variants
ppo_ram:
  <<: *ram_ppo_base

ppo_deterministic_ram:
  <<: [*ram_env_deterministic, *ram_ppo_base]

# Objects variants
ppo_objects:
  <<: [*obj_current, *obj_ppo_base]

