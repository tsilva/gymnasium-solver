# ALE/Pong-v5 configurations with explicit base envs and PPO overlays

# =====================
# environment specs
# =====================

# Base spec
spec: &spec
  source: https://ale.farama.org/environments/pong/
  description: Control right paddle; score when ball passes opponent, lose when it passes you.

  render_fps: 60

  action_space:
    discrete: 18
    valid: [0, 1, 3, 4, 11, 12]  # NOOP, FIRE, RIGHT, LEFT, RIGHTFIRE, LEFTFIRE
    labels:
      0: NOOP
      1: FIRE
      2: UP
      3: RIGHT
      4: LEFT
      5: DOWN
      6: UPRIGHT
      7: UPLEFT
      8: DOWNRIGHT
      9: DOWNLEFT
      10: UPFIRE
      11: RIGHTFIRE
      12: LEFTFIRE
      13: DOWNFIRE
      14: UPRIGHTFIRE
      15: UPLEFTFIRE
      16: DOWNRIGHTFIRE
      17: DOWNLEFTFIRE
    note: "Uses full ALE action space (18 actions) with masking. Only 6 actions are meaningful for Pong."

  observation_space:
    default: state
    variants:
      state:
        shape: [15]
        dtype: float32
        components:
          - {name: Player.y, range: [-1.0, 1.0]}
          - {name: Player.dy, range: [-1.0, 1.0]}
          - {name: Enemy.y, range: [-1.0, 1.0]}
          - {name: Enemy.dy, range: [-1.0, 1.0]}
          - {name: Ball.x, range: [-1.0, 1.0]}
          - {name: Ball.y, range: [-1.0, 1.0]}
          - {name: Ball.dx, range: [-1.0, 1.0]}
          - {name: Ball.dy, range: [-1.0, 1.0]}
          - {name: Ball.visible, range: [-1.0, 1.0]}
          - {name: LastAction.NOOP, range: [0.0, 1.0]}
          - {name: LastAction.FIRE, range: [0.0, 1.0]}
          - {name: LastAction.RIGHT, range: [0.0, 1.0]}
          - {name: LastAction.LEFT, range: [0.0, 1.0]}
          - {name: LastAction.RIGHTFIRE, range: [0.0, 1.0]}
          - {name: LastAction.LEFTFIRE, range: [0.0, 1.0]}

  rewards:
    per_point: +1/-1
    range: [-1, 1]

  returns:
    episodic: score difference (player score - opponent score)
    range: [-21, 21]
    threshold_solved: 18

  modes:
    values: [0, 1]
    default: 0

  difficulties:
    values: [0, 1, 2, 3]
    default: 0

# RGB spec
_spec_rgb: &spec_rgb
  <<: *spec
  action_space:
    discrete: 18
    labels:
      0: NOOP
      1: FIRE
      2: UP
      3: RIGHT
      4: LEFT
      5: DOWN
      6: UPRIGHT
      7: UPLEFT
      8: DOWNRIGHT
      9: DOWNLEFT
      10: UPFIRE
      11: RIGHTFIRE
      12: LEFTFIRE
      13: DOWNFIRE
      14: UPRIGHTFIRE
      15: UPLEFTFIRE
      16: DOWNRIGHTFIRE
      17: DOWNLEFTFIRE
    valid: [0, 3, 4]  # NOOP, RIGHT, LEFT
  observation_space:
    default: rgb
    variants:
      rgb:
        shape: [3, 210, 160]
        dtype: uint8
        note: RGB frames from ALE (channel-first format)

# Objects spec
_spec_objects: &spec_objects
  <<: *spec
  action_space:
    discrete: 18
    valid: [0, 3, 4]  # NOOP, RIGHT, LEFT (simplified for object-based learning)
    labels:
      0: NOOP
      1: FIRE
      2: UP
      3: RIGHT
      4: LEFT
      5: DOWN
      6: UPRIGHT
      7: UPLEFT
      8: DOWNRIGHT
      9: DOWNLEFT
      10: UPFIRE
      11: RIGHTFIRE
      12: LEFTFIRE
      13: DOWNFIRE
      14: UPRIGHTFIRE
      15: UPLEFTFIRE
      16: DOWNRIGHTFIRE
      17: DOWNLEFTFIRE
    note: "Uses full ALE action space (18 actions) with masking. Only 3 actions used for object-based learning."
  observation_space:
    default: state
    variants:
      state:
        shape: [12]
        dtype: float32
        components:
          - {name: Player.y, range: [-1.0, 1.0]}
          - {name: Player.dy, range: [-1.0, 1.0]}
          - {name: Enemy.y, range: [-1.0, 1.0]}
          - {name: Enemy.dy, range: [-1.0, 1.0]}
          - {name: Ball.x, range: [-1.0, 1.0]}
          - {name: Ball.y, range: [-1.0, 1.0]}
          - {name: Ball.dx, range: [-1.0, 1.0]}
          - {name: Ball.dy, range: [-1.0, 1.0]}
          - {name: Ball.visible, range: [-1.0, 1.0]}
          - {name: LastAction.NOOP, range: [0.0, 1.0]}
          - {name: LastAction.RIGHT, range: [0.0, 1.0]}
          - {name: LastAction.LEFT, range: [0.0, 1.0]}
        note: "Action indices updated for full action space: NOOP=0, RIGHT=3, LEFT=4"

# =====================
# env variants
# =====================

# Base env
_env: &env
  spec: *spec
  env_id: ALE/Pong-v5
  max_env_steps: 5e6

# Deterministic env
_env_deterministic: &env_deterministic
  <<: *env
  env_kwargs:
    repeat_action_probability: 0.0

# RGB env
_env_rgb: &env_rgb
  <<: *env
  spec: *spec_rgb
  obs_type: "rgb"

# Objects env
_env_objects: &env_objects
  <<: *env
  spec: *spec_objects
  obs_type: "objects"
  accelerator: "cpu"
  env_wrappers:
    - { id: PongV5_FeatureExtractor, action_ids: [0, 3, 4] }  # Updated for full action space indices

###########################
# CONFIGS: obs_type = objects
###########################

# NOTE: this seems to be working quite well, but agent is still overconfident with noops
objects_ppo: &objects_ppo
  <<: *env_objects
  project_id: ALE-Pong-v5_objects
  algo_id: ppo
  model_id: mlp_small
  description: "PPO with object-based observations using extracted game objects as features"
  early_stop_on_eval_threshold: 19.0
  max_env_steps: 3_500_000  # Extended for convergence

  n_envs: 32
  n_steps: 1024      # Increased rollout length for better value estimates
  batch_size: 2048   # Larger batches for more stable updates
  n_epochs: 8        # Fewer passes to prevent overfitting

  # All schedules now work (config loading bug fixed)
  policy_lr:
    start: 0.0055   # Strong initial learning
    end: 0.00015    # Fine-tuning at end
    from: 0.0       # Decay from start
    to: 0.92        # Complete by epoch ~125

  ent_coef:
    start: 0.04     # HIGH exploration during discovery (epochs 0-65)
    end: 0.0001     # VERY LOW for pure exploitation (epochs 90+)
    from: 0.48      # Keep high through peak learning phase
    to: 0.65        # RAPID decay after peak to force convergence

  clip_range:
    start: 0.3      # Aggressive initial updates
    end: 0.08       # Tight final clipping
    from: 0.5       # Start decay after peak learning
    to: 0.85        # Gradual tightening

  gae_lambda: 0.96  # Higher for better long-term credit assignment
  gamma: 0.995      # Increased for better episode-level optimization
  eval_async: true
  eval_warmup_epochs: 50
  eval_freq_epochs: 5
  eval_episodes: 10

# Converges in ~1.5M steps
objects_deterministic_ppo:
  <<: *env_objects
  <<: *env_deterministic
  project_id: ALE-Pong-v5_objects_deterministic
  spec: *spec_objects  # Override to use objects spec instead of base spec
  description: "PPO with object-based observations - deterministic version (no sticky actions)"
  max_env_steps: 1_500_000

  algo_id: ppo
  model_id: mlp_tiny
  early_stop_on_eval_threshold: 19.0

  ent_coef: 0.0
  clip_range: 0.1 # TODO: wider clip range?
  policy_lr: 0.01

  n_envs: 32
  n_steps: 1024
  batch_size: 2048
  eval_async: true
  eval_warmup_epochs: 20
  eval_freq_epochs: 5
  eval_episodes: 10

###########################
# CONFIGS: obs_type = rgb
###########################

rgb_ppo: &rgb_ppo
  <<: *env_rgb
  project_id: ALE-Pong-v5_rgb
  algo_id: ppo
  model_id: cnn_nature
  description: "PPO with 4-frame grayscale stacks (84x84) via ALE atari vectorization"
  accelerator: "gpu"
  n_envs: 32
  n_steps: 256
  batch_size: 1024
  n_epochs: 15
  policy_lr: 3e-4
  ent_coef: 0.0
  eval_async: true
  eval_warmup_epochs: 100
  eval_freq_epochs: 5
  eval_episodes: 10

rgb_deterministic_ppo:
  <<: *rgb_ppo
  <<: *env_deterministic
  project_id: ALE-Pong-v5_rgb_deterministic
  description: "PPO with RGB observations - deterministic version with ALE atari vectorization"
