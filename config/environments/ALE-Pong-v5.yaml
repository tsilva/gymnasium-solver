# ALE/Pong-v5 configurations with explicit base envs and PPO overlays

# =====================
# Environment Specification
# =====================
spec: &spec_base
  source: https://ale.farama.org/environments/pong/
  description: Control right paddle; score when ball passes opponent, lose when it passes you.

  render_fps: 60

  action_space:
    discrete: 6
    labels: {0: NOOP, 1: FIRE, 2: RIGHT, 3: LEFT, 4: RIGHTFIRE, 5: LEFTFIRE}
    full_space: {count: 18, enable_with: full_action_space=True}

  observation_space:
    default: state
    variants:
      state:
        shape: [15]
        dtype: float32
        components:
          - {name: Player.y, range: [-1.0, 1.0]}
          - {name: Player.dy, range: [-1.0, 1.0]}
          - {name: Enemy.y, range: [-1.0, 1.0]}
          - {name: Enemy.dy, range: [-1.0, 1.0]}
          - {name: Ball.x, range: [-1.0, 1.0]}
          - {name: Ball.y, range: [-1.0, 1.0]}
          - {name: Ball.dx, range: [-1.0, 1.0]}
          - {name: Ball.dy, range: [-1.0, 1.0]}
          - {name: Ball.visible, range: [-1.0, 1.0]}
          - {name: LastAction.NOOP, range: [0.0, 1.0]}
          - {name: LastAction.FIRE, range: [0.0, 1.0]}
          - {name: LastAction.RIGHT, range: [0.0, 1.0]}
          - {name: LastAction.LEFT, range: [0.0, 1.0]}
          - {name: LastAction.RIGHTFIRE, range: [0.0, 1.0]}
          - {name: LastAction.LEFTFIRE, range: [0.0, 1.0]}

  rewards:
    per_point: +1/-1
    range: [-1, 1]

  returns:
    episodic: score difference (player score - opponent score)
    range: [-21, 21]
    threshold_solved: 19

  modes:
    values: [0, 1]
    default: 0

  difficulties:
    values: [0, 1, 2, 3]
    default: 0

# =====================
# Spec Anchors
# =====================
_spec_objects: &spec_objects
  <<: *spec_base
  action_space:
    discrete: 3
    labels: {0: NOOP, 1: RIGHT, 2: LEFT}
    note: Subset via DiscreteActionSpaceRemapperWrapper mapping [0, 2, 3]
  observation_space:
    default: state
    variants:
      state:
        shape: [12]
        dtype: float32
        components:
          - {name: Player.y, range: [-1.0, 1.0]}
          - {name: Player.dy, range: [-1.0, 1.0]}
          - {name: Enemy.y, range: [-1.0, 1.0]}
          - {name: Enemy.dy, range: [-1.0, 1.0]}
          - {name: Ball.x, range: [-1.0, 1.0]}
          - {name: Ball.y, range: [-1.0, 1.0]}
          - {name: Ball.dx, range: [-1.0, 1.0]}
          - {name: Ball.dy, range: [-1.0, 1.0]}
          - {name: Ball.visible, range: [-1.0, 1.0]}
          - {name: LastAction.NOOP, range: [0.0, 1.0]}
          - {name: LastAction.RIGHT, range: [0.0, 1.0]}
          - {name: LastAction.LEFT, range: [0.0, 1.0]}

_spec_rgb: &spec_rgb
  <<: *spec_base
  observation_space:
    default: rgb
    variants:
      rgb:
        shape: [3, 210, 160]
        dtype: uint8
        note: RGB frames from ALE (channel-first format)

# =====================
# Configuration Anchors
# =====================

# Shared environment id
_env: &env
  env_id: ALE/Pong-v5

# Makes ALE deterministic by disabling sticky actions
_env_deterministic: &env_deterministic
  env_kwargs:
    repeat_action_probability: 0.0

# ==========================
# Object observation setup
# ==========================
_obj_env: &obj_env
  <<: *env
  obs_type: "objects"
  env_wrappers:
    - { id: PongV5_FeatureExtractor, action_ids: [0, 2, 3] }
    - { id: DiscreteActionSpaceRemapperWrapper, mapping: [0, 2, 3] }

_obj_ppo: &obj_ppo
  <<: [*obj_env]
  n_epochs: 15
  ent_coef: 0.005
  policy_lr: 0.002
  normalize_advantages: "rollout"
  algo_id: ppo
  gamma: 0.99
  gae_lambda: 0.95
  vf_coef: 0.5
  clip_range: 0.2
  n_envs: 32
  subproc: true
  n_steps: 256
  batch_size: 2048
  hidden_dims: [256, 256]
  eval_warmup_epochs: 100
  eval_episodes: 10
  eval_freq_epochs: 10

# ==========================
# RGB observation setup
# ==========================
_rgb_env: &rgb_env
  <<: *env
  obs_type: "rgb"
  grayscale_obs: true
  resize_obs: [84, 84]

_rgb_ppo: &rgb_ppo
  <<: [*rgb_env]
  policy: "cnn_actorcritic"
  accelerator: "auto"  # Use GPU if available, fall back to CPU
  n_epochs: 4
  ent_coef: 0.01
  policy_lr: 0.0001
  normalize_advantages: true
  algo_id: ppo
  gamma: 0.99
  gae_lambda: 0.95
  vf_coef: 0.5
  clip_range: 0.1
  n_envs: 8
  subproc: false
  n_steps: 128
  batch_size: 256
  hidden_dims: [512]
  policy_kwargs:
    channels: [32, 64, 64]
    kernel_sizes: [8, 4, 3]
    strides: [4, 2, 1]
  eval_warmup_epochs: 50
  eval_episodes: 10
  eval_freq_epochs: 10

# =====================
# Public variants
# =====================

# Object-based observations with PPO - uses extracted game objects as features
objects_ppo:
  <<: *obj_ppo
  description: "PPO with object-based observations using extracted game objects as features"

# Object-based observations with PPO - deterministic version (no sticky actions)
objects_deterministic_ppo:
  <<: [*obj_ppo, *env_deterministic]
  description: "PPO with object-based observations - deterministic version (no sticky actions)"

# RGB observations with PPO - uses raw pixel observations (84x84 grayscale)
# Uses ALE native vectorization for 10x faster training
rgb_ppo:
  <<: *rgb_ppo
  spec: *spec_rgb
  description: "PPO with RGB observations - uses ALE native vectorization for fast training"

# RGB observations with PPO - deterministic version (no sticky actions)
# Uses ALE native vectorization for 10x faster training
rgb_deterministic_ppo:
  <<: [*rgb_ppo, *env_deterministic]
  spec: *spec_rgb
  description: "PPO with RGB observations - deterministic version with ALE native vectorization"
