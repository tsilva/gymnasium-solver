# ALE/Pong-v5 configurations with explicit base envs and PPO overlays

# =====================
# environment specs
# =====================

# Base spec
spec: &spec
  source: https://ale.farama.org/environments/pong/
  description: Control right paddle; score when ball passes opponent, lose when it passes you.

  render_fps: 60

  action_space:
    discrete: 6
    labels: {0: NOOP, 1: FIRE, 2: RIGHT, 3: LEFT, 4: RIGHTFIRE, 5: LEFTFIRE}
    full_space: {count: 18, enable_with: full_action_space=True}

  observation_space:
    default: state
    variants:
      state:
        shape: [15]
        dtype: float32
        components:
          - {name: Player.y, range: [-1.0, 1.0]}
          - {name: Player.dy, range: [-1.0, 1.0]}
          - {name: Enemy.y, range: [-1.0, 1.0]}
          - {name: Enemy.dy, range: [-1.0, 1.0]}
          - {name: Ball.x, range: [-1.0, 1.0]}
          - {name: Ball.y, range: [-1.0, 1.0]}
          - {name: Ball.dx, range: [-1.0, 1.0]}
          - {name: Ball.dy, range: [-1.0, 1.0]}
          - {name: Ball.visible, range: [-1.0, 1.0]}
          - {name: LastAction.NOOP, range: [0.0, 1.0]}
          - {name: LastAction.FIRE, range: [0.0, 1.0]}
          - {name: LastAction.RIGHT, range: [0.0, 1.0]}
          - {name: LastAction.LEFT, range: [0.0, 1.0]}
          - {name: LastAction.RIGHTFIRE, range: [0.0, 1.0]}
          - {name: LastAction.LEFTFIRE, range: [0.0, 1.0]}

  rewards:
    per_point: +1/-1
    range: [-1, 1]

  returns:
    episodic: score difference (player score - opponent score)
    range: [-21, 21]
    threshold_solved: 18

  modes:
    values: [0, 1]
    default: 0

  difficulties:
    values: [0, 1, 2, 3]
    default: 0

# RGB spec
_spec_rgb: &spec_rgb
  <<: *spec
  observation_space:
    default: rgb
    variants:
      rgb:
        shape: [3, 210, 160]
        dtype: uint8
        note: RGB frames from ALE (channel-first format)

# Objects spec
_spec_objects: &spec_objects
  <<: *spec
  action_space:
    discrete: 3
    labels: {0: NOOP, 1: RIGHT, 2: LEFT}
    note: Subset via DiscreteActionSpaceRemapperWrapper mapping [0, 2, 3]
  observation_space:
    default: state
    variants:
      state:
        shape: [12]
        dtype: float32
        components:
          - {name: Player.y, range: [-1.0, 1.0]}
          - {name: Player.dy, range: [-1.0, 1.0]}
          - {name: Enemy.y, range: [-1.0, 1.0]}
          - {name: Enemy.dy, range: [-1.0, 1.0]}
          - {name: Ball.x, range: [-1.0, 1.0]}
          - {name: Ball.y, range: [-1.0, 1.0]}
          - {name: Ball.dx, range: [-1.0, 1.0]}
          - {name: Ball.dy, range: [-1.0, 1.0]}
          - {name: Ball.visible, range: [-1.0, 1.0]}
          - {name: LastAction.NOOP, range: [0.0, 1.0]}
          - {name: LastAction.RIGHT, range: [0.0, 1.0]}
          - {name: LastAction.LEFT, range: [0.0, 1.0]}

# =====================
# env variants
# =====================

# Base env
_env: &env
  spec: *spec
  env_id: ALE/Pong-v5

# Deterministic env
_env_deterministic: &env_deterministic
  <<: *env
  env_kwargs:
    repeat_action_probability: 0.0
  
# RGB env
_env_rgb: &env_rgb
  <<: *env
  spec: *spec_rgb
  obs_type: "rgb"

# Objects env
_env_objects: &env_objects
  <<: *env
  spec: *spec_objects
  obs_type: "objects"
  accelerator: "cpu"
  env_wrappers:
    - { id: PongV5_FeatureExtractor, action_ids: [0, 2, 3] }
    - { id: DiscreteActionSpaceRemapperWrapper, mapping: [0, 2, 3] }

###########################
# CONFIGS: obs_type = objects
###########################

objects_ppo: &objects_ppo
  <<: *env_objects
  algo_id: ppo
  description: "PPO with object-based observations using extracted game objects as features"
  eval_warmup_epochs: 200
  eval_freq_epochs: 50
  eval_episodes: 10
  #accelerator: "gpu"

# TODO: try with tanh
# TODO: try until convergence in eval over N episodes
objects_deterministic_ppo:
  <<: *objects_ppo
  <<: *env_deterministic
  spec: *spec_objects  # Override to use objects spec instead of base spec
  description: "PPO with object-based observations - deterministic version (no sticky actions)"
  early_stop_on_train_threshold: 19.0
  seed: 42 # Converged with this seed
  ent_coef: 0.0 # Deterministic, entropy not essential
  batch_size: 1024
  clip_range: 0.238
  n_envs: 32
  n_steps: 256
  policy_lr: 0.002
  hidden_dims: [64, 64]


  # Plateau intervention: automatically adjust hyperparameters when training stagnates
  # plateau_interventions:
  #   monitor: "train/roll/ep_rew/mean"
  #   patience: 5           # Epochs without improvement before intervention
  #   min_delta: 0.5        # Minimum change to count as improvement
  #   mode: "max"           # Maximize reward
  #   cooldown: 3           # Wait N epochs after intervention before trying again

  #   # Cycle through these interventions on repeated plateaus
  #   actions:
  #     - param: policy_lr
  #       operation: multiply
  #       factor: 0.5
  #       min: 1.0e-6
  #       revert_on_worse: true

  #     - param: ent_coef
  #       operation: multiply
  #       factor: 2.0
  #       max: 0.1
  #       revert_on_worse: true
  
###########################
# CONFIGS: obs_type = rgb
###########################

rgb_ppo: &rgb_ppo
  <<: *env_rgb
  algo_id: ppo
  description: "PPO with 4-frame grayscale stacks (84x84) via ALE atari vectorization"
  max_env_steps: 10_000_000
  n_epochs: 15
  ent_coef: 0.02
  policy_lr: 0.0025
  clip_range: 0.1
  n_steps: 512
  #eval_warmup_epochs: 200
  eval_freq_epochs: 2
  eval_episodes: 10
  batch_size: 0.25

rgb_deterministic_ppo:
  <<: *rgb_ppo
  <<: *env_deterministic
  description: "PPO with RGB observations - deterministic version with ALE atari vectorization"
  eval_freq_epochs: 5 # Disable evaluation during training
