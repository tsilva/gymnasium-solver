# ALE/Pong-v5 configurations with explicit base envs and PPO overlays

# =====================
# environment specs
# =====================

# Base spec
spec: &spec
  source: https://ale.farama.org/environments/pong/
  description: Control right paddle; score when ball passes opponent, lose when it passes you.

  render_fps: 60

  action_space:
    discrete: 6
    labels: {0: NOOP, 1: FIRE, 2: RIGHT, 3: LEFT, 4: RIGHTFIRE, 5: LEFTFIRE}
    full_space: {count: 18, enable_with: full_action_space=True}

  observation_space:
    default: state
    variants:
      state:
        shape: [15]
        dtype: float32
        components:
          - {name: Player.y, range: [-1.0, 1.0]}
          - {name: Player.dy, range: [-1.0, 1.0]}
          - {name: Enemy.y, range: [-1.0, 1.0]}
          - {name: Enemy.dy, range: [-1.0, 1.0]}
          - {name: Ball.x, range: [-1.0, 1.0]}
          - {name: Ball.y, range: [-1.0, 1.0]}
          - {name: Ball.dx, range: [-1.0, 1.0]}
          - {name: Ball.dy, range: [-1.0, 1.0]}
          - {name: Ball.visible, range: [-1.0, 1.0]}
          - {name: LastAction.NOOP, range: [0.0, 1.0]}
          - {name: LastAction.FIRE, range: [0.0, 1.0]}
          - {name: LastAction.RIGHT, range: [0.0, 1.0]}
          - {name: LastAction.LEFT, range: [0.0, 1.0]}
          - {name: LastAction.RIGHTFIRE, range: [0.0, 1.0]}
          - {name: LastAction.LEFTFIRE, range: [0.0, 1.0]}

  rewards:
    per_point: +1/-1
    range: [-1, 1]

  returns:
    episodic: score difference (player score - opponent score)
    range: [-21, 21]
    threshold_solved: 18

  modes:
    values: [0, 1]
    default: 0

  difficulties:
    values: [0, 1, 2, 3]
    default: 0

# RGB spec
_spec_rgb: &spec_rgb
  <<: *spec
  observation_space:
    default: rgb
    variants:
      rgb:
        shape: [3, 210, 160]
        dtype: uint8
        note: RGB frames from ALE (channel-first format)

# Objects spec
_spec_objects: &spec_objects
  <<: *spec
  action_space:
    discrete: 3
    labels: {0: NOOP, 1: RIGHT, 2: LEFT}
    note: Subset via DiscreteActionSpaceRemapperWrapper mapping [0, 2, 3]
  observation_space:
    default: state
    variants:
      state:
        shape: [12]
        dtype: float32
        components:
          - {name: Player.y, range: [-1.0, 1.0]}
          - {name: Player.dy, range: [-1.0, 1.0]}
          - {name: Enemy.y, range: [-1.0, 1.0]}
          - {name: Enemy.dy, range: [-1.0, 1.0]}
          - {name: Ball.x, range: [-1.0, 1.0]}
          - {name: Ball.y, range: [-1.0, 1.0]}
          - {name: Ball.dx, range: [-1.0, 1.0]}
          - {name: Ball.dy, range: [-1.0, 1.0]}
          - {name: Ball.visible, range: [-1.0, 1.0]}
          - {name: LastAction.NOOP, range: [0.0, 1.0]}
          - {name: LastAction.RIGHT, range: [0.0, 1.0]}
          - {name: LastAction.LEFT, range: [0.0, 1.0]}

# =====================
# env variants
# =====================

# Base env
_env: &env
  spec: *spec
  env_id: ALE/Pong-v5

# Deterministic env
_env_deterministic: &env_deterministic
  <<: *env
  env_kwargs:
    repeat_action_probability: 0.0
  
# RGB env
_env_rgb: &env_rgb
  <<: *env
  spec: *spec_rgb
  obs_type: "rgb"

# Objects env
_env_objects: &env_objects
  <<: *env
  spec: *spec_objects
  obs_type: "objects"
  env_wrappers:
    - { id: PongV5_FeatureExtractor, action_ids: [0, 2, 3] }
    - { id: DiscreteActionSpaceRemapperWrapper, mapping: [0, 2, 3] }

###########################
# CONFIGS: obs_type = objects
###########################

objects_ppo: &objects_ppo
  <<: *env_objects
  algo_id: ppo
  description: "PPO with object-based observations using extracted game objects as features"
  n_epochs: 15
  eval_warmup_epochs: 200
  eval_freq_epochs: 5
  eval_episodes: 10
  ent_coef: 0.005
  policy_lr: 0.002
  n_steps: 256

objects_deterministic_ppo:
  <<: *objects_ppo
  <<: *env_deterministic
  description: "PPO with object-based observations - deterministic version (no sticky actions)"
  early_stop_on_train_threshold: 20.0  # Override env spec threshold (18) with custom value
  eval_warmup_epochs: null
  eval_freq_epochs: 5 # Disable evaluation during training

###########################
# CONFIGS: obs_type = rgb
###########################

rgb_ppo: &rgb_ppo
  <<: *env_rgb
  algo_id: ppo
  description: "PPO with 4-frame grayscale stacks (84x84) via ALE atari vectorization"
  max_env_steps: 10_000_000
  n_epochs: 15
  ent_coef: 0.02
  policy_lr: 0.0025
  clip_range: 0.1
  n_steps: 512
  #eval_warmup_epochs: 200
  eval_freq_epochs: 2
  eval_episodes: 10
  batch_size: 0.25

rgb_deterministic_ppo:
  <<: *rgb_ppo
  <<: *env_deterministic
  description: "PPO with RGB observations - deterministic version with ALE atari vectorization"
  eval_freq_epochs: 5 # Disable evaluation during training
