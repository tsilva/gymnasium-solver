# Pong with object-based observations challenge configurations

_base: &base
  env_id: ALE/Pong-v5
  obs_type: "objects"
  hidden_dims: [256, 256]
  eval_warmup_epochs: 100
  eval_freq_epochs: 10 
  eval_episodes: 10  
  gamma: 0.99
  gae_lambda: 0.95
  env_wrappers:
    # Extract paddle/ball positions and velocities into compact normalized vector
    - id: PongV5_FeatureExtractor

_env_deterministic: &env_deterministic
  <<: *base
  env_kwargs:
    repeat_action_probability: 0.0  # Makes the environment deterministic

_env_remap_actions: &env_remap_actions
  <<: *base
  env_wrappers:
    - id: DiscreteActionSpaceRemapperWrapper
      mapping: [0, 2, 3]

_env_reward_noop: &env_reward_noop
  <<: *base
  env_wrappers:
    - id: ActionRewardShaper
      rewards: [0.0001, 0.0, 0.0]

_current_config: &_current_config
  <<: [*base, *env_remap_actions, *env_reward_noop, *env_deterministic]

ppo:
  <<: *_current_config
  algo_id: ppo
  max_timesteps: 2e5
  n_envs: 32
  subproc: true
  vf_coef: 0.5
  n_epochs: 15
  n_steps: 256
  batch_size: 0.25 
  ent_coef: 0.005
  #ent_coef_schedule: linear
  #ent_coef_schedule_target_value: 0.003
  #ent_coef_schedule_target_progress: 0.5
  clip_range: 0.2
  policy_lr: 2e-3 #4e-2 # TODO: approx_kl = inf if 1e-1
  normalize_advantages: "rollout"
  #target_kl: 0.02
