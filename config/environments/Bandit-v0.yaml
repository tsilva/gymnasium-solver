# =====================
# Environment Specification
# =====================
spec:
  source: local gym_envs.mab_env.MultiArmedBanditEnv
  description: Stateless multi-armed bandit; pull an arm to receive a stochastic reward.

  action_space:
    discrete: 10

  observation_space:
    default: constant_zero
    variants:
      constant_zero:
        shape: [n_arms]
        dtype: float32
        range: [0, 0]

  rewards:
    distribution: Gaussian per-arm; reward ~ N(means[i], stds[i])
    range: [-inf, inf]

  returns:
    episodic: reward per step (episode_length typically 1)
    range: [-inf, inf]


# =====================
# Configuration Anchors
# =====================

_env: &env
  env_id: Bandit-v0
  env_kwargs:
    n_arms: 10
    means: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
    stds: 1
    episode_length: 1
  max_timesteps: 20_000
  gamma: 1.0
  gae_lambda: 1.0
  n_envs: 16
  n_steps: 64
  batch_size: 0.25
  eval_warmup_epochs: 5
  eval_freq_epochs: 1

_ppo: &ppo
  <<: *env
  algo_id: ppo
  n_epochs: 4
  policy_lr: lin_4e-2
  clip_range: 0.2
  normalize_advantages: "rollout"

# PPO on Multi-Armed Bandit - standard configuration for bandit problem
ppo:
  <<: *ppo
  description: "PPO on Multi-Armed Bandit - standard configuration for bandit problem"
