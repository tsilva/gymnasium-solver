env_id: Bandit-v0
env_kwargs:
  n_arms: 10
  means: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
  stds: 1
  episode_length: 1
max_timesteps: 2e5
eval_episodes: 100
eval_freq_epochs: 10
early_stop_on_train_threshold: false
early_stop_on_eval_threshold: true

ppo:
  algo_id: ppo
  n_envs: 16
  n_steps: 64
  batch_size: 0.25
  n_epochs: 4
  policy_lr: 3e-3
  clip_range: 0.2
  gamma: 1.0           # single-step episodes
  gae_lambda: 1.0
  normalize_advantages: "rollout"
  
ppo_optimal:
  algo_id: ppo
  max_timesteps: 300_000

  # Bandit-friendly dynamics
  gamma: 0.0              # 1-step bandit â†’ no bootstrapping
  optimizer: adam         # AdamW not needed here
  policy_lr: 0.01         # bigger, faster updates
  clip_range: 0.35        # allow larger policy moves

  # Bigger low-variance batches
  n_envs: 64
  n_steps: 64             # 64 * 64 = 4096 samples/rollout
  batch_size: 4096
  n_epochs: 8             # a few strong passes per batch

  # Stabilizers
  ent_coef: 0.02          # explore early; small but nonzero
  vf_coef: 0.05           # tiny baseline to cut variance
  max_grad_norm: 1.0

  # Evaluation
  eval_freq_epochs: 5
  eval_episodes: 200