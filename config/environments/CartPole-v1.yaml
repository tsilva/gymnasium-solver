# CartPole-v1 configurations covering classic state-based, reward-shaped, and pixel observations

# =====================
# Environment Specification
# =====================
spec: &spec
  source: https://gymnasium.farama.org/environments/classic_control/cart_pole/
  description: Balance a pole on a moving cart by applying forces to the cart.

  action_space:
    discrete: 2
    labels: {0: push_left, 1: push_right}

  observation_space:
    default: state
    variants:
      state:
        shape: [4]
        dtype: float32
        components:
          - {name: cart_position, range: [-4.8, 4.8]}
          - {name: cart_velocity, range: [-inf, inf]}
          - {name: pole_angle_rad, range: [-0.418, 0.418]}
          - {name: pole_angular_velocity, range: [-inf, inf]}

  rewards:
    per_step: +1
    range: [0, 1]

  returns:
    episodic: number of steps balanced (max 500)
    range: [0, 500]
    threshold_solved: 475


# =====================
# Configuration Anchors
# =====================

_env: &env
  spec: *spec
  env_id: CartPole-v1

ppo: &ppo
  <<: *env
  description: "Standard PPO on state observations - baseline configuration for CartPole"
  algo_id: ppo
  eval_freq_epochs: 10
  eval_episodes: 100
  
ppo_tuned:
  <<: *ppo
  description: "PPO on CartPole - tuned configuration"
  hidden_dims: [64, 64]
  batch_size: 0.25
  n_steps: 32
  n_epochs: 20
  hidden_dims: [64, 64]
  policy_lr: 1e-3
  n_steps: 32
  n_epochs: 20
  gae_lambda: 0.8
  batch_size: 0.25

_env_rewardshaped: &env_rewardshaped
  <<: *env
  env_wrappers:
    - id: CartPoleV1_RewardShaper
      angle_reward_scale: 1.0
      position_reward_scale: 0.25
      clip_potential: true

# PPO with potential-based reward shaping for faster learning
ppo_rewardshaped:
  <<: [*ppo, *env_rewardshaped]
  description: "PPO with potential-based reward shaping (angle + position terms)"
