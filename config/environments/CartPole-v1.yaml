
env_id: CartPole-v1
gamma: 1.0
ent_coef: 0.01
eval_episodes: 100
normalize_obs: False # Normalization doesn't seem to help

reinforce_vanilla:
  algo_id: reinforce
  max_timesteps: 3e5
  n_envs: 8
  n_steps: 1024
  batch_size: 2048
  policy_lr: 6e-3
  eval_freq_epochs: 5
  returns_type: "mc:episode"
  reinforce_policy_targets: "returns"

reinforce_baseline:
  algo_id: reinforce
  max_timesteps: 3e5
  n_envs: 8
  n_steps: 1024
  batch_size: 2048
  policy_lr: 6e-3
  eval_freq_epochs: 5
  returns_type: "mc:episode"
  reinforce_policy_targets: "advantages"
  advantages_type: "baseline"

reinforce_baseline_rewardtogo:
  algo_id: reinforce
  max_timesteps: 3e5
  n_envs: 8
  n_steps: 1024
  batch_size: 2048
  policy_lr: 6e-3
  eval_freq_epochs: 5
  returns_type: "mc:rtg"
  reinforce_policy_targets: "returns"

# TUNED: 13 epochs (122880 timesteps)
# TODO: eval wramup
# TODO: hidden_dims
reinforce:
  algo_id: reinforce
  max_timesteps: 3e5
  n_envs: 8
  n_steps: 1024
  batch_size: 2048
  policy_lr: 6e-3
  eval_freq_epochs: 5
  frame_stack: 2 # Converges faster with frame stacking = 2, probably because it allows to estimate momentum better?
  returns_type: "episode"
  reinforce_policy_targets: "returns"
  #normalize_advantages: "batch"

ppo:
  #seed: 100 # TODO: seed setting does not seem to be doing anything
  algo_id: ppo
  n_envs: 8
  n_steps: 32
  batch_size: 256
  n_epochs: 20
  policy_lr: lin_0.001
  gamma: 0.98
  gae_lambda: 0.8
  clip_range: lin_0.2
  ent_coef: 0.0 
  eval_freq_epochs: 100
  #frame_stack: 2 # TODO: not converging faster with frame stacking here, why?