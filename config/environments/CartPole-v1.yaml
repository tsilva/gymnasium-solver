# CartPole-v1 configurations covering classic state-based, reward-shaped, and pixel observations

# =====================
# Environment Specification
# =====================
spec: &spec
  source: https://gymnasium.farama.org/environments/classic_control/cart_pole/
  description: Balance a pole on a moving cart by applying forces to the cart.

  action_space:
    discrete: 2
    labels: {0: push_left, 1: push_right}

  observation_space:
    default: state
    variants:
      state:
        shape: [4]
        dtype: float32
        components:
          - {name: cart_position, range: [-4.8, 4.8]}
          - {name: cart_velocity, range: [-inf, inf]}
          - {name: pole_angle_rad, range: [-0.418, 0.418]}
          - {name: pole_angular_velocity, range: [-inf, inf]}

  rewards:
    per_step: +1
    range: [0, 1]

  returns:
    episodic: number of steps balanced (max 500)
    range: [0, 500]
    threshold_solved: 475


# =====================
# env variants
# =====================

# Base env
_env: &env
  spec: *spec
  env_id: CartPole-v1
  accelerator: cpu

# Reward-shaped env
_env_rewardshaped: &env_rewardshaped
  <<: *env
  env_wrappers:
    - id: CartPoleV1_RewardShaper
      angle_reward_scale: 1.0
      position_reward_scale: 0.25
      clip_potential: true

# =====================
# configs
# =====================

# TODO: bug: not all plots are keyed under same axis
ppo: &ppo
  <<: *env
  project_id: CartPole-v1
  description: "Standard PPO on state observations - baseline configuration for CartPole"
  algo_id: ppo
  model_id: mlp_medium

  eval_warmup_epochs: 50
  eval_freq_epochs: 10
  eval_episodes: 10

  max_env_steps: 1e5
  n_envs: 8
  n_steps: 32
  batch_size: 256
  gae_lambda: 0.8
  gamma: 0.98
  n_epochs: 20
  ent_coef: 0.0

  policy_lr: 0.001
  clip_range: 0.1

ppo_rewardshaped:
  <<: *ppo
  <<: *env_rewardshaped
  project_id: CartPole-v1_rewardshaped
  description: "PPO with potential-based reward shaping (angle + position terms)"

ppo_transfer_test:
  <<: *ppo
  description: "PPO initialized from another run (for testing transfer learning)"
  # init_from_run: "abc123"           # Use @best if available, else @last
  # init_from_run: "abc123/@best"     # Explicitly use @best checkpoint
  # init_from_run: "abc123/@last"     # Explicitly use @last checkpoint
  # init_from_run: "abc123/epoch=13"  # Use specific epoch checkpoint
  # init_from_run: "@last/@best"      # Most recent run, @best checkpoint
  max_env_steps: 1000
