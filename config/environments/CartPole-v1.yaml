# CartPole-v1 configurations covering classic state-based, reward-shaped, and pixel observations

# =====================
# Environment Specification
# =====================
spec: &spec
  source: https://gymnasium.farama.org/environments/classic_control/cart_pole/
  description: Balance a pole on a moving cart by applying forces to the cart.

  action_space:
    discrete: 2
    labels: {0: push_left, 1: push_right}

  observation_space:
    default: state
    variants:
      state:
        shape: [4]
        dtype: float32
        components:
          - {name: cart_position, range: [-4.8, 4.8]}
          - {name: cart_velocity, range: [-inf, inf]}
          - {name: pole_angle_rad, range: [-0.418, 0.418]}
          - {name: pole_angular_velocity, range: [-inf, inf]}

  rewards:
    per_step: +1
    range: [0, 1]

  returns:
    episodic: number of steps balanced (max 500)
    range: [0, 500]
    threshold_solved: 475


# =====================
# env variants
# =====================

# Base env
_env: &env
  spec: *spec
  env_id: CartPole-v1
  accelerator: cpu

# Reward-shaped env
_env_rewardshaped: &env_rewardshaped
  <<: *env
  env_wrappers:
    - id: CartPoleV1_RewardShaper
      angle_reward_scale: 1.0
      position_reward_scale: 0.25
      clip_potential: true

# =====================
# configs
# =====================

ppo: &ppo
  <<: *env
  project_id: CartPole-v1
  description: "Standard PPO on state observations - baseline configuration for CartPole"
  algo_id: ppo
  eval_warmup_epochs: 10
  eval_freq_epochs: 1
  eval_episodes: 100
  policy_lr: 3e-3
  batch_size: 0.5
  #n_envs: 18 # auto
  #hidden_dims: [256, 256] # [64, 64]
  #ent_coef: 0
  #clip_range: 0.1
  #batch_size: 0.25
  #n_steps: 2048 #32
  #n_epochs: 10
  #policy_lr: 1e-3
  #gae_lambda: 0.8

ppo_rewardshaped:
  <<: *ppo
  <<: *env_rewardshaped
  project_id: CartPole-v1_rewardshaped
  description: "PPO with potential-based reward shaping (angle + position terms)"
