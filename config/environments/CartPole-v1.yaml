
seed: 42 # TODO: try different seed
env_id: CartPole-v1
algo_id: reinforce
max_timesteps: 3e5
normalize_obs: False # Normalization doesn't seem to help
n_envs: 8
n_steps: 1024
policy_lr: 6e-3
hidden_dims: [64, 64] # TODO: try changing this
gamma: 0.99 # TODO: try increasing
ent_coef: 0.0 # TODO: try increasing
frame_stack: 1 # TODO: try changing this
# TODO: try deterministic eval
# TODO: monitor gradients
eval_freq_epochs: 5
eval_episodes: 100

# Unsolved: 300k timesteps
reinforce_vanilla:
  algo_id: reinforce
  returns_type: "mc:episode"
  policy_targets: "returns"

# Solved: 82k timesteps
reinforce_rtg:
  algo_id: reinforce
  returns_type: "mc:rtg"
  policy_targets: "returns"
  normalize_returns: "batch"

reinforce_baseline:
  algo_id: reinforce
  returns_type: "mc:episode"
  advantages_type: "baseline"
  policy_targets: "advantages"
  normalize_advantages: "batch"

# TODO: same as using returns... investigate
# Solved: 82k timesteps
reinforce_baseline_rtg:
  algo_id: reinforce
  returns_type: "mc:rtg"
  advantages_type: "baseline"
  policy_targets: "advantages"
  normalize_advantages: "batch"

# Tuned: n_envs=8, n_steps=32 => total_timesteps=15k
ppo:
  algo_id: ppo
  max_timesteps: 50000
  n_envs: 8
  n_steps: 32
  batch_size: 0.25
  n_epochs: 20
  gae_lambda: 0.8
  clip_range: 0.2
  policy_lr: 1e-3 # NOTE: policy_lr = 1e-4 + clip_range 0.2  leads to explained_variance < -1, investigating this will help learn more about PPO
  normalize_advantages: "rollout"
  eval_warmup_epochs: 50
  eval_freq_epochs: 15
  early_stop_on_train_threshold: True
  #early_stop_on_eval_threshold: True
  #frame_stack: 2 # TODO: not converging faster with frame stacking here, why?

