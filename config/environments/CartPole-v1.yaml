# CartPole-v1 configurations covering classic state-based, reward-shaped, and pixel observations

# =====================
# Environment Specification
# =====================
spec:
  source: https://gymnasium.farama.org/environments/classic_control/cart_pole/
  description: Balance a pole on a moving cart by applying forces to the cart.

  action_space:
    discrete: 2
    labels: {0: push_left, 1: push_right}

  observation_space:
    default: state
    variants:
      state:
        shape: [4]
        dtype: float32
        components:
          - {name: cart_position, range: [-4.8, 4.8]}
          - {name: cart_velocity, range: [-inf, inf]}
          - {name: pole_angle_rad, range: [-0.418, 0.418]}
          - {name: pole_angular_velocity, range: [-inf, inf]}

  rewards:
    per_step: +1
    range: [0, 1]

  returns:
    episodic: number of steps balanced (max 500)
    range: [0, 500]
    threshold_solved: 475

  versions:
    v1: Default Gymnasium release

# =====================
# Configuration Anchors
# =====================

_env: &env
  env_id: CartPole-v1
  seed: 42
  max_timesteps: 3e5
  normalize_obs: false
  n_envs: 8
  n_steps: 1024
  policy_lr: 6e-3
  hidden_dims: [64, 64]
  gamma: 0.99
  ent_coef: 0.0
  frame_stack: 1
  eval_freq_epochs: 5
  eval_episodes: 100

_env_rewardshaped: &env_rewardshaped
  <<: *env
  env_wrappers:
    - id: CartPoleV1_RewardShaper
      angle_reward_scale: 1.0
      position_reward_scale: 0.25
      clip_potential: true

_ppo: &ppo
  <<: *env
  algo_id: ppo
  max_timesteps: 50000
  n_envs: 8
  n_steps: 32
  batch_size: 0.25
  n_epochs: 20
  gae_lambda: 0.8
  clip_range: 0.2
  policy_lr: 1e-3
  normalize_advantages: "rollout"
  eval_warmup_epochs: 50
  eval_freq_epochs: 10

_reinforce: &reinforce
  <<: *env
  algo_id: reinforce

_rgb_env: &rgb_env
  env_id: CartPole-v1
  env_wrappers:
    - id: PixelObservationWrapper
      pixels_only: true
  frame_stack: 4
  policy: 'cnn'
  policy_kwargs:
    channels: [32, 64, 64]
    kernel_sizes: [8, 4, 3]
    strides: [4, 2, 1]
  eval_freq_epochs: 10
  eval_episodes: 5

_rgb_ppo: &rgb_ppo
  <<: *rgb_env
  algo_id: ppo
  n_envs: 8
  max_timesteps: 5e5
  n_steps: 64
  batch_size: 512
  gae_lambda: 0.95
  gamma: 0.99
  ent_coef: 0.001
  n_epochs: 4
  policy_lr: lin_2.5e-4
  clip_range: lin_0.2

ppo:
  <<: *ppo

ppo_rewardshaped:
  <<: [*env_rewardshaped, *ppo]
  max_timesteps: 1e5
  batch_size: 256
  gamma: 0.98
  ent_coef: 0.0
  policy_lr: lin_0.001
  clip_range: lin_0.2

reinforce_vanilla:
  <<: *reinforce
  returns_type: "mc:episode"
  policy_targets: "returns"

reinforce_rtg:
  <<: *reinforce
  returns_type: "mc:rtg"
  policy_targets: "returns"
  normalize_returns: "batch"

reinforce_baseline:
  <<: *reinforce
  returns_type: "mc:episode"
  advantages_type: "baseline"
  policy_targets: "advantages"
  normalize_advantages: "batch"

reinforce_baseline_rtg:
  <<: *reinforce
  returns_type: "mc:rtg"
  advantages_type: "baseline"
  policy_targets: "advantages"
  normalize_advantages: "batch"

rgb_ppo:
  <<: *rgb_ppo
