# MountainCar standard challenge configurations

# =====================
# Environment Specification
# =====================
spec: &spec
  source: https://gymnasium.farama.org/environments/classic_control/mountain_car/
  description: Drive up the mountain by building momentum on a 1D track.

  action_space:
    discrete: 3
    labels: {0: push_left, 1: no_push, 2: push_right}

  observation_space:
    default: state
    variants:
      state:
        shape: [2]
        dtype: float32
        components:
          - {name: position, range: [-1.2, 0.6]}
          - {name: velocity, range: [-0.07, 0.07]}

  rewards:
    per_step: -1
    range: [-1, 0]

  returns:
    episodic: negative of steps to reach goal (lower is worse)
    range: [-200, 0]
    threshold_solved: -110


# =====================
# env variants
# =====================

# Base env
_env: &env
  spec: *spec
  env_id: MountainCar-v0
  accelerator: cpu

# Reward-shaped env
_env_rewardshaped: &env_rewardshaped
  <<: *env
  env_wrappers:
    - id: MountainCarV0_RewardShaper
      position_reward_scale: 100.0
      velocity_reward_scale: 10.0
      height_reward_scale: 50.0

# Curiosity-driven env
_env_curiosity: &env_curiosity
  <<: *env
  env_wrappers:
    - id: MountainCarV0_StateCountBonus
      position_bins: 50
      velocity_bins: 50
      bonus_scale: 0.1
      bonus_type: count

# =====================
# configs
# =====================

ppo: &ppo
  <<: *env
  algo_id: ppo
  description: "Standard PPO on MountainCar - baseline configuration for mountain car task"
  eval_warmup_epochs: 50
  eval_freq_epochs: 10
  eval_episodes: 10
  hidden_dims: [128, 128]
  n_steps: 1024
  gamma: 0.999
  policy_lr: 1e-4

ppo_rewardshaped:
  <<: *ppo
  <<: *env_rewardshaped
  description: "PPO with reward shaping - uses shaped rewards to guide learning up the mountain"
  frame_stack: 3
  max_env_steps: 1e6
  n_steps: 128
  policy_lr: 3e-4
  hidden_dims: [256, 256]
  max_epochs: 1000

ppo_curiosity:
  <<: *ppo
  <<: *env_curiosity
  description: "PPO with curiosity-driven exploration bonus based on state visitation counts"
  max_env_steps: 1e6
  n_steps: 256
  policy_lr: 3e-4
  hidden_dims: [256, 256]
