# Pong with object-based observations challenge configurations

Pong-v5_objects_ppo:
  env_id: ALE/Pong-v5
  algo_id: ppo
  obs_type: "objects"
  frame_stack: 1
  env_wrappers:
    - id: PongV5_FeatureExtractor
  n_envs: 8               # Reduced for better sample efficiency
  policy: 'MlpPolicy'
  n_steps: 512             # Reduced for more frequent updates
  batch_size: 512          # Larger batches for stable gradients
  n_epochs: 10             # More epochs to better utilize data
  gae_lambda: 0.95         # Standard GAE lambda for better advantage estimation
  gamma: 0.99              # Standard Atari discount factor
  ent_coef: 0.005          # Higher entropy for better exploration
  clip_range: 0.2          # Standard PPO clipping for better updates
  learning_rate: 5e-4      # Higher learning rate for faster learning
  hidden_dims: [64, 64]    # Larger network for complex patterns
  eval_freq_epochs: 50     # Less frequent eval to focus on training
  eval_episodes: 10        # More episodes for reliable evaluation
  vf_coef: 0.5             # Standard value function coefficient
  max_grad_norm: 0.5       # Gradient clipping for stability

Pong-v5_objects_reinforce:
  env_id: ALE/Pong-v5
  algo_id: reinforce
  obs_type: "objects"
  frame_stack: 1
  env_wrappers:
    - id: PongV5_FeatureExtractor
  n_envs: 8
  n_epochs: 1
  ent_coef: 0.01
  n_steps: 1024
  batch_size: 128
  gamma: 0.99
  learning_rate: 1e-3
  hidden_dims: [128, 128]
  use_baseline: true
  max_epochs: 2000
