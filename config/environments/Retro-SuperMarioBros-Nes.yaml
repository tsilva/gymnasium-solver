# SuperMarioBros-Nes with RGB observations (preprocessed to grayscale+84x84+frame-stack)

# =====================
# Environment Specification
# =====================
spec: &spec
  source: https://github.com/Farama-Foundation/stable-retro
  description: >
    Side-scrolling platformer. Guide Mario through levels to reach the flagpole
    while avoiding enemies and hazards. Observations are raw game frames; rewards
    typically reflect in-game score/progress.

  render_fps: 60
   
  # TODO: figure out other actions
  # TODO: why stuck keys
  # TODO: why env end early
  # TODO: why showing only 0-8
  action_space:
    multibinary: 10
    labels: {
      0: B,
      1: unknown1,
      2: unknown2,
      3: unknown3,
      4: unknown4,
      5: dpad_down,
      6: dpad_left,
      7: dpad_right,
      8: A,
      9: unknown5
    }
    valid: [0, 5, 6, 7, 8]


  #action_space:
    # When using stable-retro with use_restricted_actions=FILTERED, actions are a
    # filtered discrete set of gamepad combinations. Exact count can vary by game.
  #  note: filtered discrete action set (retro.Actions.FILTERED)

  observation_space:
    default: rgb
    variants:
      rgb:
        shape: [240, 256, 3]
        dtype: uint8
        range: [0, 255]
        note: NES native resolution; pipeline typically grayscales+resizes+stacks frames.

  rewards:
    description: >
      Environment returns score/progress deltas; death or timeouts end episodes.
    components:
      - {name: score_delta, sign: positive}
      - {name: progress, sign: positive}
      - {name: death_penalty, sign: negative}


# =====================
# env variants
# =====================

# Base env
_env: &env
  spec: *spec
  project_id: Retro-SuperMarioBros-Nes
  env_id: Retro/SuperMarioBros-Nes
  env_kwargs:
    state: "Level1-1"
    #states: ["Level1-1", "Level1-4", "Level2-1", "Level3-1", "Level4-1", "Level5-1", "Level6-1", "Level7-1", "Level8-1"]
  env_wrappers:
    - id: RetroSuperMarioBros_RewardShaper
      reward_scale: 1.0
      death_penalty: -50.0  # Large penalty to prevent death-farming exploits
      level_complete_bonus: 50.0
      step_penalty: -0.1
      x_position_scale: 0.2
      score_scale: 0.002
      x_reset_threshold: -100.0

# =====================
# configs
# =====================

# TODO: review VF clip fraction
ppo:
  <<: *env
  algo_id: ppo
  description: "PPO on SuperMarioBros - standard configuration for Mario platformer game"
  #max_env_steps: 3e6
  n_envs: 16 # TODO: faster training with more envs?
  n_steps: 1024
  batch_size: 0.25
  policy_lr: 5e-4 # 1e-3 resulted in approx_kl spikes mid training that damaged policy (shared trunk)
  max_env_steps: 3e6
  #ent_coef: 0.03 # TODO: how is entropy behaving? I think we can reduce this
  #max_episode_steps: 10_000
  eval_warmup_epochs: 10
  eval_freq_epochs: 10
  eval_episodes: 1 # TODO: increase once eval_async works