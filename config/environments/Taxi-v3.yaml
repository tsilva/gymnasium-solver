# Taxi standard challenge configurations

env_id: Taxi-v3
# Lower entropy; high (0.5) stalls learning on Taxi
ent_coef: 0.01
n_envs: 16
eval_freq_epochs: 10
eval_episodes: 10
env_wrappers:
  - id: DiscreteEncoder
    encoding: array

ppo:
  algo_id: ppo
  max_timesteps: 10e6 #3e5
  # Slightly longer rollouts help stabilize updates on Taxi
  n_steps: 1024                 # 16*128 = 2048 samples/update
  batch_size: 4096              # 4 minibatches

  n_epochs: 20
  clip_range: 0.2
  policy_lr: 1e-2
  hidden_dims: [4, 16]        # embeddings via DiscreteEncoder(array) -> 500-cat embedding of dim 128
  # Normalize advantages per mini-batch (PPO best practice)
  normalize_returns: "rollout"
  normalize_advantages: "batch"
  vf_coef: 0.25

  # Eval
  eval_episodes: 10

reinforce:
  algo_id: reinforce
  max_timesteps: 5e5
  n_steps: 256
  batch_size: 32
  policy_lr: 0.01
  hidden_dims: [128]
