# =====================
# Environment Specification
# =====================
spec: &spec
  source: https://vizdoom.cs.put.edu.pl/competitions/cig-2017/scenarios/#deadly-corridor
  description: Reach the armor vest through a corridor of enemies; survive and progress forward.

  render_fps: 35

  action_space:
    discrete: 8
    labels: {
      0: noop, 
      1: forward,
      2: backward, 
      3: strafe_left,
      4: strafe_right,
      5: turn_left,
      6: turn_right,
      7: attack
    }

  observation_space:
    default: rgb
    variants:
      rgb:
        shape: [height, width, 3]
        dtype: uint8
        range: [0, 255]
        note: Height/width depend on the ViZDoom cfg; wrapper enforces RGB24 and HWC.

  rewards:
    description: Dense shaping towards the vest; penalties for retreat and death.
    components:
      - {name: distance_shaping, sign: positive}
      - {name: retreat_penalty, sign: negative}
      - {name: death_penalty, value: -100.0}
    threshold_solved: 1000
    range: [-100, 1000]

# TODO: this is used to standardize vizdoom action spaces across environments
_env_wrapper_discrete_action_space_remapper: &env_wrapper_discrete_action_space_remapper
  id: DiscreteActionSpaceRemapperWrapper
  mapping: [0, 2, 5, 3, 4, 6, 7, 1]

# =====================
# env variants
# =====================

# Base env
_env: &env
  spec: *spec
  env_id: VizDoom-DeadlyCorridor-v0
  render_mode: 'rgb_array'
    # TODO: defaults are not being applied
  obs_type: rgb
  render_mode: 'rgb_array'
  policy_kwargs:
    channels: [32, 64, 64]
    kernel_sizes: [8, 4, 3]
    strides: [4, 2, 1]
  grayscale_obs: true
  resize_obs: [84, 84]
  frame_stack: 4
  env_wrappers:
    - *env_wrapper_discrete_action_space_remapper

# =====================
# configs
# =====================

ppo:
  <<: *env
  project_id: VizDoom-DeadlyCorridor-v0
  algo_id: ppo
  description: "PPO on VizDoom DeadlyCorridor - standard configuration for corridor navigation task"
  max_env_steps: 2e6

  n_envs: 32
  n_steps: 256
  batch_size: 0.25
  n_epochs: 20

  policy_lr:
    start: 2.5e-4
    end: 2.5e-5

  ent_coef: 0.03

  eval_warmup_epochs: 20
  eval_freq_epochs: 10
  eval_episodes: 10
