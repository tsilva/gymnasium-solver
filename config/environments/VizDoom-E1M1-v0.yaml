# =====================
# Environment Specification
# =====================
spec: &spec
  source: https://doom.fandom.com/wiki/E1M1:_Hangar_(Doom)
  description: Original Doom Episode 1 Map 1 (Hangar). Navigate facility, collect items, eliminate enemies, find exit.

  render_fps: 35

  action_space:
    discrete: 8
    labels: {
      0: noop, 
      1: forward, 
      2: backward, 
      3: strafe_left, 
      4: strafe_right, 
      5: turn_left, 
      6: turn_right, 
      7: attack
    }

  observation_space:
    default: rgb
    variants:
      rgb:
        shape: [height, width, 3]
        dtype: uint8
        range: [0, 255]
        note: Height/width depend on the ViZDoom cfg; wrapper enforces RGB24 and HWC.

  rewards:
    description: Complete the level by reaching the exit. Kills and items provide score.
    components:
      - {name: kill_reward, sign: positive, note: "1 point per kill"}
      - {name: item_reward, sign: positive, note: "Points for items/secrets"}
      - {name: living_penalty, value: -0.0001, note: "Encourages efficiency"}
    threshold_solved: 100
    range: [-0.21, 500]
    note: Episode terminates on death or timeout (2100 frames / 60 seconds) or level exit

_env_wrapper_discrete_action_space_remapper: &env_wrapper_discrete_action_space_remapper
  id: DiscreteActionSpaceRemapperWrapper
  mapping: [0, 1, 2, 3, 4, 5, 6, 7]

# =====================
# env variants
# =====================

# Base env
_env: &env
  spec: *spec
  env_id: VizDoom-E1M1-v0
  render_mode: 'rgb_array'
  policy: 'cnn_actorcritic'
  env_kwargs:
    scenario: e1m1
  env_wrappers:
    - *env_wrapper_discrete_action_space_remapper
    - { id: VizDoom_RewardShaper, noop_penalty: 0.01, move_reward: 0.004, turn_reward: 0.004 }
  obs_type: rgb
  render_mode: 'rgb_array'
  policy_kwargs:
    channels: [32, 64, 64]
    kernel_sizes: [8, 4, 3]
    strides: [4, 2, 1]
  grayscale_obs: true
  resize_obs: [84, 84]
  frame_stack: 4


# =====================
# configs
# =====================

ppo:
  <<: *env
  project_id: VizDoom-E1M1-v0

  algo_id: ppo
  description: "PPO on Doom E1M1 - agent learns to complete the classic Hangar level"
  max_env_steps: 1e7
  n_envs: 32
  n_steps: 256
  batch_size: 0.25
  n_epochs: 20

  policy_lr:
    start: 2.5e-4
    end: 2.5e-5

  ent_coef: 0.03

  eval_warmup_epochs: 20
  eval_freq_epochs: 20
  eval_episodes: 10
