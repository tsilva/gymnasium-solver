# Metric-centric configuration
# Each metric is defined as a top-level entry with all its properties
# Metric names should be without namespace prefixes since the same metric
# can appear across different namespaces (train/, eval/, roll/, etc.)


# Global configuration
_global:
  # Preferred display order for key metrics in the stdout table
  # Provide unnamespaced metric subkeys. The logger expands these for
  # train/val/test namespaces when ordering per-section.
  # Keys not listed here will follow alphabetically within their section.
  key_priority:
      # === Overall training progress (0.0 → 1.0) ===
      - "progress"

      - "cnt/epoch"
      - "cnt/total_episodes"
      - "cnt/total_vec_steps"
      - "cnt/total_timesteps"
      
      # === Rollout (training samples from the environment) ===
      - "roll/ep_rew/mean"
      - "roll/ep_rew/best"
      - "roll/ep_rew/last"
      - "roll/ep_len/mean"
      - "roll/ep_len/last"

      - "opt/ppo/clip_fraction"
      - "opt/ppo/kl"
      - "opt/ppo/approx_kl"
      - "opt/value/explained_var"
      
      - "roll/fps"
      - "sys/timing/fps"

      - "roll/reward/mean"
      - "roll/reward/std"
      - "roll/return/mean"
      - "roll/return/std"
      - "roll/adv/mean"
      - "roll/adv/std"
      - "roll/baseline/mean"
      - "roll/baseline/std"
      - "roll/obs/mean"
      - "roll/obs/std"
      - "roll/actions/mean"
      - "roll/actions/std"
      - "roll/episodes"
      - "roll/timesteps"
      - "roll/vec_steps"

      # === Optimization (per-update diagnostics) ===
      - "opt/loss/total"
      - "opt/loss/policy"
      - "opt/loss/value"
      - "opt/loss/value_scaled"
      - "opt/loss/entropy"
      - "opt/loss/entropy_scaled"
      - "opt/policy/entropy"
      - "opt/grads/norm/all"
      - "opt/grads/norm/backbone"
      - "opt/grads/norm/policy_head"
      - "opt/grads/norm/value_head"

      # === Counters (monotonic progress) ===
      - "cnt/total_rollouts"

      # === System & timing ===

      - "sys/timing/fps_instant"
      - "sys/timing/time_elapsed"
      - "sys/timing/eta_s"

      # === Hyperparameters ===
      - "hp/policy_lr"
      - "hp/clip_range"
      - "hp/ent_coef"
      - "hp/vf_coef"

  step_key: "train/cnt/total_vec_steps"

clip_range:
  description: "PPO clip range for policy updates (ratio clamp around 1.0)"
  precision: 4
  min: 0.0
  max: 1.0
      
# Episode and timestep counts (integers)
cnt/total_episodes:
  precision: 0
  description: "Cumulative number of episodes completed across all training"
  delta_rule: ">="  # Should always increase
  min: 0

# For CSV base fields, keep bare precision settings for compatibility
total_timesteps:
  highlight: true
  description: "Cumulative number of environment steps taken across all training"
  precision: 0
  min: 0

cnt/total_timesteps:
  highlight: true
  show_in_table: true
  # Note: delta_rule removed because eval/total_timesteps can legitimately decrease
  # between evaluation runs when using different eval strategies or env resets
  description: "Cumulative number of environment steps taken across all training"
  precision: 0
  min: 0

# Training progress (only logged when max_timesteps is defined)
progress:
  highlight: true
  description: "Normalized training progress based on total vectorized steps (0.0 start → 1.0 at max_timesteps)"
  precision: 2

roll/timesteps:
  description: "Number of timesteps collected in the current rollout buffer"
  precision: 0
  delta_rule: ">="  # Should always increase
  min: 0

# Vectorized-step counters (one per env.step() call across the vector)
cnt/total_vec_steps:
  highlight: true
  description: "Cumulative number of vectorized env.step() calls across training"
  precision: 0
  min: 0

roll/vec_steps:
  description: "Number of vectorized env.step() calls in the current rollout"
  precision: 0
  delta_rule: ">="
  min: 0

# Additional counters
cnt/total_rollouts:
  description: "Cumulative number of completed rollouts (batches) collected"
  precision: 0
  delta_rule: ">="
  min: 0

roll/episodes:
  description: "Number of episodes finished within the current rollout"
  precision: 0
  min: 0

# Episode statistics
roll/ep_rew/mean:
  bold: true
  highlight: true
  show_in_table: true
  description: "Average episode reward over recent episodes window"
  precision: 2

roll/ep_len/mean:
  show_in_table: true
  description: "Average episode length over recent episodes window"
  precision: 0
  min: 0

# Immediate episode stats
roll/ep_rew/last:
  show_in_table: true
  description: "Reward of the most recent completed episode"
  precision: 2

roll/ep_len/last:
  show_in_table: true
  description: "Length of the most recent completed episode"
  precision: 0
  min: 0

roll/ep_rew/best:
  show_in_table: true
  description: "Running best of ep_rew/mean across epochs"
  precision: 2

roll/ep_rew/min:
  description: "Minimum episode reward observed in recent episodes window"
  precision: 2

roll/ep_rew/max:
  description: "Maximum episode reward observed in recent episodes window"
  precision: 2

roll/ep_len/min: 
  precision: 0
  description: "Minimum episode length observed in recent episodes window"
  min: 0

roll/ep_len/max: 
  description: "Maximum episode length observed in recent episodes window"
  precision: 0
  min: 0

roll/ep_rew/std:
  description: "Standard deviation of episode rewards in recent episodes window"
  precision: 2

roll/ep_len/std:
  description: "Standard deviation of episode lengths in recent episodes window"
  precision: 2
  min: 0.0

# Training metrics
# For CSV base fields, keep bare precision settings for compatibility
epoch:
  description: "Current training epoch number"
  precision: 0
  delta_rule: ">="  # Should always increase
  min: 0

cnt/epoch:
  show_in_table: true
  description: "Current training epoch number"
  precision: 0
  delta_rule: ">="  # Should always increase
  min: 0

n_updates:
  description: "Total number of parameter updates (gradient steps) performed"
  precision: 0
  delta_rule: ">="  # Should always increase
  min: 0

# Time metrics
sys/timing/time_elapsed:
  description: "Total elapsed training time in seconds"
  precision: 2
  delta_rule: ">="  # Time should always increase
  min: 0.0

sys/timing/fps:
  show_in_table: true
  description: "Training throughput in frames (environment steps) per second"
  precision: 0
  min: 0.0

sys/timing/eta_s:
  description: "Estimated seconds remaining to reach max_timesteps"
  precision: 0
  min: 0.0

sys/timing/fps_instant:
  description: "Instant training throughput in frames per second measured within the current epoch"
  precision: 0
  min: 0.0

epoch_fps:
  description: "Throughput (steps/s) computed over the last validation epoch"
  precision: 0
  min: 0.0

# Loss metrics
opt/loss/policy:
  description: "Policy network loss from gradient ascent on expected returns"
  precision: 4

opt/loss/value:
  description: "Value function loss measuring prediction accuracy of state values"
  precision: 4

opt/loss/total:
  highlight: true
  description: "Total loss used for backprop (policy + value + entropy terms)"
  precision: 4

opt/loss/entropy:
  description: "Negative policy entropy used as regularization term"
  precision: 4

opt/loss/entropy_scaled:
  description: "Scaled negative entropy term used in total loss"
  precision: 4

# Algorithm-specific metrics
opt/ppo/approx_kl:
  highlight: true
  show_in_table: true
  description: "Approximate KL divergence between old and new policy distributions"
  precision: 4
  min: 0.0
  max: 0.1

opt/ppo/kl:
  description: "KL divergence between old and new policy log probabilities"
  precision: 4
  min: 0.0

opt/ppo/clip_fraction:
  highlight: true
  show_in_table: true
  description: "Fraction of probability ratios that were clipped in PPO objective"
  precision: 3
  min: 0.0
  max: 1.0

opt/value/explained_var:
  show_in_table: true
  description: "How well the value function predicts returns (1.0 = perfect, 0.0 = no better than mean, <0.0 = worse than mean)"
  precision: 3
  min: -1.0
  max: 1.0

opt/policy/entropy:
  description: "Policy entropy measuring exploration vs exploitation trade-off"
  precision: 4
  min: 0.0


# Granular gradient norms
opt/grads/norm/all:
  description: "L2 norm of all gradients before clipping"
  precision: 4
  min: 0.0

opt/grads/norm/backbone:
  description: "Gradient L2 norm for shared backbone parameters"
  precision: 4
  min: 0.0

opt/grads/norm/policy_head:
  description: "Gradient L2 norm for policy head"
  precision: 4
  min: 0.0

opt/grads/norm/value_head:
  description: "Gradient L2 norm for value head"
  precision: 4
  min: 0.0

opt/grads/norm/cnn:
  description: "Gradient L2 norm for CNN backbone (CNN models only)"
  precision: 4
  min: 0.0

opt/grads/norm/mlp:
  description: "Gradient L2 norm for MLP trunk after CNN (CNN models only)"
  precision: 4
  min: 0.0

# Algorithm-specific metrics
policy_targets_mean:
  description: "Mean of policy targets (returns or advantages) used in REINFORCE loss"
  precision: 4

policy_targets_std:
  description: "Standard deviation of policy targets (returns or advantages) used in REINFORCE loss"
  precision: 4
  min: 0.0

advantage_mean:
  description: "Mean advantage estimate across batch (should be near zero when normalized)"
  precision: 4

advantage_std:
  description: "Standard deviation of advantage estimates across batch"
  precision: 4
  min: 0.0

value_mean:
  description: "Mean value function estimate across states in batch"
  precision: 4

value_std:
  description: "Standard deviation of value function estimates across states in batch"
  precision: 4
  min: 0.0

# Observations and rewards statistics (from collectors)
roll/obs/mean:
  description: "Running mean of observations after preprocessing/normalization"
  precision: 3

roll/obs/std:
  description: "Running standard deviation of observations after preprocessing/normalization"
  precision: 3
  min: 0.0

roll/reward/mean:
  description: "Running mean of raw rewards (pre-normalization)"
  precision: 3

roll/reward/std:
  description: "Running standard deviation of raw rewards (pre-normalization)"
  precision: 3
  min: 0.0

roll/actions/mean:
  description: "Mean of actions taken (discrete: mean index; continuous: mean magnitude)"
  precision: 3

roll/actions/std:
  description: "Standard deviation of actions (proxy for exploration for continuous control)"
  precision: 3
  min: 0.0

roll/fps:
  show_in_table: true
  description: "Collector throughput in frames per second (rollout only)"
  precision: 0
  min: 0.0

roll/baseline/mean:
  description: "Running mean of the baseline used for returns normalization"
  precision: 3

roll/baseline/std:
  description: "Running std of the baseline used for returns normalization"
  precision: 3
  min: 0.0

# Additional rollout statistics with hard bounds
roll/return/mean:
  description: "Running mean of returns (post-normalization if enabled)"
  precision: 3

roll/return/std:
  description: "Running standard deviation of returns (post-normalization if enabled)"
  precision: 3
  min: 0.0

roll/adv/mean:
  description: "Running mean of advantage estimates (post-normalization if enabled)"
  precision: 4

roll/adv/std:
  description: "Running standard deviation of advantage estimates (post-normalization if enabled)"
  precision: 4
  min: 0.0

# Binary/flag metrics
opt/ppo/kl_stop_triggered:
  description: "Whether KL early stopping triggered for this epoch (0 or 1)"
  precision: 0
  min: 0
  max: 1

# Hyperparameter mirrors logged under hp/*
hp/policy_lr:
  description: "Policy learning rate at start (static reference)"
  precision: 6
  min: 0.0

hp/clip_range:
  description: "Clip range at start (static reference)"
  precision: 4
  min: 0.0

hp/ent_coef:
  description: "Entropy coefficient at start (static reference)"
  precision: 6
  min: 0.0

hp/vf_coef:
  description: "Value coefficient at start (static reference)"
  precision: 6
  min: 0.0
