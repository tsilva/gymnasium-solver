# Metric-centric configuration
# Each metric is defined as a top-level entry with all its properties
# Metric names should be without namespace prefixes since the same metric
# can appear across different namespaces (train/, eval/, rollout/, etc.)


# Global configuration
_global:
  # Preferred display order for key metrics in the stdout table
  # Provide unnamespaced metric subkeys. The logger expands these for
  # train/val/test namespaces when ordering per-section.
  # Keys not listed here will follow alphabetically within their section.
  key_priority:
    - "ep_rew/last"
    - "ep_rew/mean"
    - "ep_rew/best"
    - "ep_len/last"
    - "ep_len/mean"
    - "ppo/approx_kl"
    - "ppo/clip_fraction"
    - "policy/entropy"
    - "policy/action_std"
    - "loss/policy"
    - "loss/value"
    - "ppo/explained_variance"
    - "ppo/kl"
    - "loss/entropy"
    - "loss/value_scaled"
    - "loss/total"
    - "grad_norm/all"
    - "grad_norm/backbone"
    - "grad_norm/policy_head"
    - "grad_norm/value_head"
    - "obs/mean"
    - "obs/std"
    - "reward/mean"
    - "reward/std"
    - "total_timesteps"
    - "epoch"
    - "timing/fps"
    - "timing/fps_instant"
    - "timing/time_elapsed"
    - "total_episodes"
    - "total_rollouts"
    - "rollout/timesteps"
    - "rollout/episodes"
    - "rollout/fps"
    - "timing/eta_s"
    - "baseline_mean"
    - "baseline_std"
    - "loss/entropy_scaled"
    - "hp/policy_lr"
    - "hp/clip_range"
    - "hp/ent_coef"
    - "hp/vf_coef" 
  
  step_key: "train/total_timesteps"

clip_range:
  description: "PPO clip range for policy updates (ratio clamp around 1.0)"
  precision: 4
  min: 0.0
  max: 1.0
      
# Episode and timestep counts (integers)
total_episodes:
  precision: 0
  description: "Cumulative number of episodes completed across all training"
  delta_rule: ">="  # Should always increase

total_timesteps:
  highlight: true
  # Note: delta_rule removed because eval/total_timesteps can legitimately decrease
  # between evaluation runs when using different eval strategies or env resets
  description: "Cumulative number of environment steps taken across all training"
  precision: 0

rollout/timesteps:
  description: "Number of timesteps collected in the current rollout buffer"
  precision: 0
  delta_rule: ">="  # Should always increase

# Additional counters
total_rollouts:
  description: "Cumulative number of completed rollouts (batches) collected"
  precision: 0
  delta_rule: ">="

rollout/episodes:
  description: "Number of episodes finished within the current rollout"
  precision: 0

# Episode statistics
ep_rew/mean:
  bold: true
  highlight: true
  description: "Average episode reward over recent episodes window"
  precision: 2

ep_len/mean:
  description: "Average episode length over recent episodes window"
  precision: 0

# Immediate episode stats
ep_rew/last:
  bold: true
  highlight: true
  description: "Reward of the most recent completed episode"
  precision: 2

ep_len/last:
  description: "Length of the most recent completed episode"
  precision: 0

ep_rew/best:
  bold: true
  highlight: true
  description: "Running best of ep_rew/mean across epochs"
  precision: 2

ep_rew/min:
  description: "Minimum episode reward observed in recent episodes window"
  precision: 2

ep_rew/max:
  description: "Maximum episode reward observed in recent episodes window"
  precision: 2

ep_len/min:
  precision: 0
  description: "Minimum episode length observed in recent episodes window"

ep_len/max: 
  description: "Maximum episode length observed in recent episodes window"
  precision: 0

ep_rew/std:
  description: "Standard deviation of episode rewards in recent episodes window"
  precision: 2

ep_len/std:
  description: "Standard deviation of episode lengths in recent episodes window"
  precision: 2

# Training metrics
epoch:
  description: "Current training epoch number"
  precision: 0
  delta_rule: ">="  # Should always increase

n_updates:
  description: "Total number of parameter updates (gradient steps) performed"
  precision: 0
  delta_rule: ">="  # Should always increase

# Time metrics
time_elapsed:
  description: "Total elapsed training time in seconds"
  precision: 2
  delta_rule: ">="  # Time should always increase
  min: 0.0

timing/fps:
  description: "Training throughput in frames (environment steps) per second"
  precision: 0
  min: 0.0

timing/eta_s:
  description: "Estimated seconds remaining to reach max_timesteps"
  precision: 0
  min: 0.0

timing/fps_instant:
  description: "Instant training throughput in frames per second measured within the current epoch"
  precision: 0
  min: 0.0

epoch_fps:
  description: "Throughput (steps/s) computed over the last validation epoch"
  precision: 0
  min: 0.0

# Loss metrics
loss/policy:
  description: "Policy network loss from gradient ascent on expected returns"
  precision: 4

loss/value:
  description: "Value function loss measuring prediction accuracy of state values"
  precision: 4

loss/total:
  highlight: true
  description: "Total loss used for backprop (policy + value + entropy terms)"
  precision: 4

loss/entropy:
  description: "Negative policy entropy used as regularization term"
  precision: 4

loss/entropy_scaled:
  description: "Scaled negative entropy term used in total loss"
  precision: 4

# Algorithm-specific metrics
ppo/approx_kl:
  description: "Approximate KL divergence between old and new policy distributions"
  precision: 4
  min: 0.0
  max: 0.1

ppo/kl:
  description: "KL divergence between old and new policy log probabilities"
  precision: 4

ppo/clip_fraction:
  description: "Fraction of probability ratios that were clipped in PPO objective"
  precision: 3
  min: 0.0
  max: 1.0

ppo/explained_variance:
  description: "How well the value function predicts returns (1.0 = perfect, 0.0 = no better than mean, <0.0 = worse than mean)"
  precision: 3
  min: -1.0
  max: 1.0

policy/entropy:
  description: "Policy entropy measuring exploration vs exploitation trade-off"
  precision: 4
  min: 0.0



# Learning rates
policy_lr:
  description: "Current learning rate for policy network optimizer"
  precision: 6
  min: 0.0

value_lr:
  description: "Current learning rate for value network optimizer"
  precision: 6
  min: 0.0

# Gradient metrics
grad_norm:
  description: "L2 norm of policy network gradients before clipping"
  precision: 4
  min: 0.0

max_grad_norm:
  description: "Maximum allowed gradient norm for gradient clipping"
  precision: 4
  min: 0.0

# Granular gradient norms
grad_norm/backbone:
  description: "Gradient L2 norm for shared backbone parameters"
  precision: 4
  min: 0.0

grad_norm/policy_head:
  description: "Gradient L2 norm for policy head"
  precision: 4
  min: 0.0

grad_norm/value_head:
  description: "Gradient L2 norm for value head"
  precision: 4
  min: 0.0

# Algorithm-specific metrics
advantage_mean:
  description: "Mean advantage estimate across batch (should be near zero when normalized)"
  precision: 4

advantage_std:
  description: "Standard deviation of advantage estimates across batch"
  precision: 4
  min: 0.0

value_mean:
  description: "Mean value function estimate across states in batch"
  precision: 4

value_std:
  description: "Standard deviation of value function estimates across states in batch"
  precision: 4
  min: 0.0

# Observations and rewards statistics (from collectors)
obs/mean:
  description: "Running mean of observations after preprocessing/normalization"
  precision: 3

obs/std:
  description: "Running standard deviation of observations after preprocessing/normalization"
  precision: 3
  min: 0.0

reward/mean:
  description: "Running mean of raw rewards (pre-normalization)"
  precision: 3

reward/std:
  description: "Running standard deviation of raw rewards (pre-normalization)"
  precision: 3
  min: 0.0

policy/action_mean:
  description: "Mean of actions taken (discrete: mean index; continuous: mean magnitude)"
  precision: 3

policy/action_std:
  description: "Standard deviation of actions (proxy for exploration for continuous control)"
  precision: 3
  min: 0.0

rollout/fps:
  description: "Collector throughput in frames per second (rollout only)"
  precision: 0
  min: 0.0

baseline_mean:
  description: "Running mean of the baseline used for returns normalization"
  precision: 3

baseline_std:
  description: "Running std of the baseline used for returns normalization"
  precision: 3
  min: 0.0

# Hyperparameter mirrors logged under hp/*
hp/policy_lr:
  description: "Policy learning rate at start (static reference)"
  precision: 6
  min: 0.0

hp/clip_range:
  description: "Clip range at start (static reference)"
  precision: 4
  min: 0.0

hp/ent_coef:
  description: "Entropy coefficient at start (static reference)"
  precision: 6
  min: 0.0
