# Metric-centric configuration
# Each metric is defined as a top-level entry with all its properties
# Metric names should be without namespace prefixes since the same metric
# can appear across different namespaces (train/, eval/, roll/, etc.)


# Global configuration
_global:
  # Preferred display order for key metrics in the stdout table
  # Provide unnamespaced metric subkeys. The logger expands these for
  # train/val/test namespaces when ordering per-section.
  # Keys not listed here will follow alphabetically within their section.
  key_priority:
      # === Overall training progress (0.0 → 1.0) ===
      - "progress"

      - "cnt/epoch"
      - "cnt/total_episodes"
      - "cnt/total_vec_steps"
      - "cnt/total_timesteps"
      
      # === Rollout (training samples from the environment) ===
      - "roll/ep_rew/mean"
      - "roll/ep_rew/best"
      - "roll/ep_rew/last"
      - "roll/ep_len/mean"
      - "roll/ep_len/last"

      - "opt/ppo/clip_fraction"
      - "opt/ppo/kl"
      - "opt/ppo/approx_kl"
      - "opt/value/explained_var"
      
      - "roll/fps"
      - "sys/timing/fps"

      - "roll/reward/mean"
      - "roll/reward/std"
      - "roll/return/mean"
      - "roll/return/std"
      - "roll/adv/mean"
      - "roll/adv/std"
      - "roll/baseline/mean"
      - "roll/baseline/std"
      - "roll/obs/mean"
      - "roll/obs/std"
      - "roll/actions/mean"
      - "roll/actions/std"
      - "roll/episodes"
      - "roll/timesteps"
      - "roll/vec_steps"

      # === Optimization (per-update diagnostics) ===
      - "opt/loss/total"
      - "opt/loss/policy"
      - "opt/loss/value"
      - "opt/loss/value_scaled"
      - "opt/loss/entropy"
      - "opt/loss/entropy_scaled"
      - "opt/policy/entropy"
      - "opt/grads/norm/all"
      - "opt/grads/norm/backbone"
      - "opt/grads/norm/policy_head"
      - "opt/grads/norm/value_head"

      # === Counters (monotonic progress) ===
      - "cnt/total_rollouts"

      # === System & timing ===

      - "sys/timing/fps_instant"
      - "sys/timing/time_elapsed"
      - "sys/timing/eta_s"

      # === Hyperparameters ===
      - "hp/n_epochs"
      - "hp/policy_lr"
      - "hp/clip_range"
      - "hp/ent_coef"
      - "hp/vf_coef"

  step_key: "train/cnt/total_vec_steps"

# Episode and timestep counts (integers)
cnt/total_episodes:
  precision: 0
  description: "Cumulative number of episodes completed across all training"
  min: 0

cnt/total_env_steps:
  highlight: true
  show_in_table: true
  # Note: delta_rule removed because eval/total_env_steps can legitimately decrease
  # between evaluation runs when using different eval strategies or env resets
  description: "Cumulative individual environment steps (sum across all parallel environments)"
  precision: 0
  min: 0

# Training progress (only logged when max_timesteps is defined)
progress:
  highlight: true
  description: "Normalized training progress based on total vectorized steps (0.0 start → 1.0 at max_timesteps)"
  precision: 2
  min: 0.0  # Progress starts at 0
  max: 1.0  # Progress completes at 1

roll/env_steps:
  description: "Individual environment steps in the current rollout buffer (n_envs × n_steps)"
  precision: 0
  min: 0

# Vectorized-step counters (one per vec_env.step() call across all parallel environments)
cnt/total_vec_steps:
  show_in_table: true
  highlight: true
  description: "Cumulative vec_env.step() calls (one call steps all n_envs environments simultaneously)"
  precision: 0
  min: 0

roll/vec_steps:
  description: "Number of vec_env.step() calls in the current rollout (typically equals n_steps)"
  precision: 0
  min: 0

# Additional counters
cnt/total_rollouts:
  description: "Cumulative number of completed rollouts (batches) collected"
  precision: 0
  min: 0

roll/episodes:
  description: "Number of episodes finished within the current rollout"
  precision: 0
  min: 0

# Episode statistics
roll/ep_rew/mean:
  bold: true
  highlight: true
  show_in_table: true
  description: "Average episode reward over recent episodes window"
  precision: 2

roll/ep_len/mean:
  show_in_table: true
  description: "Average episode length over recent episodes window"
  precision: 0
  min: 0

# Immediate episode stats
roll/ep_rew/last:
  show_in_table: true
  description: "Reward of the most recent completed episode"
  precision: 2

roll/ep_len/last:
  show_in_table: true
  description: "Length of the most recent completed episode"
  precision: 0
  min: 0

roll/ep_rew/best:
  show_in_table: true
  description: "Running best of ep_rew/mean across epochs"
  precision: 2

# Training metrics
cnt/epoch:
  show_in_table: true
  description: "Current training epoch number"
  precision: 0
  min: 0

# Time metrics
sys/timing/time_elapsed:
  description: "Total elapsed training time in seconds"
  precision: 2
  min: 0.0

sys/timing/fps:
  show_in_table: true
  description: "Training throughput in frames (environment steps) per second"
  precision: 0
  min: 0.0

sys/timing/eta_s:
  description: "Estimated seconds remaining to reach max_timesteps"
  precision: 0
  min: 0.0

sys/timing/fps_instant:
  description: "Instant training throughput in frames per second measured within the current epoch"
  precision: 0
  min: 0.0

# Loss metrics
opt/loss/policy:
  description: "Policy network loss from gradient ascent on expected returns"
  precision: 4

opt/loss/value:
  description: "Value function loss measuring prediction accuracy of state values"
  precision: 4
  min: 0.0  # MSE loss cannot be negative

opt/loss/total:
  highlight: true
  description: "Total loss used for backprop (policy + value + entropy terms)"
  precision: 4

opt/loss/entropy:
  description: "Negative policy entropy used as regularization term"
  precision: 4

opt/loss/entropy_scaled:
  description: "Scaled negative entropy term used in total loss"
  precision: 4

# Algorithm-specific metrics
opt/ppo/approx_kl:
  highlight: true
  show_in_table: true
  description: "Approximate KL divergence between old and new policy distributions"
  precision: 4
  min: 0.0  # KL divergence cannot be negative

opt/ppo/kl:
  description: "KL divergence between old and new policy log probabilities"
  precision: 4
  min: 0.0

opt/ppo/clip_fraction:
  highlight: true
  show_in_table: true
  description: "Fraction of probability ratios that were clipped in PPO objective"
  precision: 3
  min: 0.0  # Fraction cannot be negative
  max: 1.0  # Fraction cannot exceed 1

opt/value/explained_var:
  show_in_table: true
  description: "How well the value function predicts returns (1.0 = perfect, 0.0 = no better than mean, <0.0 = worse than mean)"
  precision: 3
  # Explained variance can theoretically be any value: large negative (terrible predictions),
  # 0.0 (no better than mean), 1.0 (perfect), or >1.0 (overfitting/leakage).
  # No hard bounds enforced here.

opt/policy/entropy:
  description: "Policy entropy measuring exploration vs exploitation trade-off"
  precision: 4
  min: 0.0


# Granular gradient norms
opt/grads/norm/all:
  description: "L2 norm of all gradients before clipping"
  precision: 4
  min: 0.0

opt/grads/norm/backbone:
  description: "Gradient L2 norm for shared backbone parameters"
  precision: 4
  min: 0.0

opt/grads/norm/policy_head:
  description: "Gradient L2 norm for policy head"
  precision: 4
  min: 0.0

opt/grads/norm/value_head:
  description: "Gradient L2 norm for value head"
  precision: 4
  min: 0.0

opt/grads/norm/cnn:
  description: "Gradient L2 norm for CNN backbone (CNN models only)"
  precision: 4
  min: 0.0

opt/grads/norm/mlp:
  description: "Gradient L2 norm for MLP trunk after CNN (CNN models only)"
  precision: 4
  min: 0.0

# REINFORCE-specific metrics
policy_targets_mean:
  description: "Mean of policy targets (returns or advantages) used in REINFORCE loss"
  precision: 4

policy_targets_std:
  description: "Standard deviation of policy targets (returns or advantages) used in REINFORCE loss"
  precision: 4
  min: 0.0

# Observations and rewards statistics (from collectors)
roll/obs/mean:
  description: "Running mean of observations after preprocessing/normalization"
  precision: 3

roll/obs/std:
  description: "Running standard deviation of observations after preprocessing/normalization"
  precision: 3
  min: 0.0

roll/reward/mean:
  description: "Running mean of raw rewards (pre-normalization)"
  precision: 3

roll/reward/std:
  description: "Running standard deviation of raw rewards (pre-normalization)"
  precision: 3
  min: 0.0

roll/actions/mean:
  description: "Mean of actions taken (discrete: mean index; continuous: mean magnitude)"
  precision: 3

roll/actions/std:
  description: "Standard deviation of actions (proxy for exploration for continuous control)"
  precision: 3
  min: 0.0

roll/fps:
  show_in_table: true
  description: "Collector throughput in frames per second (rollout only)"
  precision: 0
  min: 0.0

roll/baseline/mean:
  description: "Running mean of the baseline used for returns normalization"
  precision: 3

roll/baseline/std:
  description: "Running std of the baseline used for returns normalization"
  precision: 3
  min: 0.0

# Additional rollout statistics with hard bounds
roll/return/mean:
  description: "Running mean of returns (post-normalization if enabled)"
  precision: 3

roll/return/std:
  description: "Running standard deviation of returns (post-normalization if enabled)"
  precision: 3
  min: 0.0

roll/adv/mean:
  description: "Running mean of advantage estimates (post-normalization if enabled)"
  precision: 4

roll/adv/std:
  description: "Running standard deviation of advantage estimates (post-normalization if enabled)"
  precision: 4
  min: 0.0

# Binary/flag metrics
opt/ppo/kl_stop_triggered:
  description: "Whether KL early stopping triggered for this epoch (0 or 1)"
  precision: 0
  min: 0
  max: 1

# Hyperparameter mirrors logged under hp/*
hp/n_epochs:
  description: "Number of optimization epochs per rollout"
  precision: 0
  min: 1

hp/policy_lr:
  description: "Policy learning rate at start (static reference)"
  precision: 6
  min: 0.0

hp/clip_range:
  description: "Clip range at start (static reference)"
  precision: 4
  min: 0.0

hp/ent_coef:
  description: "Entropy coefficient at start (static reference)"
  precision: 6
  min: 0.0

hp/vf_coef:
  description: "Value coefficient at start (static reference)"
  precision: 6
  min: 0.0
