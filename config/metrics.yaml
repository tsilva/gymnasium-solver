# Metric-centric configuration
# Each metric is defined as a top-level entry with all its properties
# Metric names should be without namespace prefixes since the same metric
# can appear across different namespaces (train/, eval/, rollout/, etc.)

# Global configuration
_global:
  default_precision: 4  # Default precision for metrics not explicitly configured
  # Preferred display order for key metrics in the stdout table
  # Keys not listed here will follow alphabetically within their section
  key_priority:
    - "train/ep_rew_best"
    - "train/ep_rew_mean"
    - "train/ep_rew_last"
    - "train/ep_len_mean"
    - "train/ep_len_last"
    - "train/epoch"
    - "train/total_timesteps"
    - "train/total_episodes"
    - "train/total_rollouts"
    - "train/rollout_timesteps"
    - "train/rollout_episodes"
    - "train/fps_instant"
    - "train/epoch_fps"
    - "train/rollout_fps"
    - "train/loss"
    - "train/policy_loss"
    - "train/value_loss"
    - "train/entropy_loss"
    - "eval/ep_rew_best"
    - "eval/ep_rew_mean"
    - "eval/ep_len_mean"
    - "eval/ep_rew_last"
    - "eval/ep_len_last"
    - "eval/epoch"
    - "eval/total_timesteps"
    - "eval/total_episodes"
    - "eval/total_rollouts"
    - "eval/rollout_timesteps"
    - "eval/rollout_episodes"
    - "eval/epoch_fps"
    - "eval/rollout_fps"
  # Console highlight configuration
  highlight:
    # Rows to highlight across namespaces (blue background + bold key)
    row_metrics:
      - ep_rew_mean
      - ep_rew_last
      - ep_rew_best
      - total_timesteps
      - loss
    # Metrics whose values are emphasized in bold (no row background required)
    value_bold_metrics:
      - ep_rew_mean
      - ep_rew_last
      - ep_rew_best
      - epoch
    row_bg_color: bg_blue
    row_bold: true

clip_range:
  precision: 4
  description: "PPO clip range for policy updates"
  min: 0.0
  max: 1.0
  algorithm_rules:
    ppo:
      threshold: 0.2
      condition: "greater_than"
      message: "High clip range ({current_value:.4f}) may lead to unstable training. Consider reducing."
      level: "warning"
      
# Episode and timestep counts (integers)
total_episodes:
  precision: 0
  force_integer: true
  delta_rule: "non_decreasing"  # Should always increase
  description: "Cumulative number of episodes completed across all training"

total_timesteps:
  precision: 0
  force_integer: true
  # Note: delta_rule removed because eval/total_timesteps can legitimately decrease
  # between evaluation runs when using different eval strategies or env resets
  description: "Cumulative number of environment steps taken across all training"

rollout_timesteps:
  precision: 0
  force_integer: true
  delta_rule: "non_decreasing"  # Should always increase
  description: "Number of timesteps collected in the current rollout buffer"

# Episode statistics
ep_rew_mean:
  precision: 2
  description: "Average episode reward over recent episodes window"

ep_len_mean:
  precision: 0
  force_integer: true
  description: "Average episode length over recent episodes window"

# Immediate episode stats
ep_rew_last:
  precision: 2
  description: "Reward of the most recent completed episode"

ep_len_last:
  precision: 0
  force_integer: true
  description: "Length of the most recent completed episode"

ep_rew_min:
  precision: 2
  description: "Minimum episode reward observed in recent episodes window"

ep_rew_max:
  precision: 2
  description: "Maximum episode reward observed in recent episodes window"

ep_len_min:
  precision: 0
  force_integer: true
  description: "Minimum episode length observed in recent episodes window"

ep_len_max:
  precision: 0
  force_integer: true
  description: "Maximum episode length observed in recent episodes window"

ep_rew_std:
  precision: 2
  description: "Standard deviation of episode rewards in recent episodes window"

ep_len_std:
  precision: 2
  description: "Standard deviation of episode lengths in recent episodes window"

# Training metrics
epoch:
  precision: 0
  force_integer: true
  delta_rule: "non_decreasing"  # Should always increase
  description: "Current training epoch number"

n_updates:
  precision: 0
  force_integer: true
  delta_rule: "non_decreasing"  # Should always increase
  description: "Total number of parameter updates (gradient steps) performed"

# Time metrics
time_elapsed:
  precision: 2
  delta_rule: "non_decreasing"  # Time should always increase
  description: "Total elapsed training time in seconds"
  min: 0.0

fps:
  precision: 0
  force_integer: true
  description: "Training throughput in frames (environment steps) per second"
  min: 0.0

fps_instant:
  precision: 0
  force_integer: true
  description: "Instant training throughput in frames per second measured within the current epoch"
  min: 0.0

# Loss metrics
policy_loss:
  precision: 4
  description: "Policy network loss from gradient ascent on expected returns"
  algorithm_rules:
    reinforce:
      threshold: 1000.0
      condition: "less_than"
      message: "Very high policy loss ({current_value:.3f}) may indicate training instability."
      level: "warning"

value_loss:
  precision: 4
  description: "Value function loss measuring prediction accuracy of state values"

entropy_loss:
  precision: 4
  description: "Negative policy entropy used as regularization term"

total_loss:
  precision: 4
  description: "Combined total loss including policy, value and entropy components"

# Algorithm-specific metrics
approx_kl:
  precision: 4
  description: "Approximate KL divergence between old and new policy distributions"
  algorithm_rules:
    ppo:
      threshold: 0.1
      condition: "less_than"
      message: "High approximate KL divergence ({current_value:.4f}) indicates large policy changes. Consider reducing learning rate."
      level: "warning"

kl_div:
  precision: 4
  description: "KL divergence between old and new policy log probabilities"
  algorithm_rules:
    ppo:
      threshold: 0.1
      condition: "less_than"
      message: "High KL divergence ({current_value:.4f}) indicates large policy changes. Consider reducing learning rate."
      level: "warning"

clip_fraction:
  precision: 3
  description: "Fraction of probability ratios that were clipped in PPO objective"
  min: 0.0
  max: 1.0
  algorithm_rules:
    ppo:
      threshold: 0.5
      condition: "less_than"
      message: "High clip fraction ({current_value:.3f}) indicates policy is changing too rapidly. Consider reducing learning rate or clip_range."
      level: "warning"

explained_variance:
  precision: 3
  description: "How well the value function predicts returns (1.0 = perfect, 0.0 = no better than mean)"
  min: -1.0
  max: 1.0
  algorithm_rules:
    ppo:
      threshold: -0.5
      condition: "greater_than"
      message: "Very negative explained variance ({current_value:.3f}) indicates value function is performing poorly. Check value function architecture or learning rate."
      level: "warning"

entropy:
  precision: 4
  description: "Policy entropy measuring exploration vs exploitation trade-off"
  min: 0.0
  algorithm_rules:
    ppo:
      threshold: 0.1
      condition: "greater_than"
      message: "Low entropy ({current_value:.3f}) indicates policy is becoming too deterministic. Consider increasing entropy coefficient."
      level: "warning"
    reinforce:
      threshold: 0.05
      condition: "greater_than"
      message: "Low entropy ({current_value:.3f}) in REINFORCE indicates policy is becoming too deterministic. Consider increasing entropy coefficient."
      level: "warning"

# Learning rates
policy_lr:
  precision: 6
  description: "Current learning rate for policy network optimizer"
  min: 0.0

value_lr:
  precision: 6
  description: "Current learning rate for value network optimizer"
  min: 0.0

policy_lr:
  precision: 6
  description: "General learning rate for optimizer"
  min: 0.0

# Gradient metrics
grad_norm:
  precision: 4
  description: "L2 norm of policy network gradients before clipping"
  min: 0.0

max_grad_norm:
  precision: 4
  description: "Maximum allowed gradient norm for gradient clipping"
  min: 0.0

# Algorithm-specific metrics
advantage_mean:
  precision: 4
  description: "Mean advantage estimate across batch (should be near zero when normalized)"

advantage_std:
  precision: 4
  description: "Standard deviation of advantage estimates across batch"
  min: 0.0

value_mean:
  precision: 4
  description: "Mean value function estimate across states in batch"

value_std:
  precision: 4
  description: "Standard deviation of value function estimates across states in batch"
  min: 0.0
