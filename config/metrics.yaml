# Metric-centric configuration
# Each metric is defined as a top-level entry with all its properties
# Metric names should be without namespace prefixes since the same metric
# can appear across different namespaces (train/, eval/, rollout/, etc.)


# Global configuration
_global:
  default_precision: 4  # Default precision for metrics not explicitly configured
  # Preferred display order for key metrics in the stdout table
  # Keys not listed here will follow alphabetically within their section
  key_priority:
    - "train/ep_rew_mean"
    - "train/ep_rew_best"
    - "train/ep_rew_last"
    - "train/ep_len_mean"
    - "train/ep_len_last"
    - "train/approx_kl" # (PPO stability: aim ~0.01–0.02/update; alert if >0.05 or <0.001)
    - "train/clip_fraction" # (healthy ~0.1–0.3; too low = under-updating, too high = over-clipping)
    - "train/entropy" # (should decay gradually; flat-low early = collapse)
    - "train/action_std" # (tracks exploration in continuous control; “breathing” but not collapsing)
    - "train/policy_loss" # (directional sanity check; big swings = instability)
    - "train/value_loss" #(critic health; falling over time is good)
    - "train/explained_variance" # (critic R²-ish; target >0.5 mid-run, >0.8 late)
    - "train/kl_div" # (secondary to approx_kl; still useful cross-check)
    - "train/entropy_loss" # (mirrors entropy; catch coef mistakes)
    - "train/value_loss_scaled" # (watch scaling/normalization effects vs unscaled)
    - "train/loss" # (aggregate: spikes = step-size or data issues)
    - "train/grad_norm/backbone" # (grad health: drift/spikes => lr/clip or bug)
    - "train/grad_norm/actor_head" 
    - "train/grad_norm/critic_head"
    - "train/obs_mean" # (norm/whitening sanity; sudden shifts = pipeline bugs)
    - "train/obs_std" # (should stabilize; zero/NaN = broken obs)
    - "train/reward_mean" # (only meaningful if not normalized; otherwise de-prioritize)
    - "train/reward_std" # (variance can explain noisy learning curves)
    - "train/total_timesteps" # (training progress metric)
    - "train/epoch" # (training progress metric)
    - "train/time_elapsed" # (training progress metric)
    - "train/total_episodes" # (training progress metric)
    - "train/total_rollouts" # (training progress metric)
    - "train/rollout_timesteps" # (fixed in PPO usually; verify config matches reality)
    - "train/rollout_episodes" # (detect pathologies: too few/too many per batch)
    - "train/fps" # (end-to-end throughput; use to spot stalls)
    - "train/rollout_fps" # (collector throughput; CPU/env bottlenecks)
    - "train/fps_instant" # (spiky; good for hiccup forensics, not trend)
    - "train/eta_s" # (eta: total expected training time; good for end-to-end planning)
    - "train/hp/policy_lr" # (constant unless policy_lr_schedule is set)
    - "train/hp/clip_range" # (constant unless clip_range_schedule is set)
    - "train/hp/ent_coef" # (constant unless ent_coef_schedule is set)
    - "train/baseline_mean" # (used for returns normalization)
    - "train/baseline_std" # (used for returns normalization)
    - "train/entropy_loss_scaled" # (used for entropy regularization)
    - "val/ep_rew_best"
    - "val/ep_rew_mean"
    - "val/ep_len_mean"
    - "val/ep_rew_last"
    - "val/ep_len_last"
    - "val/epoch"
    - "val/total_timesteps"
    - "val/total_episodes"
    - "val/total_rollouts"
    - "val/rollout_timesteps"
    - "val/rollout_episodes"
    - "val/epoch_fps"
    - "val/rollout_fps"
  # Console highlight configuration
  highlight:
    # Rows to highlight across namespaces (blue background + bold key)
    row_metrics:
      - ep_rew_mean
      - ep_rew_last
      - ep_rew_best
      - total_timesteps
      - loss
    # Metrics whose values are emphasized in bold (no row background required)
    value_bold_metrics:
      - ep_rew_mean
      - ep_rew_last
      - ep_rew_best
      - epoch
    row_bg_color: bg_blue
    row_bold: true

clip_range:
  precision: 4
  description: "PPO clip range for policy updates"
  min: 0.0
  max: 1.0
  algorithm_rules:
    ppo:
      threshold: 0.2
      condition: "greater_than"
      message: "High clip range ({current_value:.4f}) may lead to unstable training. Consider reducing."
      level: "warning"
      
# Episode and timestep counts (integers)
total_episodes:
  precision: 0
  delta_rule: "non_decreasing"  # Should always increase
  description: "Cumulative number of episodes completed across all training"

total_timesteps:
  precision: 0
  # Note: delta_rule removed because eval/total_timesteps can legitimately decrease
  # between evaluation runs when using different eval strategies or env resets
  description: "Cumulative number of environment steps taken across all training"

rollout_timesteps:
  precision: 0
  delta_rule: "non_decreasing"  # Should always increase
  description: "Number of timesteps collected in the current rollout buffer"

# Episode statistics
ep_rew_mean:
  precision: 2
  description: "Average episode reward over recent episodes window"

ep_len_mean:
  precision: 0
  description: "Average episode length over recent episodes window"

# Immediate episode stats
ep_rew_last:
  precision: 2
  description: "Reward of the most recent completed episode"

ep_len_last:
  precision: 0
  description: "Length of the most recent completed episode"

ep_rew_min:
  precision: 2
  description: "Minimum episode reward observed in recent episodes window"

ep_rew_max:
  precision: 2
  description: "Maximum episode reward observed in recent episodes window"

ep_len_min:
  precision: 0
  description: "Minimum episode length observed in recent episodes window"

ep_len_max:
  precision: 0
  description: "Maximum episode length observed in recent episodes window"

ep_rew_std:
  precision: 2
  description: "Standard deviation of episode rewards in recent episodes window"

ep_len_std:
  precision: 2
  description: "Standard deviation of episode lengths in recent episodes window"

# Training metrics
epoch:
  precision: 0
  delta_rule: "non_decreasing"  # Should always increase
  description: "Current training epoch number"

n_updates:
  precision: 0
  delta_rule: "non_decreasing"  # Should always increase
  description: "Total number of parameter updates (gradient steps) performed"

# Time metrics
time_elapsed:
  precision: 2
  delta_rule: "non_decreasing"  # Time should always increase
  description: "Total elapsed training time in seconds"
  min: 0.0

fps:
  precision: 0
  description: "Training throughput in frames (environment steps) per second"
  min: 0.0

eta:
  precision: 0
  description: "Estimated seconds remaining to reach max_timesteps"
  min: 0.0

fps_instant:
  precision: 0
  description: "Instant training throughput in frames per second measured within the current epoch"
  min: 0.0

# Loss metrics
policy_loss:
  precision: 4
  description: "Policy network loss from gradient ascent on expected returns"
  algorithm_rules:
    reinforce:
      threshold: 1000.0
      condition: "less_than"
      message: "Very high policy loss ({current_value:.3f}) may indicate training instability."
      level: "warning"

value_loss:
  precision: 4
  description: "Value function loss measuring prediction accuracy of state values"

entropy_loss:
  precision: 4
  description: "Negative policy entropy used as regularization term"

total_loss:
  precision: 4
  description: "Combined total loss including policy, value and entropy components"

# Algorithm-specific metrics
approx_kl:
  precision: 4
  description: "Approximate KL divergence between old and new policy distributions"
  min: 0.0
  max: 0.1
  algorithm_rules:
    ppo:
      threshold: 0.1
      condition: "less_than"
      message: "High approximate KL divergence ({current_value:.4f}) indicates large policy changes. Consider reducing learning rate."
      level: "warning"

kl_div:
  precision: 4
  description: "KL divergence between old and new policy log probabilities"
  algorithm_rules:
    ppo:
      threshold: 0.1
      condition: "less_than"
      message: "High KL divergence ({current_value:.4f}) indicates large policy changes. Consider reducing learning rate."
      level: "warning"

clip_fraction:
  precision: 3
  description: "Fraction of probability ratios that were clipped in PPO objective"
  min: 0.0
  max: 1.0
  algorithm_rules:
    ppo:
      threshold: 0.2
      condition: "less_than"
      message: "High clip fraction ({current_value:.3f}) indicates policy is changing too rapidly. Consider reducing learning rate or clip_range."
      level: "warning"

explained_variance:
  precision: 3
  description: "How well the value function predicts returns (1.0 = perfect, 0.0 = no better than mean)"
  min: -1.0
  max: 1.0
  algorithm_rules:
    ppo:
      threshold: -0.5
      condition: "greater_than"
      message: "Very negative explained variance ({current_value:.3f}) indicates value function is performing poorly. Check value function architecture or learning rate."
      level: "warning"

entropy:
  precision: 4
  description: "Policy entropy measuring exploration vs exploitation trade-off"
  min: 0.0
  algorithm_rules:
    ppo:
      threshold: 0.1
      condition: "greater_than"
      message: "Low entropy ({current_value:.3f}) indicates policy is becoming too deterministic. Consider increasing entropy coefficient."
      level: "warning"
    reinforce:
      threshold: 0.05
      condition: "greater_than"
      message: "Low entropy ({current_value:.3f}) in REINFORCE indicates policy is becoming too deterministic. Consider increasing entropy coefficient."
      level: "warning"

# Learning rates
policy_lr:
  precision: 6
  description: "Current learning rate for policy network optimizer"
  min: 0.0

value_lr:
  precision: 6
  description: "Current learning rate for value network optimizer"
  min: 0.0

policy_lr:
  precision: 6
  description: "General learning rate for optimizer"
  min: 0.0

# Gradient metrics
grad_norm:
  precision: 4
  description: "L2 norm of policy network gradients before clipping"
  min: 0.0

max_grad_norm:
  precision: 4
  description: "Maximum allowed gradient norm for gradient clipping"
  min: 0.0

# Algorithm-specific metrics
advantage_mean:
  precision: 4
  description: "Mean advantage estimate across batch (should be near zero when normalized)"

advantage_std:
  precision: 4
  description: "Standard deviation of advantage estimates across batch"
  min: 0.0

value_mean:
  precision: 4
  description: "Mean value function estimate across states in batch"

value_std:
  precision: 4
  description: "Standard deviation of value function estimates across states in batch"
  min: 0.0
