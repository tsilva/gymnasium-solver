# Metric-centric configuration
# Each metric is defined as a top-level entry with all its properties
# Metric names should be without namespace prefixes since the same metric
# can appear across different namespaces (train/, eval/, rollout/, etc.)

# Global configuration
_global:
  default_precision: 4  # Default precision for metrics not explicitly configured

# Episode and timestep counts (integers)
total_episodes:
  precision: 0
  force_integer: true
  delta_rule: "non_decreasing"  # Should always increase
  description: "Total number of episodes completed"

total_timesteps:
  precision: 0
  force_integer: true
  # Note: delta_rule removed because eval/total_timesteps can legitimately decrease
  # between evaluation runs when using different eval strategies or env resets
  description: "Total number of timesteps processed"

rollout_timesteps:
  precision: 0
  force_integer: true
  delta_rule: "non_decreasing"  # Should always increase
  description: "Total number of timesteps in the current rollout"

# Episode statistics
ep_rew_mean:
  precision: 2
  description: "Mean episode reward"

ep_len_mean:
  precision: 2
  description: "Mean episode length"

ep_rew_min:
  precision: 2
  description: "Minimum episode reward"

ep_rew_max:
  precision: 2
  description: "Maximum episode reward"

ep_len_min:
  precision: 0
  force_integer: true
  description: "Minimum episode length"

ep_len_max:
  precision: 0
  force_integer: true
  description: "Maximum episode length"

ep_rew_std:
  precision: 2
  description: "Episode reward standard deviation"

ep_len_std:
  precision: 2
  description: "Episode length standard deviation"

# Training metrics
epoch:
  precision: 0
  force_integer: true
  delta_rule: "non_decreasing"  # Should always increase
  description: "Training epoch"

n_updates:
  precision: 0
  force_integer: true
  delta_rule: "non_decreasing"  # Should always increase
  description: "Number of updates"

# Time metrics
time_elapsed:
  precision: 2
  delta_rule: "non_decreasing"  # Time should always increase
  description: "Elapsed time in seconds"

fps:
  precision: 0
  force_integer: true
  description: "Frames per second"

# Loss metrics
policy_loss:
  precision: 4
  description: "Policy loss"
  algorithm_rules:
    reinforce:
      threshold: 1000.0
      condition: "less_than"
      message: "Very high policy loss ({current_value:.3f}) may indicate training instability."
      level: "warning"

value_loss:
  precision: 4
  description: "Value function loss"

entropy_loss:
  precision: 4
  description: "Entropy loss"

total_loss:
  precision: 4
  description: "Total loss"

# Algorithm-specific metrics
approx_kl:
  precision: 4
  description: "Approximate KL divergence"
  algorithm_rules:
    ppo:
      threshold: 0.1
      condition: "less_than"
      message: "High approximate KL divergence ({current_value:.4f}) indicates large policy changes. Consider reducing learning rate."
      level: "warning"

kl_div:
  precision: 4
  description: "KL divergence"
  algorithm_rules:
    ppo:
      threshold: 0.1
      condition: "less_than"
      message: "High KL divergence ({current_value:.4f}) indicates large policy changes. Consider reducing learning rate."
      level: "warning"

clip_fraction:
  precision: 3
  description: "Clipping fraction"
  algorithm_rules:
    ppo:
      threshold: 0.5
      condition: "less_than"
      message: "High clip fraction ({current_value:.3f}) indicates policy is changing too rapidly. Consider reducing learning rate or clip_range."
      level: "warning"

explained_variance:
  precision: 3
  description: "Explained variance"
  algorithm_rules:
    ppo:
      threshold: -0.5
      condition: "greater_than"
      message: "Very negative explained variance ({current_value:.3f}) indicates value function is performing poorly. Check value function architecture or learning rate."
      level: "warning"

entropy:
  precision: 4
  description: "Policy entropy"
  algorithm_rules:
    ppo:
      threshold: 0.1
      condition: "greater_than"
      message: "Low entropy ({current_value:.3f}) indicates policy is becoming too deterministic. Consider increasing entropy coefficient."
      level: "warning"
    reinforce:
      threshold: 0.05
      condition: "greater_than"
      message: "Low entropy ({current_value:.3f}) in REINFORCE indicates policy is becoming too deterministic. Consider increasing entropy coefficient."
      level: "warning"

# Learning rates
policy_lr:
  precision: 6
  description: "Policy learning rate"

value_lr:
  precision: 6
  description: "Value learning rate"

learning_rate:
  precision: 6
  description: "General learning rate"

# Gradient metrics
grad_norm:
  precision: 4
  description: "Gradient norm"

max_grad_norm:
  precision: 4
  description: "Maximum gradient norm"

# Algorithm-specific metrics
advantage_mean:
  precision: 4
  description: "Mean advantage"

advantage_std:
  precision: 4
  description: "Advantage standard deviation"

value_mean:
  precision: 4
  description: "Mean value estimate"

value_std:
  precision: 4
  description: "Value estimate standard deviation"
