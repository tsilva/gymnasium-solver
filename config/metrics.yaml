# Metric-centric configuration
# Each metric is defined as a top-level entry with all its properties
# Metric names should be without namespace prefixes since the same metric
# can appear across different namespaces (train/, eval/, rollout/, etc.)


# Global configuration
_global:
  # Preferred display order for key metrics in the stdout table
  # Keys not listed here will follow alphabetically within their section
  key_priority:
    - "train/ep_rew_mean"
    - "train/ep_rew_best"
    - "train/ep_rew_last"
    - "train/ep_len_mean"
    - "train/ep_len_last"
    - "train/approx_kl"
    - "train/clip_fraction"
    - "train/entropy"
    - "train/action_std"
    - "train/policy_loss"
    - "train/value_loss"
    - "train/explained_variance"
    - "train/kl_div"
    - "train/entropy_loss"
    - "train/value_loss_scaled"
    - "train/loss"
    - "train/grad_norm/all"
    - "train/grad_norm/backbone"
    - "train/grad_norm/policy_head"
    - "train/grad_norm/value_head"
    - "train/obs_mean"
    - "train/obs_std"
    - "train/reward_mean"
    - "train/reward_std"
    - "train/total_timesteps"
    - "train/epoch"
    - "train/time_elapsed"
    - "train/total_episodes"
    - "train/total_rollouts"
    - "train/rollout_timesteps"
    - "train/rollout_episodes"
    - "train/fps"
    - "train/rollout_fps"
    - "train/fps_instant"
    - "train/eta_s"
    - "train/hp/policy_lr"
    - "train/hp/clip_range"
    - "train/hp/ent_coef"
    - "train/baseline_mean"
    - "train/baseline_std"
    - "train/entropy_loss_scaled"

    - "val/ep_rew_mean"
    - "val/ep_rew_best"
    - "val/ep_rew_last"
    - "val/ep_len_mean"
    - "val/ep_len_last"
    - "val/approx_kl"
    - "val/clip_fraction"
    - "val/entropy"
    - "val/action_std"
    - "val/policy_loss"
    - "val/value_loss"
    - "val/explained_variance"
    - "val/kl_div"
    - "val/entropy_loss"
    - "val/value_loss_scaled"
    - "val/loss"
    - "val/grad_norm/all"
    - "val/grad_norm/backbone"
    - "val/grad_norm/policy_head"
    - "val/grad_norm/value_head"
    - "val/obs_mean"
    - "val/obs_std"
    - "val/reward_mean"
    - "val/reward_std"
    - "val/total_timesteps"
    - "val/epoch"
    - "val/time_elapsed"
    - "val/total_episodes"
    - "val/total_rollouts"
    - "val/rollout_timesteps"
    - "val/rollout_episodes"
    - "val/fps"
    - "val/rollout_fps"
    - "val/fps_instant"
    - "val/eta_s"
    - "val/hp/policy_lr"
    - "val/hp/clip_range"
    - "val/hp/ent_coef"
    - "val/baseline_mean"
    - "val/baseline_std"
    - "val/entropy_loss_scaled"
  
  step_key: "train/total_timesteps"

clip_range:
  description: "PPO clip range for policy updates (ratio clamp around 1.0)"
  precision: 4
  min: 0.0
  max: 1.0
      
# Episode and timestep counts (integers)
total_episodes:
  precision: 0
  description: "Cumulative number of episodes completed across all training"
  delta_rule: ">="  # Should always increase

total_timesteps:
  highlight: true
  # Note: delta_rule removed because eval/total_timesteps can legitimately decrease
  # between evaluation runs when using different eval strategies or env resets
  description: "Cumulative number of environment steps taken across all training"
  precision: 0

rollout_timesteps:
  description: "Number of timesteps collected in the current rollout buffer"
  precision: 0
  delta_rule: ">="  # Should always increase

# Additional counters
total_rollouts:
  description: "Cumulative number of completed rollouts (batches) collected"
  precision: 0
  delta_rule: ">="

rollout_episodes:
  description: "Number of episodes finished within the current rollout"
  precision: 0

# Episode statistics
ep_rew_mean:
  bold: true
  highlight: true
  description: "Average episode reward over recent episodes window"
  precision: 2

ep_len_mean:
  description: "Average episode length over recent episodes window"
  precision: 0

# Immediate episode stats
ep_rew_last:
  bold: true
  highlight: true
  description: "Reward of the most recent completed episode"
  precision: 2

ep_len_last:
  description: "Length of the most recent completed episode"
  precision: 0

ep_rew_best:
  bold: true
  highlight: true
  description: "Running best of ep_rew_mean across epochs"
  precision: 2

ep_rew_min:
  description: "Minimum episode reward observed in recent episodes window"
  precision: 2

ep_rew_max:
  description: "Maximum episode reward observed in recent episodes window"
  precision: 2

ep_len_min:
  precision: 0
  description: "Minimum episode length observed in recent episodes window"

ep_len_max: 
  description: "Maximum episode length observed in recent episodes window"
  precision: 0

ep_rew_std:
  description: "Standard deviation of episode rewards in recent episodes window"
  precision: 2

ep_len_std:
  description: "Standard deviation of episode lengths in recent episodes window"
  precision: 2

# Training metrics
epoch:
  description: "Current training epoch number"
  precision: 0
  delta_rule: ">="  # Should always increase

n_updates:
  description: "Total number of parameter updates (gradient steps) performed"
  precision: 0
  delta_rule: ">="  # Should always increase

# Time metrics
time_elapsed:
  description: "Total elapsed training time in seconds"
  precision: 2
  delta_rule: ">="  # Time should always increase
  min: 0.0

fps:
  description: "Training throughput in frames (environment steps) per second"
  precision: 0
  min: 0.0

eta_s:
  description: "Estimated seconds remaining to reach max_timesteps"
  precision: 0
  min: 0.0

fps_instant:
  description: "Instant training throughput in frames per second measured within the current epoch"
  precision: 0
  min: 0.0

epoch_fps:
  description: "Throughput (steps/s) computed over the last validation epoch"
  precision: 0
  min: 0.0

# Loss metrics
policy_loss:
  description: "Policy network loss from gradient ascent on expected returns"
  precision: 4

value_loss:
  description: "Value function loss measuring prediction accuracy of state values"
  precision: 4

loss:
  highlight: true
  description: "Total loss used for backprop (policy + value + entropy terms)"
  precision: 4

entropy_loss:
  description: "Negative policy entropy used as regularization term"
  precision: 4

total_loss:
  description: "Combined total loss including policy, value and entropy components"
  precision: 4

# Algorithm-specific metrics
approx_kl:
  description: "Approximate KL divergence between old and new policy distributions"
  precision: 4
  min: 0.0
  max: 0.1

kl_div:
  description: "KL divergence between old and new policy log probabilities"
  precision: 4

clip_fraction:
  description: "Fraction of probability ratios that were clipped in PPO objective"
  precision: 3
  min: 0.0
  max: 1.0

explained_variance:
  description: "How well the value function predicts returns (1.0 = perfect, 0.0 = no better than mean)"
  precision: 3
  min: -1.0
  max: 1.0

entropy:
  description: "Policy entropy measuring exploration vs exploitation trade-off"
  precision: 4
  min: 0.0



# Learning rates
policy_lr:
  description: "Current learning rate for policy network optimizer"
  precision: 6
  min: 0.0

value_lr:
  description: "Current learning rate for value network optimizer"
  precision: 6
  min: 0.0

# Gradient metrics
grad_norm:
  description: "L2 norm of policy network gradients before clipping"
  precision: 4
  min: 0.0

max_grad_norm:
  description: "Maximum allowed gradient norm for gradient clipping"
  precision: 4
  min: 0.0

# Granular gradient norms
grad_norm/backbone:
  description: "Gradient L2 norm for shared backbone parameters"
  precision: 4
  min: 0.0

grad_norm/policy_head:
  description: "Gradient L2 norm for policy head"
  precision: 4
  min: 0.0

grad_norm/value_head:
  description: "Gradient L2 norm for value head"
  precision: 4
  min: 0.0

# Algorithm-specific metrics
advantage_mean:
  description: "Mean advantage estimate across batch (should be near zero when normalized)"
  precision: 4

advantage_std:
  description: "Standard deviation of advantage estimates across batch"
  precision: 4
  min: 0.0

value_mean:
  description: "Mean value function estimate across states in batch"
  precision: 4

value_std:
  description: "Standard deviation of value function estimates across states in batch"
  precision: 4
  min: 0.0

# Observations and rewards statistics (from collectors)
obs_mean:
  description: "Running mean of observations after preprocessing/normalization"
  precision: 3

obs_std:
  description: "Running standard deviation of observations after preprocessing/normalization"
  precision: 3
  min: 0.0

reward_mean:
  description: "Running mean of raw rewards (pre-normalization)"
  precision: 3

reward_std:
  description: "Running standard deviation of raw rewards (pre-normalization)"
  precision: 3
  min: 0.0

action_mean:
  description: "Mean of actions taken (discrete: mean index; continuous: mean magnitude)"
  precision: 3

action_std:
  description: "Standard deviation of actions (proxy for exploration for continuous control)"
  precision: 3
  min: 0.0

rollout_fps:
  description: "Collector throughput in frames per second (rollout only)"
  precision: 0
  min: 0.0

baseline_mean:
  description: "Running mean of the baseline used for returns normalization"
  precision: 3

baseline_std:
  description: "Running std of the baseline used for returns normalization"
  precision: 3
  min: 0.0

# Hyperparameter mirrors logged under hp/*
hp/policy_lr:
  description: "Policy learning rate at start (static reference)"
  precision: 6
  min: 0.0

hp/clip_range:
  description: "Clip range at start (static reference)"
  precision: 4
  min: 0.0

hp/ent_coef:
  description: "Entropy coefficient at start (static reference)"
  precision: 6
  min: 0.0
