# Acrobot-v1 Environment Configuration
# Reference: https://github.com/DLR-RM/rl-baselines3-zoo/tree/master/hyperparams

default:
  eval_reward_threshold: -100          # Environment is considered solved at -100

  hidden_dims: [64, 64]                # Sufficient capacity for Acrobot
  policy_lr: 1e-3                      # Default policy LR (overridden per algo below)
  value_lr: 1e-3                       # Default value LR (overridden per algo below)
  entropy_coef: 0.01                   # Small entropy for exploration

  train_rollout_interval: 1            # Frequent rollouts for fast feedback
  n_steps: 2048            # Standard rollout size for stability
  train_reward_threshold: null         # Not used for Acrobot
  batch_size: 64                 # Smaller batch for more frequent updates
  gamma: 0.99                          # Slight discount for stability (gamma=1.0 can cause instability)
  gae_lambda: 0.95                     # Standard GAE lambda for bias/variance tradeoff
  clip_epsilon: 0.2                    # Standard PPO clipping

  normalize_obs: false                 # Normalize observations for better learning
  normalize_reward: false              # Reward normalization not needed for Acrobot

  mean_reward_window: 100              # Standard window for reward smoothing

reinforce:
  policy_lr: 5e-4                      # Lower LR for REINFORCE, helps with unstable gradients
  entropy_coef: 0.02                   # Slightly higher entropy for more exploration

ppo:
  policy_lr: 1e-3                      # Standard LR for PPO, stable and fast convergence
  value_lr: 1e-3                       # Standard value network LR
  clip_epsilon: 0.2                    # Standard PPO clipping
  gae_lambda: 0.95                     # Standard GAE lambda for bias/variance tradeoff
  entropy_coef: 0.01                   # Small entropy for PPO, Acrobot doesn't need much exploration