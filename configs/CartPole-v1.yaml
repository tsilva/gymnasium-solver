# CartPole-v1 Environment Configuration
# Reference: https://github.com/DLR-RM/rl-baselines3-zoo/tree/master/hyperparams

default:
  # TODO: check how big n_envs are handled by default (do I need to add a max() in code?)
  n_envs: 8 
  n_steps: 32
  batch_size: 256


 # n_steps: 2048            # Standard rollout size for CartPole, balances speed and stability


  eval_reward_threshold: 475           # Environment is considered solved at 475

  hidden_dims: [64, 64]                # Sufficient capacity for CartPole
  policy_lr: 3e-3                      # Default policy LR (overridden per algo below)
  value_lr: 1e-3                       # Default value LR (overridden per algo below)
  entropy_coef: 0.01                   # Small entropy for minimal exploration (CartPole is easy to solve)

  train_rollout_interval: 1            # Frequent rollouts for fast feedback
  train_reward_threshold: null         # Not used for CartPole
  batch_size: 64                 # Smaller batch for more frequent updates, helps with fast learning
  gamma: 0.99                          # Slight discount to improve stability (gamma=1.0 can cause instability)
  gae_lambda: 0.95                     # Standard GAE lambda for bias/variance tradeoff
  clip_epsilon: 0.2                    # Standard PPO clipping

  normalize_obs: false                 # Normalize observations for better learning
  normalize_reward: false              # Reward normalization not needed for CartPole

  mean_reward_window: 100              # Standard window for reward smoothing

reinforce:
  policy_lr: 1e-3                      # Standard learning rate for REINFORCE
  entropy_coef: 0.02                   # Slightly higher entropy for more exploration

ppo:
  policy_lr: 3e-4                      # Higher LR for faster convergence with PPO
  value_lr: 3e-4                       # Standard value network LR
  clip_epsilon: 0.2                    # Standard PPO clipping
  gae_lambda: 0.95                     # Standard GAE lambda for bias/variance tradeoff
  entropy_coef: 0.00                   # Small entropy for PPO, CartPole doesn't need much exploration


  # TODO: use cpu
  #n_timesteps: !!float 1e5
  #policy: 'MlpPolicy'
  #gae_lambda: 0.8
  #gamma: 0.98
  #n_epochs: 20
  #ent_coef: 0.0
  #learning_rate: lin_0.001
  #clip_range: lin_0.2