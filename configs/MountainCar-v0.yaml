ppo:
  # Environment settings
  n_envs: 16
  normalize_obs: true              # Normalize observations for better learning
  normalize_reward: false          # Keep original reward structure
  reward_shaping:                  # Enable dense reward shaping
    position_reward_scale: 100.0   # Reward for position progress toward goal
    velocity_reward_scale: 10.0    # Reward for positive velocity
    height_reward_scale: 50.0      # Reward for gaining height on mountain
  
  # Training hyperparameters
  n_steps: 128                     # Longer rollouts for MountainCar
  batch_size: 128                  # Larger batches for stability
  n_epochs: 10                     # More epochs per update
  gae_lambda: 0.95                 # Standard GAE parameter
  gamma: 0.999                     # High discount for sparse rewards
  ent_coef: 0.01                   # Encourage exploration
  clip_range: 0.2                  # Standard PPO clipping
  
  # Frame stacking
  frame_stack: 1                       # Stack 2 frames for temporal information

  # Network architecture
  hidden_dims: [256, 256]          # Larger network for complex dynamics
  policy_lr: 3e-4                  # Conservative learning rate
  
  # Training limits
  max_epochs: 1000                 # Allow longer training