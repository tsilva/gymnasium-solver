# MountainCar-v0 Environment Configuration
# Reference: https://github.com/DLR-RM/rl-baselines3-zoo/tree/master/hyperparams

default:
  hidden_dims: [128, 128]             # More capacity helps model long-term strategies
  policy_lr: 1e-3                     # Default policy LR (overridden per algo below)
  value_lr: 1e-3                      # Default value LR (overridden per algo below)
  entropy_coef: 0.15                  # Encourage exploration for sparse reward

  max_epochs: 500                     # Limit training to 500 epochs for efficiency
  train_rollout_interval: 1           # Frequent rollouts help with sparse rewards
  train_rollout_steps: 10000          # Longer rollouts help with sparse rewards
  train_reward_threshold: null        # Not used for MountainCar
  train_batch_size: 1000              # Large batch for stable updates in sparse reward env
  gamma: 0.99                         # Slight discount to avoid instability from infinite horizon
  gae_lambda: 0.95                    # Standard GAE lambda for bias/variance tradeoff
  clip_epsilon: 0.2                   # Standard PPO clipping

  eval_rollout_interval: 10           # Evaluate every 10 epochs
  eval_rollout_episodes: 5            # Fewer episodes for faster eval, enough for stability
  eval_reward_threshold: -110         # Environment is considered solved around this value

  normalize_obs: true                 # Normalize observations for better learning
  normalize_reward: false             # Reward normalization not needed for this env

  mean_reward_window: 100             # Standard window for reward smoothing

reinforce:
  policy_lr: 5e-4                     # Lower LR due to unstable gradients from sparse rewards
  entropy_coef: 0.2                   # Extra exploration for REINFORCE in sparse env

ppo:
  policy_lr: 1e-3                     # Higher LR for faster convergence with PPO
  value_lr: 1e-3                      # Standard value network LR
  clip_epsilon: 0.2                   # Standard PPO clipping
  gae_lambda: 0.95                    # Standard GAE lambda for bias/variance tradeoff
  entropy_coef: 0.1                   # Encourage exploration, but less than REINFORCE

