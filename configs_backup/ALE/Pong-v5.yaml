ppo:
  # Environment settings
  n_envs: 8                        # Parallel environments for faster training
  obs_type: "ram"                  # Use RAM observations (128 bytes) instead of RGB images
  normalize_obs: true             # Don't normalize RAM observations initially
  normalize_reward: false          # Keep original reward structure
  
  # Training hyperparameters tuned for Pong RAM
  n_steps: 256                     # Rollout length
  batch_size: 64                   # Batch size for updates
  n_epochs: 4                      # Epochs per update
  gae_lambda: 0.95                 # GAE parameter
  gamma: 0.99                      # Discount factor
  ent_coef: 0.01                   # Entropy coefficient for exploration
  clip_range: 0.2                  # PPO clipping parameter
  policy_lr: 0.00025               # Learning rate (slightly lower for Atari)
  
  # Network architecture for RAM input (128-dimensional)
  hidden_dims: [256, 256]          # Larger networks for RAM processing
  
  # Frame stacking not needed for RAM observations
  frame_stack: 1
  
  # Evaluation settings
  eval_freq: 50        # Evaluate every 50 training episodes
  eval_episodes: 10        # Number of episodes for evaluation
  eval_async: true                 # Async evaluation

# Alternative RGB configuration (commented out)
# ppo_rgb:
#   n_envs: 4                      # Fewer envs due to larger obs space
#   obs_type: "rgb"                # Use RGB observations (210x160x3)
#   normalize_obs: true            # Normalize pixel values
#   n_steps: 128
#   batch_size: 32
#   hidden_dims: [512, 512]        # Larger networks for image processing
#   frame_stack: 4                 # Frame stacking for temporal info
