reinforce:
  n_envs: 8
  n_epochs: 1                 # Don't reuse data!
  n_steps: 512                # Collect full episodes if possible
  batch_size: 64              # Smaller for high-variance gradient
  gamma: 0.99                 # Less discounting
  ent_coef: 0.01              # Encourage exploration
  policy_lr: 5e-4             # Lower LR for stability
  normalize_obs: false
  normalize_reward: false      # Normalize to stabilize learning
  hidden_dims: [64]
  frame_stack: 3

ppo:
  n_envs: 8
  #n_timesteps: !!float 1e5
  #policy: 'MlpPolicy'
  n_epochs: 20
  n_steps: 32
  batch_size: 256
  gae_lambda: 0.8
  gamma: 0.98
  ent_coef: 0.0
  #learning_rate: lin_0.001
  clip_range: 0.2
  # TODO: add linear decay support
  #clip_range: lin_0.2

  # Frame stacking
  # TODO: this is broken
  frame_stack: 1                       # Stack 2 frames for temporal information

  # TODO: deprecate these, follow rlzoo style
  hidden_dims: [64]                # Sufficient capacity for CartPole
  policy_lr: 1e-3                      # Default policy LR (overridden per algo below)
  normalize_obs: false                 # Normalize observations for better learning
  normalize_reward: false              # Reward normalization not needed for CartPole