ğŸ“Œ n_envs (number of parallel environments)
- âœ… Increases rollout size.
- âœ… More diverse data â†’ less correlated samples â†’ cleaner gradient estimates.
- âœ… Faster data collection via parallelization.
- â›” Higher memory usage.
- â›” Improves only spatial diversity (different states at same timestep), not temporal credit assignment.

â¸»

ğŸ“Œ n_steps (steps per environment before update)
- âœ… Increases rollout size.
- âœ… Better temporal credit assignment (exposes policy to longer returns).
- âœ… Reduces bootstrap bias in actorâ€“critic methods.
- â›” Consecutive samples are correlated (less diverse).
- â›” Slows updates (must wait for all steps before training).
- â›” Risk of GPU idle time if rollout collection is CPU-bound.

â¸»

ğŸ“Œ Rules of thumb
- To ensure reproducibility, perform two runs in a row and ensure all wandb graphs are identical.
- Don't bother with running evals until you reach treshold on training.
- âœ… Max out n_envs as far as CPU allows â€” cheap way to scale diversity and speed.
- âœ… Set n_steps long enough to capture temporal structure, but short enough to avoid stalling updates.
- âœ… For REINFORCE, aim for larger effective rollout sizes (n_envs Ã— n_steps) to tame high-variance returns.
- âœ… Use reward-to-go and baselines to cut variance further instead of only relying on bigger rollouts.




--- 

Great question â€” batch size is the third lever alongside n_envs and n_steps, since it controls how much of your collected data is actually used in one gradient update.

Here are the rules of thumb for batch sizes in policy gradient / actorâ€“critic settings:

â¸»

ğŸ“Œ Batch Size (data used per gradient update)
	â€¢	âœ… Larger batch sizes
	â€¢	Lower gradient variance â†’ more stable learning.
	â€¢	More efficient use of GPU (parallel matrix ops).
	â€¢	Especially useful for on-policy methods (PPO, REINFORCE) where sample efficiency is limited.
	â€¢	â›” Too large batch sizes
	â€¢	Slower iteration speed (fewer updates per unit of data).
	â€¢	Risk of underfitting if learning rate is not scaled accordingly.
	â€¢	Memory bottlenecks.
	â€¢	âœ… Smaller batch sizes
	â€¢	More frequent updates â†’ faster reaction to data.
	â€¢	Potentially better exploration through noisier gradients.
	â€¢	â›” Too small batch sizes
	â€¢	Extremely noisy gradients â†’ unstable training.
	â€¢	Wastes parallel compute since GPUs run less efficiently on tiny batches.

â¸»

ğŸ“Œ Rules of thumb
	â€¢	Batch size â‰ˆ 32â€“256 per update is a good starting range for most RL workloads (mirrors supervised learning practice).
	â€¢	In PPO / A2C, set batch_size as a fraction of n_envs Ã— n_steps (commonly minibatches = 4â€“32 splits).
	â€¢	A good heuristic:
	â€¢	Keep batch_size â‰¥ action_dim Ã— 10 to avoid overly noisy gradients.
	â€¢	Ensure each batch contains samples from multiple environments (so youâ€™re not overfitting to one trajectory slice).
	â€¢	For REINFORCE / high-variance returns, bigger batches (â‰¥1k samples) are often necessary for stable updates.
	â€¢	For continuous control (MuJoCo, PyBullet, etc.), ~2â€“10k samples per update (via multiple envs Ã— steps, split into minibatches) tends to be standard.

â¸»

âš–ï¸ Balance tip:
	â€¢	Effective rollout size = n_envs Ã— n_steps.
	â€¢	From this pool, choose batch_size so that you have at least 4â€“10 minibatches per epoch.
	â€¢	Example: if rollout size = 8192, set batch_size = 256 â†’ 32 minibatches per epoch.

â¸»

ğŸ‘‰ Do you want me to make a summary diagram showing how n_envs, n_steps, and batch_size interact (like a flow of rollout â†’ minibatching â†’ updates)? That could complement your guide nicely.


0) Set the ground rules (once)

Determinism & logging

Fix seeds; turn off nondeterministic ops; log everything (returns, entropy, grad-norm, loss, FPS).

Keep an evaluation protocol: every N updates, run 5â€“10 eval episodes with exploration off; track mean/median and IQR.

Throughput check

Profile sample collection vs. update time. Aim for â‰¥70% GPU/TPU utilization during updates and no long GPU idle gaps.

1) Max out data pipeline

n_envs (parallel envs)

Increase until CPU is ~80â€“90% loaded or envs start thrashing RAM.

If GPU idles while collecting, add envs before touching anything else.

Rollout sizing (batch_size for REINFORCE)

REINFORCE is high-variance â†’ start with bigger updates.

Starting points

Discrete control: 2â€“8k timesteps/update.

Continuous control: 5â€“20k timesteps/update.

Ensure â‰¥16â€“64 episodes per batch (use many envs so youâ€™re not dominated by a few long trajectories).

If returns swing wildly across updates: double batch; if learning crawls: halve batch.

Tip: If youâ€™re also using n_steps, choose it to keep updates frequent (e.g., collect until batch_size is met across envs) rather than requiring full episodes.

2) Core optimizer sweep

Learning rate (policy)

Log-sweep: {3e-5, 1e-4, 3e-4, 1e-3, 3e-3} (Adam).

Pick by area under the learning curve (sample-efficiency + final return).

If you increase batch size, either keep LR and resweep, or try ~linear-ish scaling up to ~4Ã— before resweeping.

Epochs per batch

REINFORCE can overfit a batch quickly. Try {1, 2, 4}; prefer 1â€“2 if you see training loss collapse while eval return stalls.

Gradient clipping

Global-norm {0.5, 1.0, 2.0}.

If you see sporadic loss spikes or NaNs: lower clip or LR.

3) Variance reduction (must-have)

Reward-to-Go

Use it. It strictly reduces variance vs. full-episode returns. No sweep needed.

Baseline (state-value) & advantage norm

Add a learned V(s) baseline (a tiny MLP).

Train with MSE; value-loss coeff ~0.5; value LR = policy LR (try {1Ã—, 2Ã—}).

Normalize advantages per batch (zero mean, unit std). Always onâ€”donâ€™t sweep.

Reward/return normalization

If reward scales drift across tasks or time, normalize returns by a running std (or PopArt). Turn on if learning is twitchy.

4) Horizon & exploration

Discount factor Î³

Start 0.99.

Use the horizon heuristic: target effective H â‰ˆ 1/(1âˆ’Î³). For task horizon T, try Î³ â‰ˆ 1 âˆ’ 1/(0.25T â€¦ 1.0T).

Sweep small set: {0.95, 0.98, 0.99, 0.995, 0.999}.

Symptoms: too myopic â†’ improve with higher Î³; credit assignment too fuzzy/slow â†’ lower Î³.

Entropy bonus (exploration)

Start 0.01 (discrete) / 0.001 (continuous). Sweep {Ã—0.3, Ã—1, Ã—3}.

Then anneal to 0 over the last 50â€“80% of training.

If policy collapses early: raise entropy or slow the anneal.

5) Finishing touches

Optimizer betas / weight decay

Adam Î²2 0.999 (default). If value loss lags, try Î²2 0.99.

Tiny L2 (e.g., 1e-4) only if you see overfitting on short horizons.

Observation normalization

Running mean/var per state dim. Essential for continuous control or nonstationary rewards.

Learning-rate schedule

Cosine or linear decay to 0; warmup 0â€“2k updates if gradients are spiky at start.

6) How to run the tuning (playbook)

Phase A (throughput): tune n_envs â†’ pick a stable batch_size (Section 1).

Phase B (stability): sweep LR Ã— epochs Ã— clip (Sections 5â€“7).

Phase C (variance): enable baseline + adv-norm; if still noisy, add return normalization (Sections 9â€“10).

Phase D (horizon/exploration): sweep Î³ Ã— entropy small grids (Sections 11â€“12).

Phase E (polish): LR schedule, observation norm, tiny L2 (Section 13â€“15).

Budgeting: use coarse sweeps first (1â€“2 seeds, 0.5â€“1h runs), then re-run top 2â€“3 configs with 3â€“5 seeds for your final pick.

7) Quick starting configs

Discrete (e.g., Atari-like):

batch_size 4â€“8k, epochs 1â€“2, LR 3e-4, Î³ 0.99, entropy 0.01â†’0, clip 1.0.

Continuous (e.g., MuJoCo):

batch_size 10â€“20k, epochs 1â€“2, LR 1e-4â€“3e-4, Î³ 0.99â€“0.995, entropy 0.001â†’0, clip 1.0.

8) Debug/triage cheatsheet

Divergence/NaNs â†’ lower LR, add/raise clip, bigger batch, check obs/reward norms.

Wild update-to-update swings â†’ bigger batch, advantage/return norm, stronger baseline.

Stuck learning â†’ raise LR a notch, add epochs (to 2), increase entropy (temporarily), or bump Î³.

Overfitting a batch (train loss â†“, eval flat) â†’ fewer epochs (1), larger batch, stronger entropy.

If you want, I can turn this into a one-page checklist you can drop into your repoâ€™s README.