CartPole-v1:
  env_id: CartPole-v1
  n_envs: 8
  n_timesteps: !!float 1e5
  policy: 'MlpPolicy'
  frame_stack: 1
  n_steps: 32
  batch_size: 256
  gae_lambda: 0.8
  gamma: 0.98
  ent_coef: 0.0
  n_epochs: 20
  learning_rate: 1e-3
  clip_range: 0.2
  hidden_dims: [64]
  reward_threshold: 500

CartPole-v1_normalized:
  inherit_from: CartPole-v1
  normalize_obs: "static"

Acrobot-v1:
  env_id: Acrobot-v1
  n_envs: 16
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  n_epochs: 4
  batch_size: 64
  normalize: true
  n_steps: 256
  gae_lambda: 0.94
  gamma: 0.99
  ent_coef: 0.0

LunarLander-v3:
  env_id: LunarLander-v3
  n_envs: 16
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  n_epochs: 4
  batch_size: 64
  n_steps: 1024
  gae_lambda: 0.98
  gamma: 0.999
  ent_coef: 0.01

MountainCar-v0:
  env_id: MountainCar-v0
  n_envs: 16
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  n_epochs: 4
  batch_size: 64
  normalize: false
  n_steps: 128
  batch_size: 128
  gae_lambda: 0.95
  gamma: 0.999
  n_epochs: 10
  ent_coef: 0.01
  clip_range: 0.2
  learning_rate: 3e-4
  hidden_dims: [256, 256]
  frame_stack: 1
  max_epochs: 1000

MountainCar-v0_rewardshaping:
  inherit_from: MountainCar-v0
  normalize: true
  frame_stack: 3
  env_wrappers:
    - id: RewardShaper_MountainCarV0
      position_reward_scale: 100.0
      velocity_reward_scale: 10.0
      height_reward_scale: 50.0

ALE/Pong-v5_ram:
  env_id: ALE/Pong-v5
  n_envs: 16               # Reduced for better sample efficiency
  policy: 'MlpPolicy'
  frame_stack: 1           # RAM contains full game state - no stacking needed
  #n_timesteps: !!float 10e6  # Increased training budget
  n_steps: 512             # Reduced for more frequent updates
  batch_size: 512          # Larger batches for stable gradients
  n_epochs: 10             # More epochs to better utilize data
  gae_lambda: 0.95         # Standard GAE lambda for better advantage estimation
  gamma: 0.99              # Standard Atari discount factor
  ent_coef: 0.01           # Higher entropy for better exploration
  clip_range: 0.2          # Standard PPO clipping for better updates
  learning_rate: 0.0003    # Higher learning rate for faster learning
  obs_type: "ram"
  normalize_obs: "static"
  hidden_dims: [512, 512]  # Larger network for complex RAM patterns
  eval_freq_epochs: 50     # Less frequent eval to focus on training
  eval_episodes: 10        # More episodes for reliable evaluation
  # Additional hyperparams for better performance
  vf_coef: 0.5             # Standard value function coefficient
  max_grad_norm: 0.5       # Gradient clipping for stability

ALE/Pong-v5_ram_throughput:
  env_id: ALE/Pong-v5
  policy: "MlpPolicy"
  obs_type: "ram"

  # Max-throughput knobs
  n_envs: 32                # ≈ (# of logical CPU cores − 2). Raise/lower to saturate CPU.
  frame_stack: 1            # RAM stacking is usually unnecessary and costs copies.
  normalize_obs: "static"     # or false – avoid per-step normalization work.
  hidden_dims: [128, 128]   # Smaller net = faster. Try [64, 64] for even more speed.

  # Rollout/update (amortize optimizer & Python overhead)
  n_steps: 1024             # Larger rollouts => fewer updates per wall-clock second
  batch_size: 4096          # Big minibatches => fewer optimizer steps
  n_epochs: 1               # Single pass per update maximizes throughput

  # PPO essentials (unchanged for speed)
  gae_lambda: 0.95
  gamma: 0.99
  ent_coef: 0.01
  clip_range: 0.2
  learning_rate: 0.0005     # Slightly higher LR tends to suit huge batches

  # Housekeeping
  eval_freq_epochs: 100       # Disable eval to avoid pauses
  eval_episodes: 10
  #device: "cpu"             # For Atari RAM, CPU avoids CPU↔GPU transfer overhead
  # torch_num_threads: 1    # If your trainer supports it; helps with many envs
  # vec_env: "async"        # If available: Async/Subproc vector env

Taxi-v3:
  env_id: Taxi-v3
  n_envs: 8
  n_timesteps: !!float 5e5
  policy: 'MlpPolicy'
  n_steps: 128
  batch_size: 64
  n_epochs: 4
  gae_lambda: 0.95
  gamma: 0.99
  ent_coef: 0.5
  learning_rate: 0.0003
  clip_range: 0.1
  hidden_dims: [256, 256]
