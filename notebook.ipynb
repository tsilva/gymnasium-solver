{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fae7ff8f",
   "metadata": {},
   "source": [
    "# Environment Solver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f29d81",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We will use gymnasium and torch for this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "768cf7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autoreloading of modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a074cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_ID = \"CartPole-v1\"\n",
    "#ENV_ID = \"LunarLander-v3\"\n",
    "ALGO_ID = \"PPO\"  # Change to \"REINFORCE\" for REINFORCE algorithm\n",
    "#ALGO_ID = \"PPO\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ecffa8",
   "metadata": {},
   "source": [
    "Install packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b165239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"pkg_resources is deprecated as an API.*\")\n",
    "# Suppress pygame-specific pkg_resources warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*pkg_resources.*\", category=UserWarning)\n",
    "# Suppress NSXPCSharedListener warnings on macOS\n",
    "warnings.filterwarnings(\"ignore\", message=\".*NSXPCSharedListener.*\")\n",
    "# General pygame warnings suppression\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pygame.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b028503",
   "metadata": {},
   "source": [
    "Load secrets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5101c4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsilva_notebook_utils.colab import load_secrets_into_env\n",
    "\n",
    "_ = load_secrets_into_env([\n",
    "    'WANDB_API_KEY'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82b4d5a",
   "metadata": {},
   "source": [
    "Retrieve enviroment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "834554f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tsilva_notebook_utils.torch import get_default_device\n",
    "\n",
    "DEVICE = get_default_device()\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d847b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RLConfig(env_id='CartPole-v1', seed=42, max_epochs=-1, gamma=0.99, lam=0.95, clip_epsilon=0.2, batch_size=256, train_rollout_steps=512, eval_interval=20, eval_episodes=5, reward_threshold=475, policy_lr=0.001, value_lr=0.001, hidden_dim=32, entropy_coef=0.01, normalize=False, mean_reward_window=100, rollout_interval=1, n_envs='auto', async_rollouts=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from tsilva_notebook_utils.gymnasium import build_env as _build_env, set_random_seed\n",
    "from dataclasses import dataclass\n",
    "from typing import Union, Tuple\n",
    "\n",
    "@dataclass\n",
    "class RLConfig:\n",
    "    # Environment\n",
    "    env_id: str\n",
    "    seed: int = 42\n",
    "    \n",
    "    # Training\n",
    "    max_epochs: int = -1\n",
    "    gamma: float = 0.99\n",
    "    lam: float = 0.95\n",
    "    clip_epsilon: float = 0.2\n",
    "    batch_size: int = 64\n",
    "    train_rollout_steps: int = 2048\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_interval: int = 10\n",
    "    eval_episodes: int = 32\n",
    "    reward_threshold: float = 200\n",
    "    \n",
    "    # Networks\n",
    "    policy_lr: float = 3e-4\n",
    "    value_lr: float = 1e-3\n",
    "    hidden_dim: Union[int, Tuple[int, ...]] = 64\n",
    "    entropy_coef: float = 0.01\n",
    "    \n",
    "    # Other\n",
    "    normalize: bool = False\n",
    "    mean_reward_window: int = 100\n",
    "    rollout_interval: int = 10\n",
    "    n_envs: Union[str, int] = \"auto\"\n",
    "    async_rollouts: bool = True\n",
    "    \n",
    "    # Environment-specific configurations with flat structure\n",
    "    # Use these as reference: https://github.com/DLR-RM/rl-baselines3-zoo/tree/master/hyperparams\n",
    "    ENV_CONFIGS = {\n",
    "        \"CartPole-v1\": {\n",
    "            # Default config for this environment (applies to all algorithms unless overridden)\n",
    "            \"default\": dict(\n",
    "                train_rollout_steps=512,\n",
    "                batch_size=256,\n",
    "                rollout_interval=1,\n",
    "                eval_interval=20,\n",
    "                eval_episodes=5,\n",
    "                reward_threshold=475,\n",
    "                policy_lr=1e-3,\n",
    "                value_lr=1e-3,\n",
    "                hidden_dim=32,\n",
    "            ),\n",
    "            # Algorithm-specific overrides\n",
    "            \"reinforce\": dict(\n",
    "                train_rollout_steps=2048,\n",
    "                batch_size=512,\n",
    "                policy_lr=1e-3,\n",
    "                entropy_coef=0.02,\n",
    "            ),\n",
    "        },\n",
    "        \"Acrobot-v1\": {\n",
    "            \"default\": dict(\n",
    "                gamma=0.99,\n",
    "                lam=0.98,\n",
    "                clip_epsilon=0.2,\n",
    "                batch_size=128,\n",
    "                train_rollout_steps=2048,\n",
    "                eval_interval=5,\n",
    "                reward_threshold=-100,\n",
    "                policy_lr=3e-4,\n",
    "                value_lr=3e-4,\n",
    "                hidden_dim=(128, 64),\n",
    "                entropy_coef=0.01,\n",
    "                rollout_interval=1\n",
    "            ),\n",
    "            \"reinforce\": dict(\n",
    "                train_rollout_steps=4096,\n",
    "                batch_size=256,\n",
    "                policy_lr=5e-4,\n",
    "                entropy_coef=0.05,\n",
    "            ),\n",
    "        },\n",
    "        # TODO:\n",
    "        #n_envs: 16\n",
    "        #n_epochs: 4 \n",
    "        #n_steps: 1024\n",
    "        \"LunarLander-v3\": {\n",
    "            \"default\": dict(\n",
    "                reward_threshold=200,\n",
    "                total_timesteps=1e6, # TODO: call this n_timesteps\n",
    "                gamma=0.999,\n",
    "                # TODO: this is not being propagated to collect_rollouts\n",
    "                lam=0.98, # gae_lambda: 0.98\n",
    "                clip_epsilon=0.2,\n",
    "                batch_size=64,\n",
    "                eval_interval=2,\n",
    "                policy_lr=1e-4,\n",
    "                value_lr=5e-4,\n",
    "                hidden_dim=32,\n",
    "                entropy_coef=0.01\n",
    "            ),\n",
    "            \"reinforce\": dict(\n",
    "                entropy_coef=0.03,\n",
    "                batch_size=128,\n",
    "            ),\n",
    "        },\n",
    "        \"Pendulum-v1\": {\n",
    "            \"default\": dict(\n",
    "                gamma=0.99,\n",
    "                lam=0.95,\n",
    "                clip_epsilon=0.2,\n",
    "                batch_size=64,\n",
    "                eval_interval=2,\n",
    "                eval_episodes=5,\n",
    "                reward_threshold=-200,\n",
    "                policy_lr=3e-4,\n",
    "                value_lr=1e-3,\n",
    "                hidden_dim=(128, 64),\n",
    "                entropy_coef=0.0\n",
    "            ),\n",
    "            \"reinforce\": dict(\n",
    "                entropy_coef=0.02,\n",
    "                batch_size=128,\n",
    "            ),\n",
    "        },\n",
    "        \"MountainCar-v0\": {\n",
    "            \"default\": dict(\n",
    "                gamma=0.99,\n",
    "                lam=0.97,\n",
    "                clip_epsilon=0.15,\n",
    "                batch_size=16,\n",
    "                eval_interval=2,\n",
    "                eval_episodes=10,\n",
    "                reward_threshold=-110,\n",
    "                policy_lr=1e-4,\n",
    "                value_lr=5e-4,\n",
    "                hidden_dim=(128, 64),\n",
    "                entropy_coef=0.05\n",
    "            ),\n",
    "            \"reinforce\": dict(\n",
    "                entropy_coef=0.08,\n",
    "                batch_size=32,\n",
    "            ),\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def create(cls, env_id: str, algorithm: str = \"ppo\") -> 'RLConfig':\n",
    "        \"\"\"\n",
    "        Create config with hierarchical overrides:\n",
    "        1. Start with dataclass defaults\n",
    "        2. Apply environment default config\n",
    "        3. Apply algorithm-specific config for that environment\n",
    "        \"\"\"\n",
    "        config = cls(env_id=env_id)\n",
    "        \n",
    "        # Level 2 & 3: Apply environment and algorithm configs\n",
    "        if env_id in cls.ENV_CONFIGS:\n",
    "            env_config = cls.ENV_CONFIGS[env_id]\n",
    "            \n",
    "            # Apply environment default config first\n",
    "            if \"default\" in env_config:\n",
    "                for key, value in env_config[\"default\"].items():\n",
    "                    setattr(config, key, value)\n",
    "            \n",
    "            # Apply algorithm-specific config if it exists\n",
    "            if algorithm in env_config:\n",
    "                for key, value in env_config[algorithm].items():\n",
    "                    setattr(config, key, value)\n",
    "        \n",
    "        return config\n",
    "\n",
    "# Create configs for different algorithms\n",
    "CONFIG = RLConfig.create(ENV_ID, ALGO_ID)\n",
    "CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90fd389",
   "metadata": {},
   "source": [
    "Build environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7605ffa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Info (SubprocVecEnv with 8 envs)\n",
      "  Env ID: CartPole-v1\n",
      "  Observation space: Box(low=[-4.8, -inf, -0.419, -inf], high=[4.8, inf, 0.419, inf], shape=(4,), dtype=float32)\n",
      "  Action space: Discrete(2)\n",
      "  Max episode steps: 500\n"
     ]
    }
   ],
   "source": [
    "from tsilva_notebook_utils.gymnasium import log_env_info\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_random_seed(CONFIG.seed)\n",
    "\n",
    "# Wrap build env with config parameters\n",
    "build_env_fn = lambda seed, n_envs=None: _build_env(\n",
    "    CONFIG.env_id, \n",
    "    norm_obs=CONFIG.normalize, \n",
    "    n_envs=n_envs if n_envs is not None else CONFIG.n_envs, \n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# Test building env\n",
    "env = build_env_fn(CONFIG.seed)\n",
    "log_env_info(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df342506",
   "metadata": {},
   "source": [
    "## Build Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123f099b",
   "metadata": {},
   "source": [
    "Define models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b62ead4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNet(nn.Module):\n",
    "    \"\"\"Reusable MLP with configurable hidden dimensions\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=64, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        \n",
    "        if isinstance(hidden_dim, (int, float)):\n",
    "            hidden_dims = [int(hidden_dim)]\n",
    "        else:\n",
    "            hidden_dims = [int(dim) for dim in hidden_dim]\n",
    "        \n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        \n",
    "        for hidden_size in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(current_dim, hidden_size),\n",
    "                activation()\n",
    "            ])\n",
    "            current_dim = hidden_size\n",
    "        \n",
    "        layers.append(nn.Linear(current_dim, output_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class PolicyNet(MLPNet):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_dim=64):\n",
    "        super().__init__(obs_dim, act_dim, hidden_dim)\n",
    "\n",
    "class ValueNet(MLPNet):\n",
    "    def __init__(self, obs_dim, hidden_dim=64):\n",
    "        super().__init__(obs_dim, 1, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6bad226",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtsilva\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250716_164410-4j54uw3d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tsilva/CartPole-v1/runs/4j54uw3d' target=\"_blank\">PPO-pwqzc</a></strong> to <a href='https://wandb.ai/tsilva/CartPole-v1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tsilva/CartPole-v1' target=\"_blank\">https://wandb.ai/tsilva/CartPole-v1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tsilva/CartPole-v1/runs/4j54uw3d' target=\"_blank\">https://wandb.ai/tsilva/CartPole-v1/runs/4j54uw3d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— W&B Run: https://wandb.ai/tsilva/CartPole-v1/runs/4j54uw3d\n",
      "Waiting for initial rollout...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name         | Type      | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | policy_model | PolicyNet | 226    | train\n",
      "1 | value_model  | ValueNet  | 193    | train\n",
      "---------------------------------------------------\n",
      "419       Trainable params\n",
      "0         Non-trainable params\n",
      "419       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "10        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started at 2025-07-16 16:44:12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tsilva/repos/tsilva/gymnasium-solver/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb579047975d44889e9db9e1bd01b372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 39 with eval mean reward 479.80 >= threshold 475\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>â–â–â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>epoch/approx_kl</td><td>â–ˆâ–‚â–‚â–‚â–â–â–‚â–â–‚â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>epoch/clip_fraction</td><td>â–ˆâ–‚â–ƒâ–ƒâ–â–â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>epoch/entropy</td><td>â–…â–‡â–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–„â–ƒâ–ƒâ–„â–ƒâ–„â–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–â–â–â–</td></tr><tr><td>epoch/explained_var</td><td>â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–‡â–†â–„â–…â–„â–„â–„â–…â–‚â–„â–â–„â–ƒâ–„â–ƒ</td></tr><tr><td>epoch/kl_div</td><td>â–ˆâ–â–‚â–‚â–â–â–‚â–â–‚â–â–â–‚â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>epoch/policy_loss</td><td>â–ˆâ–â–‚â–â–‚â–â–â–â–â–â–â–â–‚â–â–â–â–â–‚â–â–â–‚â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚</td></tr><tr><td>epoch/value_loss</td><td>â–ƒâ–‚â–â–â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–„â–ƒâ–„â–ƒâ–ƒâ–…â–„â–„â–ˆâ–†â–…â–„â–ˆâ–…â–†â–‡â–…â–†â–ˆâ–†â–…â–‡â–…â–†â–‡â–†â–…â–†</td></tr><tr><td>eval/mean_reward</td><td>â–â–ˆ</td></tr><tr><td>rollout/avg_steps_per_episode</td><td>â–â–ˆ</td></tr><tr><td>rollout/mean_reward</td><td>â–â–ˆ</td></tr><tr><td>rollout/num_episodes</td><td>â–â–</td></tr><tr><td>rollout/num_steps</td><td>â–â–ˆ</td></tr><tr><td>rollout/queue_updated</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>rollout/steps_per_second</td><td>â–â–ˆ</td></tr><tr><td>rollout/time_elapsed</td><td>â–â–ˆ</td></tr><tr><td>train/advantage_mean</td><td>â–…â–ƒâ–„â–…â–…â–„â–…â–…â–„â–…â–„â–†â–†â–…â–„â–ƒâ–…â–‡â–…â–…â–…â–â–†â–„â–†â–…â–‡â–„â–…â–†â–„â–‡â–…â–„â–‡â–†â–†â–„â–„â–ˆ</td></tr><tr><td>train/advantage_std</td><td>â–‡â–‡â–†â–ˆâ–†â–‡â–‡â–†â–‡â–‡â–‡â–‡â–†â–†â–‡â–†â–†â–†â–‡â–†â–‡â–‡â–…â–‡â–‡â–‡â–†â–ˆâ–‡â–‡â–ˆâ–…â–ˆâ–…â–ˆâ–†â–‡â–„â–†â–</td></tr><tr><td>train/approx_kl</td><td>â–ˆâ–‚â–‚â–‚â–â–â–â–‚â–â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train/clip_fraction</td><td>â–ˆâ–ƒâ–ƒâ–ƒâ–‚â–â–‚â–â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>train/entropy</td><td>â–…â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–„â–ƒâ–„â–„â–…â–„â–ƒâ–ƒâ–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–â–‚</td></tr><tr><td>train/explained_var</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–†â–„â–…â–…â–…â–†â–â–…â–‚â–ƒâ–ƒâ–„â–„</td></tr><tr><td>train/kl_div</td><td>â–ˆâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–â–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚</td></tr><tr><td>train/mean_reward</td><td>â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–ˆ</td></tr><tr><td>train/policy_loss</td><td>â–†â–ƒâ–†â–†â–„â–…â–…â–„â–ƒâ–†â–â–…â–…â–„â–…â–†â–„â–‚â–ƒâ–„â–ƒâ–ˆâ–ƒâ–…â–ƒâ–‚â–…â–ƒâ–„â–…â–ƒâ–…â–‚â–†â–ƒâ–…â–…â–…â–ƒâ–</td></tr><tr><td>train/returns_mean</td><td>â–â–â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ</td></tr><tr><td>train/total_steps</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆ</td></tr><tr><td>train/value_loss</td><td>â–‚â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–„â–„â–„â–‡â–‡â–„â–„â–‡â–‡â–…â–…â–‡â–†â–†â–„â–…â–…â–‡â–…â–ƒâ–ˆâ–…â–…â–…â–„</td></tr><tr><td>train/value_mean</td><td>â–â–â–â–â–â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–‡â–‡â–‡â–ˆ</td></tr><tr><td>trainer/global_step</td><td>â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>39</td></tr><tr><td>epoch/approx_kl</td><td>0.00073</td></tr><tr><td>epoch/clip_fraction</td><td>0.00024</td></tr><tr><td>epoch/entropy</td><td>0.59759</td></tr><tr><td>epoch/explained_var</td><td>-0.55656</td></tr><tr><td>epoch/kl_div</td><td>0.00042</td></tr><tr><td>epoch/policy_loss</td><td>-0.00667</td></tr><tr><td>epoch/value_loss</td><td>53.91238</td></tr><tr><td>eval/mean_reward</td><td>479.79999</td></tr><tr><td>rollout/avg_steps_per_episode</td><td>484.70306</td></tr><tr><td>rollout/mean_reward</td><td>479.79999</td></tr><tr><td>rollout/num_episodes</td><td>5</td></tr><tr><td>rollout/num_steps</td><td>2424</td></tr><tr><td>rollout/queue_updated</td><td>1</td></tr><tr><td>rollout/steps_per_second</td><td>1057.87915</td></tr><tr><td>rollout/time_elapsed</td><td>2.29038</td></tr><tr><td>train/advantage_mean</td><td>0.12681</td></tr><tr><td>train/advantage_std</td><td>0.72967</td></tr><tr><td>train/approx_kl</td><td>0.0006</td></tr><tr><td>train/clip_fraction</td><td>0</td></tr><tr><td>train/entropy</td><td>0.60411</td></tr><tr><td>train/explained_var</td><td>-0.62251</td></tr><tr><td>train/kl_div</td><td>-0.00131</td></tr><tr><td>train/mean_reward</td><td>177.07001</td></tr><tr><td>train/policy_loss</td><td>-0.1323</td></tr><tr><td>train/returns_mean</td><td>33.78345</td></tr><tr><td>train/total_steps</td><td>163840</td></tr><tr><td>train/value_loss</td><td>43.57422</td></tr><tr><td>train/value_mean</td><td>22.55031</td></tr><tr><td>trainer/global_step</td><td>639</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">PPO-pwqzc</strong> at: <a href='https://wandb.ai/tsilva/CartPole-v1/runs/4j54uw3d' target=\"_blank\">https://wandb.ai/tsilva/CartPole-v1/runs/4j54uw3d</a><br> View project at: <a href='https://wandb.ai/tsilva/CartPole-v1' target=\"_blank\">https://wandb.ai/tsilva/CartPole-v1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250716_164410-4j54uw3d/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed in 43.71 seconds (0.73 minutes)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import wandb\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from tsilva_notebook_utils.lightning import WandbCleanup\n",
    "from learners.ppo import PPOLearner\n",
    "from learners.reinforce import REINFORCELearner\n",
    "from utils.rollouts import AsyncRolloutCollector, SyncRolloutCollector\n",
    "\n",
    "# Rollout collection\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.n if hasattr(env.action_space, 'n') else env.action_space.shape[0]\n",
    "policy_model = PolicyNet(obs_dim, act_dim, CONFIG.hidden_dim)\n",
    "value_model = ValueNet(obs_dim, CONFIG.hidden_dim)\n",
    "rollout_env = build_env_fn(CONFIG.seed + 1000, n_envs=CONFIG.n_envs)\n",
    "rollout_collector_cls = AsyncRolloutCollector if CONFIG.async_rollouts else SyncRolloutCollector\n",
    "rollout_collector = rollout_collector_cls(CONFIG, rollout_env, policy_model, value_model=value_model)\n",
    "    \n",
    "# Create PPO agent\n",
    "policy_model = PolicyNet(obs_dim, act_dim, CONFIG.hidden_dim)\n",
    "value_model = ValueNet(obs_dim, CONFIG.hidden_dim)\n",
    "if ALGO_ID.upper() == \"PPO\": agent = PPOLearner(CONFIG, build_env_fn, rollout_collector, policy_model, value_model)\n",
    "elif ALGO_ID.upper() == \"REINFORCE\": agent = REINFORCELearner(CONFIG, build_env_fn, rollout_collector, policy_model)\n",
    "else: raise ValueError(f\"Unsupported algorithm: {ALGO_ID}. Choose 'PPO' or 'REINFORCE'\")\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    project=f\"{ENV_ID}\",\n",
    "    name=f\"{ALGO_ID}-{wandb.util.generate_id()[:5]}\",\n",
    "    log_model=True\n",
    ")\n",
    "\n",
    "# Print W&B run URL explicitly\n",
    "print(f\"ğŸ”— W&B Run: {wandb_logger.experiment.url}\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=wandb_logger,\n",
    "    log_every_n_steps=10,\n",
    "    max_epochs=CONFIG.max_epochs,\n",
    "    enable_progress_bar=True,\n",
    "    enable_checkpointing=False,  # Disable checkpointing for speed\n",
    "    accelerator=\"auto\",\n",
    "    callbacks=[WandbCleanup()]\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "trainer.fit(agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b67ba7e",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8966dc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tsilva/repos/tsilva/gymnasium-solver/.venv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n",
      "/Users/tsilva/repos/tsilva/gymnasium-solver/.venv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n",
      "/Users/tsilva/repos/tsilva/gymnasium-solver/.venv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n",
      "/Users/tsilva/repos/tsilva/gymnasium-solver/.venv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n",
      "/Users/tsilva/repos/tsilva/gymnasium-solver/.venv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n",
      "/Users/tsilva/repos/tsilva/gymnasium-solver/.venv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n",
      "/Users/tsilva/repos/tsilva/gymnasium-solver/.venv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n",
      "/Users/tsilva/repos/tsilva/gymnasium-solver/.venv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n",
      "2025-07-16 16:44:59.413 python[24936:28153630] +[NSXPCSharedListener endpointForReply:withListenerName:replyErrorCode:]: an error occurred while attempting to obtain endpoint for listener 'ClientCallsAuxiliary': Connection invalid\n",
      "2025-07-16 16:44:59.414 python[24937:28153631] +[NSXPCSharedListener endpointForReply:withListenerName:replyErrorCode:]: an error occurred while attempting to obtain endpoint for listener 'ClientCallsAuxiliary': Connection invalid\n",
      "2025-07-16 16:44:59.414 python[24933:28153627] +[NSXPCSharedListener endpointForReply:withListenerName:replyErrorCode:]: an error occurred while attempting to obtain endpoint for listener 'ClientCallsAuxiliary': Connection invalid\n",
      "2025-07-16 16:44:59.414 python[24935:28153629] +[NSXPCSharedListener endpointForReply:withListenerName:replyErrorCode:]: an error occurred while attempting to obtain endpoint for listener 'ClientCallsAuxiliary': Connection invalid\n",
      "2025-07-16 16:44:59.414 python[24932:28153626] +[NSXPCSharedListener endpointForReply:withListenerName:replyErrorCode:]: an error occurred while attempting to obtain endpoint for listener 'ClientCallsAuxiliary': Connection invalid\n",
      "2025-07-16 16:44:59.414 python[24930:28153624] +[NSXPCSharedListener endpointForReply:withListenerName:replyErrorCode:]: an error occurred while attempting to obtain endpoint for listener 'ClientCallsAuxiliary': Connection invalid\n",
      "2025-07-16 16:44:59.415 python[24931:28153625] +[NSXPCSharedListener endpointForReply:withListenerName:replyErrorCode:]: an error occurred while attempting to obtain endpoint for listener 'ClientCallsAuxiliary': Connection invalid\n",
      "2025-07-16 16:44:59.415 python[24934:28153628] +[NSXPCSharedListener endpointForReply:withListenerName:replyErrorCode:]: an error occurred while attempting to obtain endpoint for listener 'ClientCallsAuxiliary': Connection invalid\n",
      "2025-07-16 16:44:59.413 python[24936:28153630] +[NSXPCSharedListener endpointForReply:withListenerName:replyErrorCode:]: an error occurred while attempting to obtain endpoint for listener 'ClientCallsAuxiliary': Connection invalid\n",
      "2025-07-16 16:44:59.414 python[24937:28153631] +[NSXPCSharedListener endpointForReply:withListenerName:replyErrorCode:]: an error occurred while attempting to obtain endpoint for listener 'ClientCallsAuxiliary': Connection invalid\n",
      "2025-07-16 16:44:59.414 python[24933:28153627] +[NSXPCSharedListener endpointForReply:withListenerName:replyErrorCode:]: an error occurred while attempting to obtain endpoint for listener 'ClientCallsAuxiliary': Connection invalid\n",
      "2025-07-16 16:44:59.414 python[24935:28153629] +[NSXPCSharedListener endpointForReply:withListenerName:replyErrorCode:]: an error occurred while attempting to obtain endpoint for listener 'ClientCallsAuxiliary': Connection invalid\n",
      "2025-07-16 16:44:59.414 python[24932:28153626] +[NSXPCSharedListener endpointForReply:withListenerName:replyErrorCode:]: an error occurred while attempting to obtain endpoint for listener 'ClientCallsAuxiliary': Connection invalid\n",
      "2025-07-16 16:44:59.414 python[24930:28153624] +[NSXPCSharedListener endpointForReply:withListenerName:replyErrorCode:]: an error occurred while attempting to obtain endpoint for listener 'ClientCallsAuxiliary': Connection invalid\n",
      "2025-07-16 16:44:59.415 python[24931:28153625] +[NSXPCSharedListener endpointForReply:withListenerName:replyErrorCode:]: an error occurred while attempting to obtain endpoint for listener 'ClientCallsAuxiliary': Connection invalid\n",
      "2025-07-16 16:44:59.415 python[24934:28153628] +[NSXPCSharedListener endpointForReply:withListenerName:replyErrorCode:]: an error occurred while attempting to obtain endpoint for listener 'ClientCallsAuxiliary': Connection invalid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 500.00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"max-width:640px\">\n",
       "    <video id=\"vid_a0165f00393d4a41bbe4079365c6994b\" src=\"tmp/video_dd9f5846c2ab43d49532b7823602bb31.mp4\" width=\"100%\" controls playsinline\n",
       "            style=\"background:#000\"></video>\n",
       "\n",
       "    <div style=\"font-size:0.9em;margin-top:4px\">\n",
       "    <a href=\"tmp/video_dd9f5846c2ab43d49532b7823602bb31.mp4\" target=\"_blank\">ğŸ”— open in new tab (fullscreen âœ“)</a>\n",
       "    &nbsp;â€¢&nbsp; double-click video to toggle fullscreen\n",
       "    </div>\n",
       "</div>\n",
       "\n",
       "<script>\n",
       "const v = document.getElementById('vid_a0165f00393d4a41bbe4079365c6994b');\n",
       "v.addEventListener('dblclick', () => {\n",
       "    if (!document.fullscreenElement) {\n",
       "        (v.requestFullscreen || v.webkitRequestFullscreen ||\n",
       "            v.msRequestFullscreen).call(v);\n",
       "    } else {\n",
       "        (document.exitFullscreen || document.webkitExitFullscreen ||\n",
       "            document.msExitFullscreen).call(document);\n",
       "    }\n",
       "});\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from utils.rollouts import collect_rollouts, group_trajectories_by_episode\n",
    "from tsilva_notebook_utils.gymnasium import render_episode_frames\n",
    "\n",
    "n_episodes = 8\n",
    "trajectories_with_info = collect_rollouts(\n",
    "    build_env_fn(random.randint(0, 1_000_000), n_envs=n_episodes),\n",
    "    agent.policy_model,\n",
    "    n_episodes=n_episodes,\n",
    "    deterministic=True,\n",
    "    collect_frames=True\n",
    ")\n",
    "trajectories, info = trajectories_with_info\n",
    "# Extract frames separately (last element in trajectories tuple)\n",
    "frames_flat = trajectories[-1]  # frames_env_major from collect_rollouts\n",
    "# Get just the first 8 elements for episode grouping (without frames)\n",
    "trajectories_no_frames = trajectories[:-1]\n",
    "\n",
    "episodes = group_trajectories_by_episode(trajectories_no_frames)\n",
    "mean_reward = np.mean([sum(step[2] for step in episode) for episode in episodes])\n",
    "\n",
    "# Reconstruct episode frames from flat frames\n",
    "# frames_flat is in env-major order: [env0_t0, env0_t1, ..., env1_t0, env1_t1, ...]\n",
    "# We need to group them by episode using the done signals\n",
    "episode_frames = []\n",
    "current_episode_frames = []\n",
    "frame_idx = 0\n",
    "\n",
    "for episode in episodes:\n",
    "    episode_len = len(episode)\n",
    "    episode_frames.append(frames_flat[frame_idx:frame_idx + episode_len])\n",
    "    frame_idx += episode_len\n",
    "\n",
    "print(f\"Mean reward: {mean_reward:.2f}\")\n",
    "render_episode_frames(episode_frames, out_dir=\"./tmp\", grid=(2, 2), text_color=(0, 0, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837c19f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03fc5b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key metrics to watch on W&B dashboard:\n",
    "primary_metrics = [\n",
    "    \"eval/mean_reward\",           # Main success indicator\n",
    "    \"train/mean_reward\",          # Training progress\n",
    "    \"epoch/explained_var\",        # Value function quality\n",
    "    \"epoch/entropy\",              # Exploration level\n",
    "    \"epoch/clip_fraction\"         # Policy update stability\n",
    "]\n",
    "\n",
    "warning_conditions = {\n",
    "    \"epoch/clip_fraction > 0.5\": \"Reduce policy_lr or clip_epsilon\",\n",
    "    \"epoch/approx_kl > 0.1\": \"Reduce policy_lr\", \n",
    "    \"epoch/explained_var < 0.3\": \"Increase value_lr or network size\",\n",
    "    \"epoch/entropy < 0.01\": \"Increase entropy_coef\",\n",
    "    \"rollout/queue_miss > rollout/queue_updated\": \"Check async collection\"\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymnasium-solver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
