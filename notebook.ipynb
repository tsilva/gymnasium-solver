{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fae7ff8f",
   "metadata": {},
   "source": [
    "# Environment Solver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddee31c",
   "metadata": {},
   "source": [
    "Make notebook autoreload when imported repo modules change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "768cf7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7192430d",
   "metadata": {},
   "source": [
    "Define which environment we want to solve and with which algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a074cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_ID = \"CartPole-v1\"\n",
    "#ENV_ID = \"LunarLander-v3\"\n",
    "#ALGORITHM = \"reinforce\"\n",
    "ALGO_ID = \"ppo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b028503",
   "metadata": {},
   "source": [
    "Load secrets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5101c4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsilva_notebook_utils.colab import load_secrets_into_env\n",
    "_ = load_secrets_into_env(['WANDB_API_KEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009c423d",
   "metadata": {},
   "source": [
    "Load training configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d847b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config for CartPole-v1 with ppo algorithm:\n",
      "RLConfig(env_id='CartPole-v1', seed=42, max_epochs=-1, gamma=0.99, lam=0.95, clip_epsilon=0.2, batch_size=256, train_rollout_steps=512, eval_interval=20, eval_episodes=5, reward_threshold=475, policy_lr=0.001, value_lr=0.001, hidden_dims=(32,), entropy_coef=0.01, normalize=False, mean_reward_window=100, rollout_interval=1)\n"
     ]
    }
   ],
   "source": [
    "from utils.config import load_config\n",
    "CONFIG = load_config(ENV_ID, ALGO_ID)\n",
    "print(f\"Loaded config for {ENV_ID} with {ALGO_ID} algorithm:\")\n",
    "print(CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90fd389",
   "metadata": {},
   "source": [
    "Build environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7605ffa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Info (DummyVecEnv with 1 envs)\n",
      "  Env ID: CartPole-v1\n",
      "  Observation space: Box(low=[-4.8, -inf, -0.419, -inf], high=[4.8, inf, 0.419, inf], shape=(4,), dtype=float32)\n",
      "  Action space: Discrete(2)\n",
      "  Max episode steps: 500\n"
     ]
    }
   ],
   "source": [
    "from tsilva_notebook_utils.gymnasium import log_env_info\n",
    "from utils.environment import setup_environment\n",
    "build_env_fn = setup_environment(CONFIG) # TODO: consider getting rid of this method or moving everything inside it\n",
    "env = build_env_fn(CONFIG.seed)\n",
    "log_env_info(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a69afe6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m policy_model = PolicyNet(obs_dim, act_dim, CONFIG.hidden_dims)\n\u001b[32m      6\u001b[39m value_model = ValueNet(obs_dim, CONFIG.hidden_dims) \u001b[38;5;28;01mif\u001b[39;00m ALGO_ID == \u001b[33m\"\u001b[39m\u001b[33mppo\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m train_rollout_collector = \u001b[43mSyncRolloutCollector\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbuild_env_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m.\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpolicy_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_rollout_steps\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/tsilva/gymnasium-solver/utils/rollouts.py:265\u001b[39m, in \u001b[36mSyncRolloutCollector.__init__\u001b[39m\u001b[34m(self, env, policy_model, value_model, n_steps)\u001b[39m\n\u001b[32m    263\u001b[39m \u001b[38;5;28mself\u001b[39m.n_steps = n_steps\n\u001b[32m    264\u001b[39m \u001b[38;5;28mself\u001b[39m.dataset = RolloutDataset()\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m \u001b[38;5;28mself\u001b[39m.dataloader = \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m# TODO: softcode self.config.batch_size,\u001b[39;49;00m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# TODO: fix num_workers, training not converging when they are on\u001b[39;49;00m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Pin memory is not supported on MPS\u001b[39;49;00m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#pin_memory=True if self.device.type != 'mps' else False,\u001b[39;49;00m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# TODO: Persistent workers + num_workers is fast but doesn't converge\u001b[39;49;00m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#persistent_workers=True if self.device.type != 'mps' else False,\u001b[39;49;00m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Using multiple workers stalls the start of each epoch when persistent workers are disabled\u001b[39;49;00m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#num_workers=multiprocessing.cpu_count() // 2 if self.device.type != 'mps' else 0\u001b[39;49;00m\n\u001b[32m    276\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[38;5;66;03m# TODO: move inside rollout collector?\u001b[39;00m\n\u001b[32m    278\u001b[39m \u001b[38;5;28mself\u001b[39m.episode_reward_deque = deque(maxlen=\u001b[32m100\u001b[39m)\u001b[38;5;66;03m#TODO: softcode config.mean_reward_window)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/tsilva/gymnasium-solver/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:385\u001b[39m, in \u001b[36mDataLoader.__init__\u001b[39m\u001b[34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device, in_order)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         sampler = \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    387\u001b[39m         sampler = SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/tsilva/gymnasium-solver/.venv/lib/python3.12/site-packages/torch/utils/data/sampler.py:155\u001b[39m, in \u001b[36mRandomSampler.__init__\u001b[39m\u001b[34m(self, data_source, replacement, num_samples, generator)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.replacement, \u001b[38;5;28mbool\u001b[39m):\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    152\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.replacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    153\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_samples\u001b[49m, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_samples <= \u001b[32m0\u001b[39m:\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    157\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    158\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/tsilva/gymnasium-solver/.venv/lib/python3.12/site-packages/torch/utils/data/sampler.py:164\u001b[39m, in \u001b[36mRandomSampler.num_samples\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnum_samples\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# dataset size might change at runtime\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_samples \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata_source\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_samples\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/tsilva/gymnasium-solver/utils/rollouts.py:252\u001b[39m, in \u001b[36mRolloutDataset.__len__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrajectories\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from utils.rollouts import SyncRolloutCollector\n",
    "from utils.models import PolicyNet, ValueNet\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.n if hasattr(env.action_space, 'n') else env.action_space.shape[0]\n",
    "policy_model = PolicyNet(obs_dim, act_dim, CONFIG.hidden_dims)\n",
    "value_model = ValueNet(obs_dim, CONFIG.hidden_dims) if ALGO_ID == \"ppo\" else None\n",
    "train_rollout_collector = SyncRolloutCollector(\n",
    "    build_env_fn(CONFIG.seed),\n",
    "    policy_model,\n",
    "    value_model=value_model,\n",
    "    n_steps=CONFIG.train_rollout_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c758e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories = train_rollout_collector.collect_rollouts() # TODO: consider making get rollout be async method even in sync rollout?\n",
    "sum(1 if t else 0 for t in trajectories[3]), len(trajectories[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123f099b",
   "metadata": {},
   "source": [
    "Define models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bad226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.training import create_trainer\n",
    "from utils.rollouts import SyncRolloutCollector # TODO: restore async functionality\n",
    "from utils.models import PolicyNet, ValueNet\n",
    "from learners.ppo import PPOLearner\n",
    "from learners.reinforce import REINFORCELearner\n",
    "\n",
    "# TODO: make rollout collector clone models?\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = env.action_space.n\n",
    "policy_model = PolicyNet(input_dim, output_dim, CONFIG.hidden_dims)\n",
    "value_model = ValueNet(input_dim, CONFIG.hidden_dims) if ALGO_ID == \"ppo\" else None # TODO: softcode this better\n",
    "\n",
    "train_rollout_collector = SyncRolloutCollector(\n",
    "    build_env_fn(CONFIG.seed),\n",
    "    policy_model,\n",
    "    value_model=value_model,\n",
    "    n_steps=CONFIG.train_rollout_steps\n",
    ")\n",
    "eval_rollout_collector = SyncRolloutCollector(\n",
    "    # TODO: pass env factory and rebuild env on start/stop? this allows using same rollout collector for final evaluation\n",
    "    build_env_fn(CONFIG.seed + 1000),  # Use a different seed for evaluation\n",
    "    policy_model,\n",
    "    n_steps=CONFIG.train_rollout_steps # TODO: change this\n",
    ")\n",
    "algo_id = ALGO_ID.lower()\n",
    "if algo_id == \"ppo\": agent = PPOLearner(CONFIG, train_rollout_collector, policy_model, value_model, eval_rollout_collector=eval_rollout_collector)\n",
    "elif algo_id == \"reinforce\": agent = REINFORCELearner(CONFIG, train_rollout_collector, policy_model, eval_rollout_collector=eval_rollout_collector)\n",
    "\n",
    "# Create trainer with W&B logging\n",
    "# TODO: infer most args\n",
    "trainer = create_trainer(CONFIG, project_name=ENV_ID, run_name=f\"{ALGO_ID}-{CONFIG.seed}\")\n",
    "\n",
    "# Fit the model\n",
    "trainer.fit(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8966dc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import evaluate_agent\n",
    "\n",
    "# Evaluate agent and render episodes\n",
    "# TODO: evaluate agent should receive rollout collector as an argument, not the agent and build_env_fn\n",
    "results = evaluate_agent(\n",
    "    agent, \n",
    "    build_env_fn, \n",
    "    n_episodes=8, \n",
    "    deterministic=True, \n",
    "    render=True,\n",
    "    grid=(2, 2), \n",
    "    text_color=(0, 0, 0), \n",
    "    out_dir=\"./tmp\"\n",
    ")\n",
    "\n",
    "print(f\"Mean reward: {results['mean_reward']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymnasium-solver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
