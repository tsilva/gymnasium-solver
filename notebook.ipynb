{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fae7ff8f",
   "metadata": {},
   "source": [
    "# Environment Solver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f29d81",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We will use gymnasium and torch for this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a074cf9",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'gymnasium-solver (Python 3.12.2)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/tsilva/repos/tsilva/gymnasium-solver/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "ENV_ID = \"CartPole-v1\"\n",
    "#ENV_ID = \"LunarLander-v3\"\n",
    "ALGO_ID = \"REINFORCE\"  # Change to \"REINFORCE\" for REINFORCE algorithm\n",
    "#ALGO_ID = \"PPO\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ecffa8",
   "metadata": {},
   "source": [
    "Install packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9275be49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%CXX=clang++ pip install \"gymnasium[box2d]\" --quiet\n",
    "%pip install stable-baselines3 wandb swig tsilva-notebook-utils==0.0.121 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b165239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"pkg_resources is deprecated as an API.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b028503",
   "metadata": {},
   "source": [
    "Load secrets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5101c4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsilva_notebook_utils.colab import load_secrets_into_env\n",
    "\n",
    "_ = load_secrets_into_env([\n",
    "    'WANDB_API_KEY'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82b4d5a",
   "metadata": {},
   "source": [
    "Retrieve enviroment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834554f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tsilva_notebook_utils.torch import get_default_device\n",
    "\n",
    "DEVICE = get_default_device()\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d847b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from tsilva_notebook_utils.gymnasium import build_env as _build_env, set_random_seed\n",
    "from dataclasses import dataclass\n",
    "from typing import Union, Tuple\n",
    "\n",
    "@dataclass\n",
    "class RLConfig:\n",
    "    # Environment\n",
    "    env_id: str\n",
    "    seed: int = 42\n",
    "    \n",
    "    # Training\n",
    "    max_epochs: int = -1\n",
    "    gamma: float = 0.99\n",
    "    lam: float = 0.95\n",
    "    clip_epsilon: float = 0.2\n",
    "    batch_size: int = 64\n",
    "    train_rollout_steps: int = 2048\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_interval: int = 10\n",
    "    eval_episodes: int = 32\n",
    "    reward_threshold: float = 200\n",
    "    \n",
    "    # Networks\n",
    "    policy_lr: float = 3e-4\n",
    "    value_lr: float = 1e-3\n",
    "    hidden_dim: Union[int, Tuple[int, ...]] = 64\n",
    "    entropy_coef: float = 0.01\n",
    "    \n",
    "    # Other\n",
    "    normalize: bool = False\n",
    "    mean_reward_window: int = 100\n",
    "    rollout_interval: int = 10\n",
    "    n_envs: Union[str, int] = \"auto\"\n",
    "    async_rollouts: bool = True\n",
    "    \n",
    "    # Environment-specific configurations with flat structure\n",
    "    # Use these as reference: https://github.com/DLR-RM/rl-baselines3-zoo/tree/master/hyperparams\n",
    "    ENV_CONFIGS = {\n",
    "        \"CartPole-v1\": {\n",
    "            # Default config for this environment (applies to all algorithms unless overridden)\n",
    "            \"default\": dict(\n",
    "                train_rollout_steps=512,\n",
    "                batch_size=256,\n",
    "                rollout_interval=1,\n",
    "                eval_interval=20,\n",
    "                eval_episodes=5,\n",
    "                reward_threshold=475,\n",
    "                policy_lr=1e-3,\n",
    "                value_lr=1e-3,\n",
    "                hidden_dim=32,\n",
    "            ),\n",
    "            # Algorithm-specific overrides\n",
    "            \"reinforce\": dict(\n",
    "                train_rollout_steps=2048,\n",
    "                batch_size=512,\n",
    "                policy_lr=1e-3,\n",
    "                entropy_coef=0.02,\n",
    "            ),\n",
    "        },\n",
    "        \"Acrobot-v1\": {\n",
    "            \"default\": dict(\n",
    "                gamma=0.99,\n",
    "                lam=0.98,\n",
    "                clip_epsilon=0.2,\n",
    "                batch_size=128,\n",
    "                train_rollout_steps=2048,\n",
    "                eval_interval=5,\n",
    "                reward_threshold=-100,\n",
    "                policy_lr=3e-4,\n",
    "                value_lr=3e-4,\n",
    "                hidden_dim=(128, 64),\n",
    "                entropy_coef=0.01,\n",
    "                rollout_interval=1\n",
    "            ),\n",
    "            \"reinforce\": dict(\n",
    "                train_rollout_steps=4096,\n",
    "                batch_size=256,\n",
    "                policy_lr=5e-4,\n",
    "                entropy_coef=0.05,\n",
    "            ),\n",
    "        },\n",
    "        # TODO:\n",
    "        #n_envs: 16\n",
    "        #n_epochs: 4 \n",
    "        #n_steps: 1024\n",
    "        \"LunarLander-v3\": {\n",
    "            \"default\": dict(\n",
    "                reward_threshold=200,\n",
    "                total_timesteps=1e6, # TODO: call this n_timesteps\n",
    "                gamma=0.999,\n",
    "                # TODO: this is not being propagated to collect_rollouts\n",
    "                lam=0.98, # gae_lambda: 0.98\n",
    "                clip_epsilon=0.2,\n",
    "                batch_size=64,\n",
    "                eval_interval=2,\n",
    "                policy_lr=1e-4,\n",
    "                value_lr=5e-4,\n",
    "                hidden_dim=32,\n",
    "                entropy_coef=0.01\n",
    "            ),\n",
    "            \"reinforce\": dict(\n",
    "                entropy_coef=0.03,\n",
    "                batch_size=128,\n",
    "            ),\n",
    "        },\n",
    "        \"Pendulum-v1\": {\n",
    "            \"default\": dict(\n",
    "                gamma=0.99,\n",
    "                lam=0.95,\n",
    "                clip_epsilon=0.2,\n",
    "                batch_size=64,\n",
    "                eval_interval=2,\n",
    "                eval_episodes=5,\n",
    "                reward_threshold=-200,\n",
    "                policy_lr=3e-4,\n",
    "                value_lr=1e-3,\n",
    "                hidden_dim=(128, 64),\n",
    "                entropy_coef=0.0\n",
    "            ),\n",
    "            \"reinforce\": dict(\n",
    "                entropy_coef=0.02,\n",
    "                batch_size=128,\n",
    "            ),\n",
    "        },\n",
    "        \"MountainCar-v0\": {\n",
    "            \"default\": dict(\n",
    "                gamma=0.99,\n",
    "                lam=0.97,\n",
    "                clip_epsilon=0.15,\n",
    "                batch_size=16,\n",
    "                eval_interval=2,\n",
    "                eval_episodes=10,\n",
    "                reward_threshold=-110,\n",
    "                policy_lr=1e-4,\n",
    "                value_lr=5e-4,\n",
    "                hidden_dim=(128, 64),\n",
    "                entropy_coef=0.05\n",
    "            ),\n",
    "            \"reinforce\": dict(\n",
    "                entropy_coef=0.08,\n",
    "                batch_size=32,\n",
    "            ),\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def create(cls, env_id: str, algorithm: str = \"ppo\") -> 'RLConfig':\n",
    "        \"\"\"\n",
    "        Create config with hierarchical overrides:\n",
    "        1. Start with dataclass defaults\n",
    "        2. Apply environment default config\n",
    "        3. Apply algorithm-specific config for that environment\n",
    "        \"\"\"\n",
    "        config = cls(env_id=env_id)\n",
    "        \n",
    "        # Level 2 & 3: Apply environment and algorithm configs\n",
    "        if env_id in cls.ENV_CONFIGS:\n",
    "            env_config = cls.ENV_CONFIGS[env_id]\n",
    "            \n",
    "            # Apply environment default config first\n",
    "            if \"default\" in env_config:\n",
    "                for key, value in env_config[\"default\"].items():\n",
    "                    setattr(config, key, value)\n",
    "            \n",
    "            # Apply algorithm-specific config if it exists\n",
    "            if algorithm in env_config:\n",
    "                for key, value in env_config[algorithm].items():\n",
    "                    setattr(config, key, value)\n",
    "        \n",
    "        return config\n",
    "\n",
    "# Create configs for different algorithms\n",
    "CONFIG = RLConfig.create(ENV_ID, ALGO_ID)\n",
    "CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90fd389",
   "metadata": {},
   "source": [
    "Build environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7605ffa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsilva_notebook_utils.gymnasium import log_env_info\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_random_seed(CONFIG.seed)\n",
    "\n",
    "# Wrap build env with config parameters\n",
    "build_env = lambda seed, n_envs=None: _build_env(\n",
    "    CONFIG.env_id, \n",
    "    norm_obs=CONFIG.normalize, \n",
    "    n_envs=n_envs if n_envs is not None else CONFIG.n_envs, \n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# Test building env\n",
    "env = build_env(CONFIG.seed)\n",
    "log_env_info(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df342506",
   "metadata": {},
   "source": [
    "## Build Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123f099b",
   "metadata": {},
   "source": [
    "Define models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62ead4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNet(nn.Module):\n",
    "    \"\"\"Reusable MLP with configurable hidden dimensions\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=64, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        \n",
    "        if isinstance(hidden_dim, (int, float)):\n",
    "            hidden_dims = [int(hidden_dim)]\n",
    "        else:\n",
    "            hidden_dims = [int(dim) for dim in hidden_dim]\n",
    "        \n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "        \n",
    "        for hidden_size in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(current_dim, hidden_size),\n",
    "                activation()\n",
    "            ])\n",
    "            current_dim = hidden_size\n",
    "        \n",
    "        layers.append(nn.Linear(current_dim, output_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class PolicyNet(MLPNet):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_dim=64):\n",
    "        super().__init__(obs_dim, act_dim, hidden_dim)\n",
    "\n",
    "class ValueNet(MLPNet):\n",
    "    def __init__(self, obs_dim, hidden_dim=64):\n",
    "        super().__init__(obs_dim, 1, hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bad226",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from tsilva_notebook_utils.lightning import WandbCleanup\n",
    "\n",
    "# Create PPO agent\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.n if hasattr(env.action_space, 'n') else env.action_space.shape[0]\n",
    "ppo_agent = PPOAgent(obs_dim, act_dim, CONFIG, build_env)\n",
    "\n",
    "agent_cls = None\n",
    "if ALGO_ID.upper() == \"PPO\": agent_cls = PPOAgent\n",
    "elif ALGO_ID.upper() == \"REINFORCE\": agent_cls = REINFORCEAgent\n",
    "else: raise ValueError(f\"Unsupported algorithm: {ALGO_ID}. Choose 'PPO' or 'REINFORCE'\")\n",
    "agent = agent_cls(obs_dim, act_dim, CONFIG, build_env)\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    project=f\"{ENV_ID}\",\n",
    "    name=f\"{ALGO_ID}-{wandb.util.generate_id()[:5]}\",\n",
    "    log_model=True\n",
    ")\n",
    "\n",
    "# Print W&B run URL explicitly\n",
    "print(f\"🔗 W&B Run: {wandb_logger.experiment.url}\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    logger=wandb_logger,\n",
    "    log_every_n_steps=10,\n",
    "    max_epochs=CONFIG.max_epochs,\n",
    "    enable_progress_bar=True,\n",
    "    enable_checkpointing=False,  # Disable checkpointing for speed\n",
    "    accelerator=\"auto\",\n",
    "    callbacks=[WandbCleanup()]\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "trainer.fit(ppo_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b67ba7e",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8966dc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tsilva_notebook_utils.gymnasium import render_episode_frames\n",
    "\n",
    "n_episodes = 8\n",
    "trajectories, _ = collect_rollouts(\n",
    "    build_env(random.randint(0, 1_000_000), n_envs=n_episodes),\n",
    "    ppo_agent.policy_model,\n",
    "    n_episodes=n_episodes,\n",
    "    deterministic=True,\n",
    "    collect_frames=True\n",
    ")\n",
    "episodes = group_trajectories_by_episode(trajectories) # something is wrong in frame collection\n",
    "mean_reward = np.mean([sum(step[2] for step in episode) for episode in episodes])\n",
    "episode_frames = [[step[-1] for step in episode] for episode in episodes]\n",
    "print(f\"Mean reward: {mean_reward:.2f}\")\n",
    "render_episode_frames(episode_frames, out_dir=\"./tmp\", grid=(2, 2), text_color=(0, 0, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837c19f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fc5b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key metrics to watch on W&B dashboard:\n",
    "primary_metrics = [\n",
    "    \"eval/mean_reward\",           # Main success indicator\n",
    "    \"train/mean_reward\",          # Training progress\n",
    "    \"epoch/explained_var\",        # Value function quality\n",
    "    \"epoch/entropy\",              # Exploration level\n",
    "    \"epoch/clip_fraction\"         # Policy update stability\n",
    "]\n",
    "\n",
    "warning_conditions = {\n",
    "    \"epoch/clip_fraction > 0.5\": \"Reduce policy_lr or clip_epsilon\",\n",
    "    \"epoch/approx_kl > 0.1\": \"Reduce policy_lr\", \n",
    "    \"epoch/explained_var < 0.3\": \"Increase value_lr or network size\",\n",
    "    \"epoch/entropy < 0.01\": \"Increase entropy_coef\",\n",
    "    \"rollout/queue_miss > rollout/queue_updated\": \"Check async collection\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afee3e2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymnasium-solver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
