{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fae7ff8f",
   "metadata": {},
   "source": [
    "# Environment Solver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddee31c",
   "metadata": {},
   "source": [
    "Make notebook autoreload when imported repo modules change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "768cf7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7192430d",
   "metadata": {},
   "source": [
    "Define which environment we want to solve and with which algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a074cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_ID = \"CartPole-v1\"\n",
    "#ENV_ID = \"LunarLander-v3\"\n",
    "#ALGORITHM = \"reinforce\"\n",
    "ALGO_ID = \"ppo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b028503",
   "metadata": {},
   "source": [
    "Load secrets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5101c4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsilva_notebook_utils.colab import load_secrets_into_env\n",
    "_ = load_secrets_into_env(['WANDB_API_KEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009c423d",
   "metadata": {},
   "source": [
    "Load training configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d847b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config for CartPole-v1 with ppo algorithm:\n",
      "RLConfig(env_id='CartPole-v1', seed=42, max_epochs=-1, gamma=0.99, lam=0.95, clip_epsilon=0.2, batch_size=256, train_rollout_steps=512, eval_interval=20, eval_episodes=5, reward_threshold=475, policy_lr=0.001, value_lr=0.001, hidden_dims=(32,), entropy_coef=0.01, normalize=False, mean_reward_window=100, rollout_interval=1)\n"
     ]
    }
   ],
   "source": [
    "from utils.config import load_config\n",
    "CONFIG = load_config(ENV_ID, ALGO_ID)\n",
    "print(f\"Loaded config for {ENV_ID} with {ALGO_ID} algorithm:\")\n",
    "print(CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90fd389",
   "metadata": {},
   "source": [
    "Build environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7605ffa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Info (DummyVecEnv with 1 envs)\n",
      "  Env ID: CartPole-v1\n",
      "  Observation space: Box(low=[-4.8, -inf, -0.419, -inf], high=[4.8, inf, 0.419, inf], shape=(4,), dtype=float32)\n",
      "  Action space: Discrete(2)\n",
      "  Max episode steps: 500\n"
     ]
    }
   ],
   "source": [
    "from tsilva_notebook_utils.gymnasium import log_env_info\n",
    "from utils.environment import setup_environment\n",
    "build_env_fn = setup_environment(CONFIG) # TODO: consider getting rid of this method or moving everything inside it\n",
    "env = build_env_fn(CONFIG.seed)\n",
    "log_env_info(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a69afe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.rollouts import SyncRolloutCollector\n",
    "from utils.models import PolicyNet, ValueNet\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.n if hasattr(env.action_space, 'n') else env.action_space.shape[0]\n",
    "policy_model = PolicyNet(obs_dim, act_dim, CONFIG.hidden_dims)\n",
    "value_model = ValueNet(obs_dim, CONFIG.hidden_dims) if ALGO_ID == \"ppo\" else None\n",
    "train_rollout_collector = SyncRolloutCollector(\n",
    "    build_env_fn(CONFIG.seed),\n",
    "    policy_model,\n",
    "    value_model=value_model,\n",
    "    n_steps=CONFIG.train_rollout_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40c758e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 512)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectories = train_rollout_collector.collect_rollouts() # TODO: consider making get rollout be async method even in sync rollout?\n",
    "sum(1 if t else 0 for t in trajectories[3]), len(trajectories[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123f099b",
   "metadata": {},
   "source": [
    "Define models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6bad226",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtsilva\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250722_152657-3jsx1fs1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tsilva/CartPole-v1/runs/3jsx1fs1' target=\"_blank\">ppo-42</a></strong> to <a href='https://wandb.ai/tsilva/CartPole-v1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tsilva/CartPole-v1' target=\"_blank\">https://wandb.ai/tsilva/CartPole-v1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tsilva/CartPole-v1/runs/3jsx1fs1' target=\"_blank\">https://wandb.ai/tsilva/CartPole-v1/runs/3jsx1fs1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type      | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | policy_model | PolicyNet | 226    | train\n",
      "1 | value_model  | ValueNet  | 193    | train\n",
      "---------------------------------------------------\n",
      "419       Trainable params\n",
      "0         Non-trainable params\n",
      "419       Total params\n",
      "0.002     Total estimated model params size (MB)\n",
      "10        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó W&B Run: https://wandb.ai/tsilva/CartPole-v1/runs/3jsx1fs1\n",
      "Training started at 2025-07-22 15:26:58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tsilva/repos/tsilva/gymnasium-solver/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/tsilva/repos/tsilva/gymnasium-solver/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3030064f71a6426faaa4ce7df472dee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>epoch/approx_kl</td><td>‚ñà‚ñá‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ</td></tr><tr><td>epoch/clip_fraction</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>epoch/entropy</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñÜ</td></tr><tr><td>epoch/explained_var</td><td>‚ñÜ‚ñà‚ñá‚ñÑ‚ñÉ‚ñÜ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÅ</td></tr><tr><td>epoch/kl_div</td><td>‚ñÇ‚ñÑ‚ñÜ‚ñÉ‚ñÅ‚ñÖ‚ñÑ‚ñÜ‚ñÉ‚ñÖ‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñà</td></tr><tr><td>epoch/policy_loss</td><td>‚ñÉ‚ñÇ‚ñÑ‚ñÜ‚ñÖ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñÑ‚ñÅ‚ñà‚ñÇ</td></tr><tr><td>epoch/value_loss</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñà‚ñÅ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÅ</td></tr><tr><td>train/advantage_mean</td><td>‚ñÑ‚ñÜ‚ñà‚ñÅ</td></tr><tr><td>train/advantage_std</td><td>‚ñÖ‚ñà‚ñÜ‚ñÅ</td></tr><tr><td>train/approx_kl</td><td>‚ñÇ‚ñà‚ñÅ‚ñÉ</td></tr><tr><td>train/clip_fraction</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/entropy</td><td>‚ñÅ‚ñÑ‚ñà‚ñÉ</td></tr><tr><td>train/explained_var</td><td>‚ñà‚ñá‚ñá‚ñÅ</td></tr><tr><td>train/kl_div</td><td>‚ñÅ‚ñà‚ñà‚ñá</td></tr><tr><td>train/mean_reward</td><td>‚ñÅ‚ñÑ‚ñà‚ñÑ</td></tr><tr><td>train/policy_loss</td><td>‚ñÖ‚ñÉ‚ñÅ‚ñà</td></tr><tr><td>train/returns_mean</td><td>‚ñà‚ñÉ‚ñÉ‚ñÅ</td></tr><tr><td>train/total_steps</td><td>‚ñÅ‚ñÉ‚ñÜ‚ñà</td></tr><tr><td>train/value_loss</td><td>‚ñà‚ñÉ‚ñÉ‚ñÅ</td></tr><tr><td>train/value_mean</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñà</td></tr><tr><td>trainer/global_step</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>epoch/approx_kl</td><td>1e-05</td></tr><tr><td>epoch/clip_fraction</td><td>0</td></tr><tr><td>epoch/entropy</td><td>0.68832</td></tr><tr><td>epoch/explained_var</td><td>0.00081</td></tr><tr><td>epoch/kl_div</td><td>0.00026</td></tr><tr><td>epoch/policy_loss</td><td>-0.0073</td></tr><tr><td>epoch/value_loss</td><td>35.77073</td></tr><tr><td>train/advantage_mean</td><td>0.00686</td></tr><tr><td>train/advantage_std</td><td>0.96531</td></tr><tr><td>train/approx_kl</td><td>1e-05</td></tr><tr><td>train/clip_fraction</td><td>0</td></tr><tr><td>train/entropy</td><td>0.68737</td></tr><tr><td>train/explained_var</td><td>0.00032</td></tr><tr><td>train/kl_div</td><td>-8e-05</td></tr><tr><td>train/mean_reward</td><td>22.64</td></tr><tr><td>train/policy_loss</td><td>-0.01445</td></tr><tr><td>train/returns_mean</td><td>8.6832</td></tr><tr><td>train/total_steps</td><td>10240</td></tr><tr><td>train/value_loss</td><td>41.00718</td></tr><tr><td>train/value_mean</td><td>0.53785</td></tr><tr><td>trainer/global_step</td><td>39</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ppo-42</strong> at: <a href='https://wandb.ai/tsilva/CartPole-v1/runs/3jsx1fs1' target=\"_blank\">https://wandb.ai/tsilva/CartPole-v1/runs/3jsx1fs1</a><br> View project at: <a href='https://wandb.ai/tsilva/CartPole-v1' target=\"_blank\">https://wandb.ai/tsilva/CartPole-v1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250722_152657-3jsx1fs1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'SyncRolloutCollector' object has no attribute 'get_rollout'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m trainer = create_trainer(CONFIG, project_name=ENV_ID, run_name=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mALGO_ID\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG.seed\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Fit the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/tsilva/gymnasium-solver/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/tsilva/gymnasium-solver/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     51\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/tsilva/gymnasium-solver/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    592\u001b[39m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    602\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/tsilva/gymnasium-solver/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1017\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/tsilva/gymnasium-solver/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1056\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1054\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_sanity_check()\n\u001b[32m   1055\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1057\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1058\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.state\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/tsilva/gymnasium-solver/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:217\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    215\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m    216\u001b[39m     \u001b[38;5;28mself\u001b[39m.advance()\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mon_advance_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    219\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/tsilva/gymnasium-solver/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:469\u001b[39m, in \u001b[36m_FitLoop.on_advance_end\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;66;03m# call train epoch end hooks\u001b[39;00m\n\u001b[32m    465\u001b[39m \u001b[38;5;66;03m# we always call callback hooks first, but here we need to make an exception for the callbacks that\u001b[39;00m\n\u001b[32m    466\u001b[39m \u001b[38;5;66;03m# monitor a metric, otherwise they wouldn't be able to monitor a key logged in\u001b[39;00m\n\u001b[32m    467\u001b[39m \u001b[38;5;66;03m# `LightningModule.on_train_epoch_end`\u001b[39;00m\n\u001b[32m    468\u001b[39m call._call_callback_hooks(trainer, \u001b[33m\"\u001b[39m\u001b[33mon_train_epoch_end\u001b[39m\u001b[33m\"\u001b[39m, monitoring_callbacks=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mon_train_epoch_end\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    470\u001b[39m call._call_callback_hooks(trainer, \u001b[33m\"\u001b[39m\u001b[33mon_train_epoch_end\u001b[39m\u001b[33m\"\u001b[39m, monitoring_callbacks=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    472\u001b[39m trainer._logger_connector.on_epoch_end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/tsilva/gymnasium-solver/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:176\u001b[39m, in \u001b[36m_call_lightning_module_hook\u001b[39m\u001b[34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m pl_module._current_fx_name = hook_name\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    179\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/tsilva/gymnasium-solver/learners/base.py:109\u001b[39m, in \u001b[36mLearner.on_train_epoch_end\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    106\u001b[39m     \u001b[38;5;28mself\u001b[39m.metrics.log_metrics(epoch_metrics, prefix=\u001b[33m\"\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.current_epoch + \u001b[32m1\u001b[39m) % \u001b[38;5;28mself\u001b[39m.config.eval_interval == \u001b[32m0\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_eval_early_stop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repos/tsilva/gymnasium-solver/learners/base.py:182\u001b[39m, in \u001b[36mLearner._check_eval_early_stop\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_check_eval_early_stop\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    181\u001b[39m     trajectories = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m trajectories \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: trajectories = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meval_rollout_collector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_rollout\u001b[49m() \u001b[38;5;66;03m# TODO: move this loop inside the collector\u001b[39;00m\n\u001b[32m    183\u001b[39m     episodes = group_trajectories_by_episode(trajectories, max_episodes=\u001b[38;5;28mself\u001b[39m.config.eval_episodes)\n\u001b[32m    184\u001b[39m     episode_rewards = [\u001b[38;5;28msum\u001b[39m(step[\u001b[32m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m episode) \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m episodes]\n",
      "\u001b[31mAttributeError\u001b[39m: 'SyncRolloutCollector' object has no attribute 'get_rollout'"
     ]
    }
   ],
   "source": [
    "from utils.training import create_trainer\n",
    "from tsilva_notebook_utils.torch import get_default_device # TODO: get rid of tsilva_noteb\n",
    "from utils.rollouts import SyncRolloutCollector # TODO: restore async functionality\n",
    "from utils.models import PolicyNet, ValueNet\n",
    "from learners.ppo import PPOLearner\n",
    "from learners.reinforce import REINFORCELearner\n",
    "\n",
    "# TODO: make rollout collector clone models?\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = env.action_space.n\n",
    "policy_model = PolicyNet(input_dim, output_dim, CONFIG.hidden_dims)\n",
    "value_model = ValueNet(input_dim, CONFIG.hidden_dims) if ALGO_ID == \"ppo\" else None # TODO: softcode this better\n",
    "\n",
    "train_rollout_collector = SyncRolloutCollector(\n",
    "    build_env_fn(CONFIG.seed),\n",
    "    policy_model,\n",
    "    value_model=value_model,\n",
    "    n_steps=CONFIG.train_rollout_steps\n",
    ")\n",
    "eval_rollout_collector = SyncRolloutCollector(\n",
    "    # TODO: pass env factory and rebuild env on start/stop? this allows using same rollout collector for final evaluation\n",
    "    build_env_fn(CONFIG.seed + 1000),  # Use a different seed for evaluation\n",
    "    policy_model,\n",
    "    n_steps=CONFIG.train_rollout_steps # TODO: change this\n",
    ")\n",
    "algo_id = ALGO_ID.lower()\n",
    "if algo_id == \"ppo\": agent = PPOLearner(CONFIG, train_rollout_collector, policy_model, value_model, eval_rollout_collector=eval_rollout_collector)\n",
    "elif algo_id == \"reinforce\": agent = REINFORCELearner(CONFIG, train_rollout_collector, policy_model, eval_rollout_collector=eval_rollout_collector)\n",
    "\n",
    "# Create trainer with W&B logging\n",
    "# TODO: infer most args\n",
    "trainer = create_trainer(CONFIG, project_name=ENV_ID, run_name=f\"{ALGO_ID}-{CONFIG.seed}\")\n",
    "\n",
    "# Fit the model\n",
    "trainer.fit(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8966dc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import evaluate_agent\n",
    "\n",
    "# Evaluate agent and render episodes\n",
    "# TODO: evaluate agent should receive rollout collector as an argument, not the agent and build_env_fn\n",
    "results = evaluate_agent(\n",
    "    agent, \n",
    "    build_env_fn, \n",
    "    n_episodes=8, \n",
    "    deterministic=True, \n",
    "    render=True,\n",
    "    grid=(2, 2), \n",
    "    text_color=(0, 0, 0), \n",
    "    out_dir=\"./tmp\"\n",
    ")\n",
    "\n",
    "print(f\"Mean reward: {results['mean_reward']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymnasium-solver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
