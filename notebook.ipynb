{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fae7ff8f",
   "metadata": {},
   "source": [
    "# Environment Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768cf7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autoreloading of modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a074cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_ID = \"CartPole-v1\"\n",
    "#ENV_ID = \"LunarLander-v3\"\n",
    "#ALGORITHM = \"reinforce\"\n",
    "ALGO_ID = \"ppo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b028503",
   "metadata": {},
   "source": [
    "Load secrets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5101c4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsilva_notebook_utils.colab import load_secrets_into_env\n",
    "_ = load_secrets_into_env(['WANDB_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d847b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.config import load_config\n",
    "\n",
    "# Load configuration from YAML files\n",
    "CONFIG = load_config(ENV_ID, ALGO_ID)\n",
    "print(f\"Loaded config for {ENV_ID} with {ALGO_ID} algorithm:\")\n",
    "print(CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90fd389",
   "metadata": {},
   "source": [
    "Build environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7605ffa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsilva_notebook_utils.gymnasium import log_env_info\n",
    "from utils.environment import setup_environment\n",
    "build_env_fn = setup_environment(CONFIG)\n",
    "env = build_env_fn(CONFIG.seed)\n",
    "log_env_info(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69afe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.rollouts import SyncRolloutCollector\n",
    "from utils.models import PolicyNet, ValueNet\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.n if hasattr(env.action_space, 'n') else env.action_space.shape[0]\n",
    "policy_model = PolicyNet(obs_dim, act_dim, CONFIG.hidden_dims)\n",
    "value_model = ValueNet(obs_dim, CONFIG.hidden_dims) if ALGO_ID == \"ppo\" else None\n",
    "train_rollout_collector = SyncRolloutCollector( # TODO: assert models are in eval mode when running collect_rollouts\n",
    "    build_env_fn(CONFIG.seed),\n",
    "    policy_model,\n",
    "    value_model=value_model,\n",
    "    n_steps=CONFIG.train_rollout_steps\n",
    ")\n",
    "train_rollout_collector.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c758e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories = train_rollout_collector.get_rollout()\n",
    "len(trajectories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123f099b",
   "metadata": {},
   "source": [
    "Define models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bad226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.training import create_trainer\n",
    "from tsilva_notebook_utils.torch import get_default_device # TODO: get rid of tsilva_noteb\n",
    "from utils.rollouts import SyncRolloutCollector # TODO: restore async functionality\n",
    "from utils.models import PolicyNet, ValueNet\n",
    "from learners.ppo import PPOLearner\n",
    "from learners.reinforce import REINFORCELearner\n",
    "\n",
    "# TODO: get this out of here\n",
    "# Get environment dimensions\n",
    "# TODO: use actionspace/observation_space utils\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.n if hasattr(env.action_space, 'n') else env.action_space.shape[0]\n",
    "\n",
    "# TODO: get this out of here\n",
    "# Debug device information\n",
    "print(f\"Default device: {get_default_device()}\")\n",
    "\n",
    "# TODO: create policy model and value model outside?\n",
    "# TODO: make rollout collector clone models?\n",
    "# TODO: get action-space, observation-space, etc. from env\n",
    "policy_model = PolicyNet(obs_dim, act_dim, CONFIG.hidden_dims)\n",
    "value_model = ValueNet(obs_dim, CONFIG.hidden_dims) if ALGO_ID == \"ppo\" else None # TODO: rename ALGORITHM to algo_id\n",
    "\n",
    "train_rollout_collector = SyncRolloutCollector(\n",
    "    build_env_fn(CONFIG.seed),\n",
    "    policy_model,\n",
    "    value_model=value_model,\n",
    "    n_steps=CONFIG.train_rollout_steps\n",
    ")\n",
    "eval_rollout_collector = SyncRolloutCollector(\n",
    "    # TODO: pass env factory and rebuild env on start/stop? this allows using same rollout collector for final evaluation\n",
    "    build_env_fn(CONFIG.seed + 1000),  # Use a different seed for evaluation\n",
    "    policy_model,\n",
    "    value_model=value_model,\n",
    "    n_steps=CONFIG.train_rollout_steps # TODO: change this\n",
    ")\n",
    "algo_id = ALGO_ID.lower()\n",
    "if algo_id == \"ppo\": agent = PPOLearner(CONFIG, train_rollout_collector, policy_model, value_model, eval_rollout_collector=eval_rollout_collector)\n",
    "elif algo_id == \"reinforce\": agent = REINFORCELearner(CONFIG, train_rollout_collector, policy_model, eval_rollout_collector=eval_rollout_collector)\n",
    "\n",
    "# Create trainer with W&B logging\n",
    "# TODO: infer most args\n",
    "trainer = create_trainer(CONFIG, project_name=ENV_ID, run_name=f\"{ALGO_ID}-{CONFIG.seed}\")\n",
    "\n",
    "# Fit the model\n",
    "trainer.fit(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8966dc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluation import evaluate_agent\n",
    "\n",
    "# Evaluate agent and render episodes\n",
    "# TODO: evaluate agent should receive rollout collector as an argument, not the agent and build_env_fn\n",
    "results = evaluate_agent(\n",
    "    agent, \n",
    "    build_env_fn, \n",
    "    n_episodes=8, \n",
    "    deterministic=True, \n",
    "    render=True,\n",
    "    grid=(2, 2), \n",
    "    text_color=(0, 0, 0), \n",
    "    out_dir=\"./tmp\"\n",
    ")\n",
    "\n",
    "print(f\"Mean reward: {results['mean_reward']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymnasium-solver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
