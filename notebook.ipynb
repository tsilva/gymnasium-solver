{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fae7ff8f",
   "metadata": {},
   "source": [
    "# Environment Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "768cf7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autoreloading of modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a074cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_ID = \"CartPole-v1\"\n",
    "#ENV_ID = \"LunarLander-v3\"\n",
    "#ALGORITHM = \"reinforce\"\n",
    "ALGORITHM = \"ppo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ecffa8",
   "metadata": {},
   "source": [
    "Install packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b165239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.environment import suppress_warnings\n",
    "\n",
    "# Suppress common warnings\n",
    "suppress_warnings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b028503",
   "metadata": {},
   "source": [
    "Load secrets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5101c4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsilva_notebook_utils.colab import load_secrets_into_env\n",
    "\n",
    "_ = load_secrets_into_env([\n",
    "    'WANDB_API_KEY'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d847b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config for CartPole-v1 with ppo algorithm:\n",
      "RLConfig(env_id='CartPole-v1', seed=42, max_epochs=-1, gamma=0.99, lam=0.95, clip_epsilon=0.2, batch_size=256, train_rollout_steps=512, eval_interval=20, eval_episodes=5, reward_threshold=475, policy_lr=0.001, value_lr=0.001, hidden_dim=32, entropy_coef=0.01, shared_backbone=True, backbone_dim=64, normalize=False, mean_reward_window=100, rollout_interval=1, n_envs='auto', async_rollouts=True)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from tsilva_notebook_utils.gymnasium import build_env as _build_env, set_random_seed\n",
    "from utils.config import load_config\n",
    "\n",
    "# Load configuration from YAML files\n",
    "CONFIG = load_config(ENV_ID, ALGORITHM)\n",
    "print(f\"Loaded config for {ENV_ID} with {ALGORITHM} algorithm:\")\n",
    "print(CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90fd389",
   "metadata": {},
   "source": [
    "Build environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7605ffa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Info (SubprocVecEnv with 12 envs)\n",
      "  Env ID: CartPole-v1\n",
      "  Observation space: Box(low=[-4.8, -inf, -0.419, -inf], high=[4.8, inf, 0.419, inf], shape=(4,), dtype=float32)\n",
      "  Action space: Discrete(2)\n",
      "  Max episode steps: 500\n"
     ]
    }
   ],
   "source": [
    "from tsilva_notebook_utils.gymnasium import log_env_info\n",
    "from utils.environment import setup_environment\n",
    "\n",
    "# Setup environment with configuration\n",
    "build_env_fn = setup_environment(CONFIG)\n",
    "\n",
    "# Test building env\n",
    "env = build_env_fn(CONFIG.seed)\n",
    "log_env_info(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123f099b",
   "metadata": {},
   "source": [
    "Define models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6bad226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default device: cuda\n",
      "Policy model device: cuda:0\n",
      "Value model device: cuda:0\n",
      "Rollout collector type: <class 'utils.rollouts.SyncRolloutCollector'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtsilva\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250717_132011-xo29l021</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tsilva/CartPole-v1/runs/xo29l021' target=\"_blank\">ppo-42</a></strong> to <a href='https://wandb.ai/tsilva/CartPole-v1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tsilva/CartPole-v1' target=\"_blank\">https://wandb.ai/tsilva/CartPole-v1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tsilva/CartPole-v1/runs/xo29l021' target=\"_blank\">https://wandb.ai/tsilva/CartPole-v1/runs/xo29l021</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 W&B Run: https://wandb.ai/tsilva/CartPole-v1/runs/xo29l021\n",
      "Waiting for initial rollout...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type              | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | policy_model | SharedPolicyNet   | 4.6 K  | train\n",
      "1 | value_model  | SharedValueNet    | 4.6 K  | train\n",
      "2 | shared_model | SharedBackboneNet | 4.6 K  | eval \n",
      "-----------------------------------------------------------\n",
      "4.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "4.6 K     Total params\n",
      "0.018     Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "12        Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started at 2025-07-17 13:20:14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsilva/repos/tsilva/gymnasium-solver/.venv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ebe5ac99734073a44e04317fb3f303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 59 with eval mean reward 500.00 >= threshold 475\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>epoch/approx_kl</td><td>▂▄▃▂▄▁▂▃▂▂▅▂▃▂▂▅▇▆▄▆▃▃▂▄▆▄▆█▂▃▃▁▄▃▁▁▃▂▃▁</td></tr><tr><td>epoch/clip_fraction</td><td>▁▄▃▃▂▂▂▃▂▄▂▄▂▂▃▆▆▄▅▁▂▂▃▆▂▇█▂▃▃▃▂▁▂▃▄▄▁▁▂</td></tr><tr><td>epoch/entropy</td><td>█▇▇▆▅▄▃▃▃▃▃▄▃▃▃▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch/explained_var</td><td>▄▄▄▃▂▁▁▃▃▄▆▆▆▇██▇█████████████▇██▇▆▆▆▅▅▆</td></tr><tr><td>epoch/kl_div</td><td>▃▄▂▂▃▃▂▃▄▂▂▁▃▃▅█▅▂▆▁▄▂▃▂▇▄▆▃▃▄▅▄▃▁▂▂▃▃▂▂</td></tr><tr><td>epoch/policy_loss</td><td>▂▁▂▃▅▆▆▇▆▇▅▆▇▆▇▇▅▇█▇█▆▆▇▆▆▇▆█▆▇▇▆▇▇▇▇▇▇▇</td></tr><tr><td>epoch/total_loss</td><td>▃▆▆▆█▆▇▅▅▅▇▆▃▄▂▄▅▂▂▂▂▁▃▂▁▂▁▁▁▃▃▄▁▄▅▄▆▆▃▅</td></tr><tr><td>epoch/value_loss</td><td>▃▅▆▆▆▆▅▅▅▅▄▆█▄▅▂▃▄▅▂▂▂▂▂▁▁▂▂▁▁▃▂▄▂▁▅▅▆▅▅</td></tr><tr><td>eval/mean_reward</td><td>▁▅█</td></tr><tr><td>rollout/avg_steps_per_episode</td><td>▁▅█</td></tr><tr><td>rollout/mean_reward</td><td>▁▅█</td></tr><tr><td>rollout/num_episodes</td><td>▁▁▁</td></tr><tr><td>rollout/num_steps</td><td>▁▅█</td></tr><tr><td>rollout/queue_updated</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>rollout/steps_per_second</td><td>█▁█</td></tr><tr><td>rollout/time_elapsed</td><td>▁▆█</td></tr><tr><td>train/advantage_mean</td><td>▅▃▂▄▆▄▆▃▅▂▄▄▇▂█▂▅▄▁▄▂▇▃▄▄▁▄▄▅▅▃▆▄▇▁▄▃▂▃▆</td></tr><tr><td>train/advantage_std</td><td>▄▅▄▅▄▄▄▅▆▅▆▂▄▃▆▆▃▅▅▁▄▅▃▅█▂▅▆▄▂▆▅▂▃▃▂▃▅▄▆</td></tr><tr><td>train/approx_kl</td><td>▁▂▂▁▁▂▁▂▂▁▂▁▂▃▁▅▃█▁▂▃▁▁▁▃▁▂▁▂▁▃▁▄▁▁▁▂▁▆▃</td></tr><tr><td>train/clip_fraction</td><td>▂▁▂▁▂▁▁▂▁▁▆█▃▂▂▁▁▁▁▂▃▁▁▃▁▂▂▁▁▁▁▃▁▁▁▄▁▁▁▂</td></tr><tr><td>train/entropy</td><td>█▇▆▅▅▅▃▃▃▄▃▃▄▄▃▄▃▂▂▃▃▃▃▃▂▃▂▂▃▂▃▂▂▂▂▂▂▁▂▂</td></tr><tr><td>train/explained_var</td><td>▄▄▃▁▁▃▆▆▆▅▅▇▇██▇▇▇█▇█▇▇██▇▇█▇█▇▆▇▆▆▇▇▆▅▆</td></tr><tr><td>train/kl_div</td><td>▄▁▃█▄▄▅▅▃▃▆▃▃▅▄▄▄▃▄▃▅█▃▇▄▄▆▅▄▁▄▄▂▅▄▅▃▅▅▄</td></tr><tr><td>train/mean_reward</td><td>▁▁▁▁▂▃▃▄▄▄▅▅▅▅▄▄▄▄▄▄▅▅▅▅▅▄▅▅▅▆▇▇▇▇▇▇████</td></tr><tr><td>train/policy_loss</td><td>▂▅▆▆▆▃▅▄▇▆▅▅▇▆▄█▆▇▂▆▆▁▂▇▅▇▂▃▄▃▄▆▅▆▆▁▅▆█▆</td></tr><tr><td>train/returns_mean</td><td>▁▁▁▄▄▆▆▆▇▆▆▆▆▆▆▇▇▇▇▇▆▇▇▇▆▇▇▇▇▇▇███████▇█</td></tr><tr><td>train/total_loss</td><td>▄▄▆▅█▅▆▆▅▇▅▃▄▂▃▄▁▄▂▁▂▁▃▁▃▂▄▁▂▅▇▄▃▃▆▆▆▄▅▃</td></tr><tr><td>train/total_steps</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train/value_loss</td><td>▅▅▅▅▄▅█▇▅▆▃▇▆▄▂▂▂▄▄▄▂▁▂▁▁▅▄▃▂▃▄▂▁▄▃▃▆▇▃▃</td></tr><tr><td>train/value_mean</td><td>▁▁▁▁▂▄▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███▇▇█████████▇</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>59</td></tr><tr><td>epoch/approx_kl</td><td>0.00275</td></tr><tr><td>epoch/clip_fraction</td><td>0.01904</td></tr><tr><td>epoch/entropy</td><td>0.4876</td></tr><tr><td>epoch/explained_var</td><td>0.41417</td></tr><tr><td>epoch/kl_div</td><td>0.00215</td></tr><tr><td>epoch/policy_loss</td><td>-0.0045</td></tr><tr><td>epoch/total_loss</td><td>55.96505</td></tr><tr><td>epoch/value_loss</td><td>55.96955</td></tr><tr><td>eval/mean_reward</td><td>500</td></tr><tr><td>rollout/avg_steps_per_episode</td><td>499.90002</td></tr><tr><td>rollout/mean_reward</td><td>500</td></tr><tr><td>rollout/num_episodes</td><td>5</td></tr><tr><td>rollout/num_steps</td><td>2500</td></tr><tr><td>rollout/queue_updated</td><td>1</td></tr><tr><td>rollout/steps_per_second</td><td>743.16345</td></tr><tr><td>rollout/time_elapsed</td><td>3.363</td></tr><tr><td>train/advantage_mean</td><td>0.03932</td></tr><tr><td>train/advantage_std</td><td>0.91258</td></tr><tr><td>train/approx_kl</td><td>0.00422</td></tr><tr><td>train/clip_fraction</td><td>0.04688</td></tr><tr><td>train/entropy</td><td>0.46153</td></tr><tr><td>train/explained_var</td><td>0.39219</td></tr><tr><td>train/kl_div</td><td>0.00548</td></tr><tr><td>train/mean_reward</td><td>482.32001</td></tr><tr><td>train/policy_loss</td><td>-0.05164</td></tr><tr><td>train/returns_mean</td><td>85.67981</td></tr><tr><td>train/total_loss</td><td>45.73189</td></tr><tr><td>train/total_steps</td><td>368640</td></tr><tr><td>train/value_loss</td><td>45.78353</td></tr><tr><td>train/value_mean</td><td>86.02058</td></tr><tr><td>trainer/global_step</td><td>1439</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ppo-42</strong> at: <a href='https://wandb.ai/tsilva/CartPole-v1/runs/xo29l021' target=\"_blank\">https://wandb.ai/tsilva/CartPole-v1/runs/xo29l021</a><br> View project at: <a href='https://wandb.ai/tsilva/CartPole-v1' target=\"_blank\">https://wandb.ai/tsilva/CartPole-v1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250717_132011-xo29l021/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed in 98.76 seconds (1.65 minutes)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from utils.training import create_agent, create_trainer\n",
    "from tsilva_notebook_utils.torch import get_default_device\n",
    "\n",
    "# Get environment dimensions\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.n if hasattr(env.action_space, 'n') else env.action_space.shape[0]\n",
    "\n",
    "# Debug device information\n",
    "print(f\"Default device: {get_default_device()}\")\n",
    "\n",
    "# Create agent using utility function\n",
    "agent = create_agent(CONFIG, build_env_fn, obs_dim, act_dim, algorithm=ALGORITHM)\n",
    "\n",
    "# Debug model devices\n",
    "print(f\"Policy model device: {next(agent.policy_model.parameters()).device}\")\n",
    "if hasattr(agent, 'value_model') and agent.value_model is not None:\n",
    "    print(f\"Value model device: {next(agent.value_model.parameters()).device}\")\n",
    "print(f\"Rollout collector type: {type(agent.rollout_collector)}\")\n",
    "\n",
    "# Create trainer with W&B logging\n",
    "trainer = create_trainer(CONFIG, project_name=ENV_ID, run_name=f\"{ALGORITHM}-{CONFIG.seed}\")\n",
    "\n",
    "# Fit the model\n",
    "trainer.fit(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8966dc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 500.00\n"
     ]
    }
   ],
   "source": [
    "from utils.evaluation import evaluate_agent\n",
    "\n",
    "# Evaluate agent and render episodes\n",
    "results = evaluate_agent(\n",
    "    agent, \n",
    "    build_env_fn, \n",
    "    n_episodes=8, \n",
    "    deterministic=True, \n",
    "    render=True,\n",
    "    grid=(2, 2), \n",
    "    text_color=(0, 0, 0), \n",
    "    out_dir=\"./tmp\"\n",
    ")\n",
    "\n",
    "print(f\"Mean reward: {results['mean_reward']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
