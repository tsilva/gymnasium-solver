# Run Report: ${title}

- Date: ${date}
${run_id_bullet}- Run dir: ${run_dir}
${duration_bullet}

## Summary

${summary_lines}

## Environment & Config

- env_id: ${env_id}
- algo_id: ${algo_id}
- seed: ${seed}
- n_envs: ${n_envs}
- eval_freq_epochs: ${eval_freq}

### Full config (YAML)

${config_block}

## Metrics (last known values)

${metrics_block}

## LLM prompt

You are an RL training assistant. Based on the config and metrics above, suggest concrete hyperparameter adjustments to improve sample efficiency and/or final reward. Consider: learning rate scheduling, entropy coefficient, batch size, n_steps, clip range (for PPO), advantage normalization, evaluation cadence, and environment-specific wrappers. Prioritize changes likely to increase eval/ep_rew_mean sooner without destabilizing training. Return a short, actionable checklist with 3â€“7 items and brief justifications.
